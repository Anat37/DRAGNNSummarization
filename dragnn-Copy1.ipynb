{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.cuda import FloatTensor, LongTensor\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "\n",
    "PAD_TOKEN = '[PAD]' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNKNOWN_TOKEN = '[UNK]' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "START_DECODING = '[START]' # This has a vocab id, which is used at the start of every decoder input sequence\n",
    "STOP_DECODING = '[STOP]' # This has a vocab id, which is used at the end of untruncated target sequences\n",
    "\n",
    "VOCAB_SIZE = 20000\n",
    "ADDITIONAL_WORDS = 100\n",
    "\n",
    "class Vocab(object):\n",
    "\n",
    "  def __init__(self, vocab_file, max_size):\n",
    "    self._word_to_id = {}\n",
    "    self._id_to_word = {}\n",
    "    self._count = 0 # keeps track of total number of words in the Vocab\n",
    "\n",
    "    # [UNK], [PAD], [START] and [STOP] get the ids 0,1,2,3.\n",
    "    for w in [UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
    "      self._word_to_id[w] = self._count\n",
    "      self._id_to_word[self._count] = w\n",
    "      self._count += 1\n",
    "\n",
    "    # Read the vocab file and add words up to max_size\n",
    "    with open(vocab_file, 'r') as vocab_f:\n",
    "      for line in vocab_f:\n",
    "        pieces = line.split()\n",
    "        if len(pieces) != 2:\n",
    "          print('Warning: incorrectly formatted line in vocabulary file: %s\\n' % line)\n",
    "          continue\n",
    "        w = pieces[0]\n",
    "        if w in [SENTENCE_START, SENTENCE_END, UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
    "          raise Exception('<s>, </s>, [UNK], [PAD], [START] and [STOP] shouldn\\'t be in the vocab file, but %s is' % w)\n",
    "        if w in self._word_to_id:\n",
    "          raise Exception('Duplicated word in vocabulary file: %s' % w)\n",
    "        self._word_to_id[w] = self._count\n",
    "        self._id_to_word[self._count] = w\n",
    "        self._count += 1\n",
    "        if max_size != 0 and self._count >= max_size:\n",
    "          print(\"max_size of vocab was specified as %i; we now have %i words. Stopping reading.\" % (max_size, self._count))\n",
    "          break\n",
    "\n",
    "    print(\"Finished constructing vocabulary of %i total words. Last word added: %s\" % (self._count, self._id_to_word[self._count-1]))\n",
    "\n",
    "  def word2id(self, word):\n",
    "    if word not in self._word_to_id:\n",
    "      return self._word_to_id[UNKNOWN_TOKEN]\n",
    "    return self._word_to_id[word]\n",
    "\n",
    "  def id2word(self, word_id):\n",
    "    if word_id not in self._id_to_word:\n",
    "      raise ValueError('Id not found in vocab: %d' % word_id)\n",
    "    return self._id_to_word[word_id]\n",
    "\n",
    "  def size(self):\n",
    "    return self._count\n",
    "\n",
    "  def write_metadata(self, fpath):\n",
    "    print(\"Writing word embedding metadata file to %s...\" % (fpath))\n",
    "    with open(fpath, \"w\") as f:\n",
    "      fieldnames = ['word']\n",
    "      writer = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=fieldnames)\n",
    "      for i in xrange(self.size()):\n",
    "        writer.writerow({\"word\": self._id_to_word[i]})\n",
    "        \n",
    "\n",
    "\n",
    "def article2ids(article_words, vocab):\n",
    "    ids = []\n",
    "    oovs = []\n",
    "    unk_id = vocab.word2id(UNKNOWN_TOKEN)\n",
    "    for w in article_words.split():\n",
    "        w = str(w)\n",
    "        i = vocab.word2id(w)\n",
    "        if i == unk_id: # If w is OOV\n",
    "            if w not in oovs: # Add to list of OOVs\n",
    "                oovs.append(w)\n",
    "            oov_num = oovs.index(w) # This is 0 for the first article OOV, 1 for the second article OOV...\n",
    "            if oov_num < ADDITIONAL_WORDS:\n",
    "                ids.append(vocab.size() + oov_num) # This is e.g. 50000 for the first article OOV, 50001 for the second...\n",
    "        else:\n",
    "            ids.append(i)\n",
    "    return ids, oovs\n",
    "\n",
    "\n",
    "def abstract2ids(abstract_words, vocab, article_oovs):\n",
    "    ids = []\n",
    "    unk_id = vocab.word2id(UNKNOWN_TOKEN)\n",
    "    for w in abstract_words.split():\n",
    "        i = vocab.word2id(w)\n",
    "        if i == unk_id: # If w is an OOV word\n",
    "            if w in article_oovs: # If w is an in-article OOV\n",
    "                vocab_idx = vocab.size() + article_oovs.index(w) # Map to its temporary article OOV number\n",
    "                if vocab_idx < VOCAB_SIZE + ADDITIONAL_WORDS:\n",
    "                    ids.append(vocab_idx)\n",
    "        else:\n",
    "            ids.append(i)\n",
    "    return ids\n",
    "\n",
    "def outputids2words(id_list, vocab, article_oovs):\n",
    "    words = []\n",
    "    for i in id_list:\n",
    "        if i < VOCAB_SIZE:\n",
    "            w = vocab.id2word(i)\n",
    "        else:\n",
    "            article_oov_idx = i - vocab.size()\n",
    "            print('unknown generated')\n",
    "            w = article_oovs[article_oov_idx]\n",
    "        words.append(w)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "from tensorflow.core.example import example_pb2\n",
    "\n",
    "def example_gen(filename):\n",
    "    reader = open(filename, 'rb')\n",
    "    examples = []\n",
    "    while True:\n",
    "        len_bytes = reader.read(8)\n",
    "        if not len_bytes: break # finished reading this file\n",
    "        str_len = struct.unpack('q', len_bytes)[0]\n",
    "        example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
    "        e = example_pb2.Example.FromString(example_str)\n",
    "        examples.append(e)\n",
    "        \n",
    "    for e in examples:  \n",
    "        article_text = e.features.feature['article'].bytes_list.value[0]\n",
    "        abstract_text = e.features.feature['abstract'].bytes_list.value[0]\n",
    "        yield (article_text.decode('utf-8'), abstract_text.decode('utf-8'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_size of vocab was specified as 20000; we now have 20000 words. Stopping reading.\n",
      "Finished constructing vocabulary of 20000 total words. Last word added: then-president\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(\"finished_files/vocab\", VOCAB_SIZE)\n",
    "SENTENCE_START_ID = vocab.word2id('<s>')\n",
    "SENTENCE_END_ID = vocab.word2id('</s>')\n",
    "\n",
    "PAD_TOKEN_ID = vocab.word2id('[PAD]')\n",
    "UNKNOWN_TOKEN_ID = vocab.word2id('[UNK]')\n",
    "START_DECODING_ID = vocab.word2id('[START]')\n",
    "STOP_DECODING_ID = vocab.word2id('[STOP]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoder import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24981504"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from decoder import *\n",
    "from pointerTBRU import BeamSearchProviderTBRU, LSTMTBRU, AttentionTBRU, ConcatTBRU, PointerTBRU, ContextTBRU, CoverageAttentionTBRU\n",
    "\n",
    "def build_decoder_model2():\n",
    "    master = DRAGNNDecoderMaster()\n",
    "    embeddings_computer = EmbeddingComputer(VOCAB_SIZE + ADDITIONAL_WORDS, 100, PAD_TOKEN_ID)\n",
    "    master.add_component_encoder(TBRU(\"embed\", TaggerRecurrent(\"input\", \"embed\", False), embeddings_computer, (1,), True).cuda())\n",
    "    #master.add_component(TBRU(\"extractive\", TaggerRecurrent(\"embed\", \"extractive\"), TaggerComputer(1000, 1000), (1,), True).cuda())\n",
    "    master.add_component_encoder(TBRU(\"rnn\", RNNSolidRecurrent(\"embed\", \"rnn\"), RNNSolidComputer(100, 50), (1,), True).cuda())\n",
    "    \n",
    "    master.add_component_decoder(TBRU(\"decoder_embed\", TaggerRecurrent(None, \"decoder_embed\", True),embeddings_computer, (1,), True).cuda())\n",
    "    master.add_component_decoder(BeamSearchProviderTBRU(\"beamProvider\", (1,), None, \"decoder\", True, False))\n",
    "    pTBRU = AttentiveLSTMTBRU(\"decoder\", (1,), True, 100, 100, 100, \"rnn\", \"decoder_embed\", False)\n",
    "    master.add_component_decoder(pTBRU.cuda())\n",
    "    master.add_component_decoder(TBRU(\"output\", TaggerRecurrent(\"decoder\", \"output\", False), TaggerComputer(100, VOCAB_SIZE), (1,), True).cuda())\n",
    "    return master\n",
    "\n",
    "def build_decoder_model():\n",
    "    master = DRAGNNDecoderMaster()\n",
    "    embeddings_computer = EmbeddingComputer(VOCAB_SIZE + ADDITIONAL_WORDS, 100, PAD_TOKEN_ID)\n",
    "    master.add_component_encoder(TBRU(\"embed\", TaggerRecurrent(\"input\", \"embed\", False), embeddings_computer, (1,), True).cuda())\n",
    "    #master.add_component(TBRU(\"extractive\", TaggerRecurrent(\"embed\", \"extractive\"), TaggerComputer(1000, 1000), (1,), True).cuda())\n",
    "    master.add_component_encoder(TBRU(\"rnn\", RNNSolidRecurrent(\"embed\", \"rnn\"), RNNSolidComputer(100, 50), (1,), True).cuda())\n",
    "    \n",
    "    \n",
    "    master.add_component_decoder(TBRU(\"decoder_embed\", TaggerRecurrent(None, \"decoder_embed\", True),embeddings_computer, (1,), is_solid=True).cuda())\n",
    "    \n",
    "    master.add_component_decoder(BeamSearchProviderTBRU(\"beamProvider\", None, 0, \"decoder\", True, is_solid=False))\n",
    "    master.add_component_decoder(BeamSearchProviderTBRU(\"beamProvider1\", None, 0, \"lstm_state_layer\", True, is_solid=False))\n",
    "    master.add_component_decoder(BeamSearchProviderTBRU(\"beamProvider2\", None, 1, \"coverage_layer\", True, is_solid=False))\n",
    "    \n",
    "    \n",
    "    pTBRU = LSTMTBRU(\"decoder\", \"lstm_state_layer\", False, 100, 100, \"rnn\", \"decoder_embed\", False, solid_modifiable=False)\n",
    "    master.add_component_decoder(pTBRU.cuda())\n",
    "    master.add_component_decoder(CoverageAttentionTBRU(\"attention_layer\", \"coverage_layer\", False, 100, 100, 100, 'decoder','rnn', is_first=False, solid_modifiable=False).cuda())\n",
    "    master.add_component_decoder(ContextTBRU('context', False, 'attention_layer', 'rnn', is_first=False, solid_modifiable=False))\n",
    "    master.add_component_decoder(ConcatTBRU('context_concat', False, 'decoder', 'context', is_first=False, solid_modifiable=False))\n",
    "    master.add_component_decoder(TBRU(\"output\", TaggerRecurrent(\"context_concat\", \"output\", False), TaggerComputer(200, VOCAB_SIZE), (1,), is_solid=False, solid_modifiable=False).cuda())\n",
    "    master.add_component_decoder(PointerTBRU(\"pointer_final\", False, 300, VOCAB_SIZE + ADDITIONAL_WORDS, \"attention_layer\", \"context\", \"output\", \"decoder_embed\", \"input\", \"decoder\", solid_modifiable=False).cuda())\n",
    "    return master\n",
    "\n",
    "model = build_decoder_model()\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(articles):\n",
    "    lens = [len(article) for article in articles]\n",
    "    max_len = max(lens)\n",
    "    \n",
    "    for i in range(len(articles)):\n",
    "        articles[i].extend([vocab.word2id(PAD_TOKEN)]*(max_len - len(articles[i])))\n",
    "    return np.array(articles).T\n",
    "\n",
    "def add_padding_for_tagging(articles, targets):\n",
    "    lens = [len(article) for article in articles]\n",
    "    max_len = max(lens)\n",
    "    \n",
    "    for i in range(len(articles)):\n",
    "        targets[i].extend([0]*(max_len - len(articles[i])))\n",
    "        articles[i].extend([vocab.word2id(PAD_TOKEN)]*(max_len - len(articles[i])))\n",
    "    return np.array(articles).T, np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def calculate_mask(articles):\n",
    "    mask = (articles == vocab.word2id(PAD_TOKEN))\n",
    "    mask = np.logical_xor(mask, np.ones(articles.shape))\n",
    "    return mask\n",
    "\n",
    "def get_target(self, article, abstract):\n",
    "    return [ int(i in abstract and i != vocab.word2id(UNKNOWN_TOKEN)) for i in article]\n",
    "\n",
    "def push_abs_ptr(article, abstract, i, abs_ptr):\n",
    "    while abs_ptr < len(abstract) and (not abstract[abs_ptr] in article[i+1:] \n",
    "                                               or abstract[abs_ptr] == vocab.word2id(SENTENCE_START)\n",
    "                                               or abstract[abs_ptr] == vocab.word2id(SENTENCE_STOP)\n",
    "                                               or abstract[abs_ptr] == vocab.word2id(UNKNOWN_TOKEN)):\n",
    "        abs_ptr += 1\n",
    "    return abs_ptr\n",
    "\n",
    "class Batcher():\n",
    "    \n",
    "    def __init__(self, filename, batch_size, max_article_len, max_target_len):\n",
    "        self.batch_size = batch_size\n",
    "        generator = example_gen(filename)\n",
    "\n",
    "        self.batches = []\n",
    "        unknown_words_cnt = 0\n",
    "        self.articles = []\n",
    "        self.targets = []\n",
    "        self.unknown_words = []\n",
    "        self.decoder_inputs = []\n",
    "        while True:\n",
    "            articles = []\n",
    "            targets = []\n",
    "            unknown_words = []\n",
    "            decoder_inputs = []\n",
    "            for i in range(batch_size):\n",
    "                try:\n",
    "                    \n",
    "                    article_text, abstract_text = next(generator)\n",
    "                    article_ids, unknown_words_list = article2ids(article_text, vocab)\n",
    "                    art_len = min(max_article_len, len(article_ids))\n",
    "                    article_ids = article_ids[:art_len]\n",
    "                    target = abstract2ids(abstract_text, vocab, unknown_words_list)\n",
    "                    tar_len = min(max_target_len, len(target))\n",
    "                    target = target[:tar_len - 1]\n",
    "                    target.append(STOP_DECODING_ID)\n",
    "                    decoder_input = [START_DECODING_ID]\n",
    "                    decoder_input.extend(abstract2ids(abstract_text, vocab, unknown_words_list))\n",
    "                    decoder_input = decoder_input[:tar_len]\n",
    "                    articles.append(article_ids)\n",
    "                    targets.append(target)\n",
    "                    decoder_inputs.append(decoder_input)\n",
    "                    unknown_words.append(unknown_words_list)\n",
    "                    unknown_words_cnt = max(len(unknown_words_list), unknown_words_cnt)\n",
    "                except:\n",
    "                    break\n",
    "            if len(articles) == 0:\n",
    "                break\n",
    "            self.articles.extend(articles)\n",
    "            self.targets.extend(targets)\n",
    "            self.unknown_words.extend(unknown_words)\n",
    "            self.decoder_inputs.extend(decoder_inputs)\n",
    "            articles = add_padding(articles)\n",
    "            targets = add_padding(targets)\n",
    "            decoder_inputs = add_padding(decoder_inputs)\n",
    "            mask = calculate_mask(articles)\n",
    "            self.batches.append( (articles, targets, mask, decoder_inputs) )\n",
    "        print(len(self.batches))\n",
    "        print(unknown_words_cnt)\n",
    "    \n",
    "    def generator(self):\n",
    "        for batch in self.batches:\n",
    "            yield batch\n",
    "            \n",
    "    def get_random_sample(self):\n",
    "        i = random.randint(0, len(self.articles) - 1)\n",
    "        return np.array([self.articles[i]]).T, np.array(self.targets[i]), np.array([self.decoder_inputs[i]]).T, self.unknown_words[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "SENTENCE_START_ID = vocab.word2id('<s>')\n",
    "SENTENCE_END_ID = vocab.word2id('</s>')\n",
    "\n",
    "PAD_TOKEN_ID = vocab.word2id('[PAD]')\n",
    "UNKNOWN_TOKEN_ID = vocab.word2id('[UNK]')\n",
    "START_DECODING_ID = vocab.word2id('[START]')\n",
    "STOP_DECODING_ID = vocab.word2id('[STOP]')\n",
    "\n",
    "def calculate_bleu(result, target, weights): #TODO\n",
    "    if not isinstance(result, list):\n",
    "        result = result.tolist()\n",
    "    if STOP_DECODING_ID in result:\n",
    "        result = result[:result.index(STOP_DECODING_ID)]\n",
    "    if not isinstance(target, list):\n",
    "        target = target.tolist()\n",
    "    if STOP_DECODING_ID in target:\n",
    "        target = target[:target.index(STOP_DECODING_ID)]\n",
    "    while PAD_TOKEN_ID in target:\n",
    "        target.remove(PAD_TOKEN_ID)\n",
    "    BLEUscore = sentence_bleu([target], result, weights=weights)\n",
    "    return BLEUscore\n",
    "    \n",
    "def calculate_bleu_ngramm(result, target, n):\n",
    "    cnt = 0\n",
    "    for i in range(len(result) - n + 1):\n",
    "        if check_ngramm_in_string(result[i:i+n], target):\n",
    "            cnt += 1\n",
    "    return cnt / (len(result) - n + 1)\n",
    "    \n",
    "def check_ngramm_in_string(ngramm, target):\n",
    "    for i in range(len(target) - len(ngramm) + 1):\n",
    "        flag = True\n",
    "        for j in range(len(ngramm)):\n",
    "            flag = flag and (ngramm[j] == target[i + j])\n",
    "        if flag:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def calculate_logits_bleu_and_rouge(logits, target, weights):\n",
    "    result = logits.argmax(-1).cpu().detach().numpy().T\n",
    "    \n",
    "    target = target.T\n",
    "    #print(result)\n",
    "    #print(target)\n",
    "    bleu = 0.\n",
    "    rouge = 0.\n",
    "    for i in range(result.shape[0]):\n",
    "        bleu += calculate_bleu(result[i], target[i], weights)\n",
    "        rouge += calculate_bleu(target[i], result[i], weights)\n",
    "    return bleu / result.shape[0], rouge / result.shape[0]\n",
    "\n",
    "def generate_summary(article, model, beam_width):\n",
    "    symbols = [START_DECODING_ID]\n",
    "    beam_ids = [0]\n",
    "    probs = [1.]\n",
    "    result = np.array([[]*beam_width])\n",
    "    X_batch = LongTensor(article)\n",
    "    inputs = InputLayerState(\"input\", False, X_batch)\n",
    "    model.eval_run_encoder(inputs)\n",
    "    for i in range(40):\n",
    "        hidden = model.decode((LongTensor([symbols]), LongTensor(beam_ids)))\n",
    "        new_probs = []\n",
    "        new_result = []\n",
    "        for i, s in enumerate(symbols):\n",
    "            values, indices = hidden[i].topk(beam_width)\n",
    "            new_probs.extend(((values + 1) * probs[i]).cpu().detach().tolist())\n",
    "            for j in range(beam_width):\n",
    "                tmp = result[i].tolist()\n",
    "                tmp.append(indices[j].item())\n",
    "                new_result.append(tmp)\n",
    "        top_idx = np.argsort(new_probs)[-beam_width:]\n",
    "        probs = np.array(new_probs)[top_idx]\n",
    "        result = np.array(new_result)[top_idx]\n",
    "        symbols = result[:,-1]\n",
    "        beam_ids = top_idx // beam_width\n",
    "        #symbol[0] == STOP_DECODING_ID\n",
    "    return result[0]\n",
    "\n",
    "def gen_and_print_summary(batcher, model, beam_width):\n",
    "    article_text, target, decoder_input, unk_words = batcher.get_random_sample()\n",
    "    result = generate_summary(article_text, model, beam_width)\n",
    "    result = outputids2words(result, vocab, unk_words)\n",
    "    target = outputids2words(target, vocab, unk_words)\n",
    "    print('result is \\n' + result)\n",
    "    print('target is \\n' + target)\n",
    "    print('BLEU = {:.4f}'.format(sentence_bleu([target], result, weights=(0.33, 0.33, 0.33, 0))))\n",
    "    \n",
    "def gen_and_print_summary_by_target(batcher, model):\n",
    "    article_text, target, decoder_inputs, unk_words = batcher.get_random_sample()\n",
    "    X_batch, y_batch, decoder_batch = LongTensor(article_text), LongTensor(target), LongTensor(decoder_inputs)\n",
    "    inputs = InputLayerState(\"input\", True, X_batch)\n",
    "    targetLayer = InputLayerState(\"target\", True, decoder_batch)\n",
    "    logits = model.train_run(inputs, targetLayer)\n",
    "    \n",
    "\n",
    "    print(calculate_logits_bleu_and_rouge(logits, np.array([target]).T, (0.33,0.33,0.33,0)))\n",
    "    result = logits.argmax(-1).squeeze(0).squeeze(-1).cpu().detach().tolist()\n",
    "    \n",
    "    result = outputids2words(result, vocab, unk_words)\n",
    "    target = outputids2words(target, vocab, unk_words)\n",
    "    print('result is \\n' + result)\n",
    "    print('target is \\n' + target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_f1(tp, fp, tn, fn):\n",
    "    precision = tp/(fp + tp)\n",
    "    recall = tp/(tp + fn)\n",
    "    f1 = 2*precision*recall/(precision+recall)\n",
    "    return f1\n",
    "\n",
    "def precalc_f1(articles_tokens, articles, target):\n",
    "    mask = calculate_mask(articles_tokens).T\n",
    "    result = (articles > 0.5)\n",
    "    #print(result[0])\n",
    "    #print(articles.shape, target.shape, mask.shape)\n",
    "    n_res = np.logical_not(result)\n",
    "    n_tar = np.logical_not(target)\n",
    "    tp = (result * target * mask).sum()\n",
    "    fp = (n_res * target * mask).sum()\n",
    "    tn = (n_res * n_tar * mask).sum()\n",
    "    fn = (result * n_tar * mask).sum()\n",
    "    return tp, fp, tn, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n",
      "146\n"
     ]
    }
   ],
   "source": [
    "train_data=Batcher(\"finished_files/chunked/train_000.bin\", 8, 1000, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown generated\n",
      "result is \n",
      "wright lowest researcher dramatic lengthy swam rodgers annie priscilla stemming recruiting stabbing rev. border wa recruiting bronze urgency wiltshire wiltshire wiltshire pleasant indigenous morality developmental serena lawsuit lawsuit life carrington tampering corps tearful emissions failings recruiting stabbing rev. border wa\n",
      "target is \n",
      "debate focuses on health care , iraq , economy . clinton , obama keep it cordial , draw comparisons to . john edwards . clinton pushes experience , obama pushes judgment . candidate mike gravel did n't meet criteria , was n't invited [STOP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "BLEU = 0.2686\n"
     ]
    }
   ],
   "source": [
    "gen_and_print_summary(train_data, model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, bleu_weights, optimizer=None):  \n",
    "    epoch_loss = 0.\n",
    "    bleu = 0.\n",
    "    rouge = 0.\n",
    "    batch_cnt = 1\n",
    "    is_train = not optimizer is None\n",
    "    model.train(is_train)\n",
    "\n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        for i, (article_text, target, mask, decoder_inputs) in enumerate(data.generator()):\n",
    "            batch_cnt =  i + 1\n",
    "            X_batch, y_batch, decoder_batch = LongTensor(article_text), LongTensor(target), LongTensor(decoder_inputs)\n",
    "            inputs = InputLayerState(\"input\", True, X_batch)\n",
    "            targetLayer = InputLayerState(\"target\", True, decoder_batch)\n",
    "            logits = model.train_run(inputs, targetLayer)\n",
    "        \n",
    "            #print(logits.view(1, -1).shape)\n",
    "            loss = criterion(logits.view(-1, logits.shape[-1]), y_batch.view(-1))\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            cur_bleu, cur_rouge = calculate_logits_bleu_and_rouge(logits, target, bleu_weights)\n",
    "            \n",
    "            bleu += cur_bleu\n",
    "            rouge += cur_rouge\n",
    "            \n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                #nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "                optimizer.step()\n",
    "            \n",
    "            print('\\r[{}]: Loss = {:.4f}, BLEU = {:.4f}, ROUGE = {:.4f}'.format(i, loss.item(), cur_bleu, cur_rouge), end='')\n",
    "    \n",
    "    print()\n",
    "    gen_and_print_summary(data, model, 10)\n",
    "    #gen_and_print_summary_by_target(data, model)\n",
    "    return epoch_loss, bleu / batch_cnt, rouge / batch_cnt\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, \n",
    "        batch_size=32, val_data=None, val_batch_size=None):\n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    bleu_weights = (0.5, 0.5, 0, 0)\n",
    "    for epoch in range(epochs_count):\n",
    "        start_time = time.time()\n",
    "        train_loss, bleu, rouge = do_epoch(model, criterion, train_data, batch_size, bleu_weights, optimizer)\n",
    "        output_info = '\\rEpoch {} / {}, Epoch Time = {:.2f}s: Train Loss = {:.4f}: BLEU = {:.4f}, ROUGE = {:.4f}'\n",
    "        if not val_data is None:\n",
    "            val_loss, bleu, rouge = do_epoch(model, criterion, val_data, val_batch_size, bleu_weights, None)\n",
    "            epoch_time = time.time() - start_time\n",
    "            output_info += ', Val Loss = {:.4f}'\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss, bleu, rouge, val_loss))\n",
    "        else:\n",
    "            epoch_time = time.time() - start_time\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss, bleu, rouge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n",
      "146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\python36\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\python36\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[124]: Loss = 7.0678, BLEU = 0.0016, ROUGE = 0.0213\n",
      "result is \n",
      "new new : new : [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] in\n",
      "target is \n",
      "new : john edwards calls for immediate withdrawal of ,000 troops . sen. hillary clinton says withdrawal of 30,000 [STOP]\n",
      "BLEU = 0.0657\n",
      "Epoch 1 / 50, Epoch Time = 79.97s: Train Loss = 1005.4756: BLEU = 0.0034, ROUGE = 0.0068\n",
      "[8]: Loss = 6.6315, BLEU = 0.0000, ROUGE = 0.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-7e9ead809faa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m fit(model, criterion, optimizer, epochs_count=50, batch_size=32, train_data=Batcher(\"finished_files/chunked/train_000.bin\",8, 40, 20),\n\u001b[1;32m----> 5\u001b[1;33m     val_data=None, val_batch_size=32)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-cb1a9beb23c9>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(model, criterion, optimizer, train_data, epochs_count, batch_size, val_data, val_batch_size)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbleu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrouge\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdo_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbleu_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[0moutput_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\rEpoch {} / {}, Epoch Time = {:.2f}s: Train Loss = {:.4f}: BLEU = {:.4f}, ROUGE = {:.4f}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mval_data\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-cb1a9beb23c9>\u001b[0m in \u001b[0;36mdo_epoch\u001b[1;34m(model, criterion, data, batch_size, bleu_weights, optimizer)\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m                 \u001b[1;31m#nn.utils.clip_grad_norm_(model.parameters(), 1.)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = vocab.word2id(PAD_TOKEN)).cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, epochs_count=50, batch_size=32, train_data=Batcher(\"finished_files/chunked/train_000.bin\",8, 40, 20),\n",
    "    val_data=None, val_batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"first_try_decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, criterion, optimizer, epochs_count=50, batch_size=32, train_data=\"finished_files/chunked/train_000.bin\",\n",
    "    val_data=\"finished_files/chunked/val_000.bin\", val_batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
