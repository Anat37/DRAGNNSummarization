{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.cuda import FloatTensor, LongTensor\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "\n",
    "PAD_TOKEN = '[PAD]' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNKNOWN_TOKEN = '[UNK]' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "START_DECODING = '[START]' # This has a vocab id, which is used at the start of every decoder input sequence\n",
    "STOP_DECODING = '[STOP]' # This has a vocab id, which is used at the end of untruncated target sequences\n",
    "\n",
    "VOCAB_SIZE = 20000\n",
    "ADDITIONAL_WORDS = 100\n",
    "\n",
    "class Vocab(object):\n",
    "\n",
    "  def __init__(self, vocab_file, max_size):\n",
    "    self._word_to_id = {}\n",
    "    self._id_to_word = {}\n",
    "    self._count = 0 # keeps track of total number of words in the Vocab\n",
    "\n",
    "    # [UNK], [PAD], [START] and [STOP] get the ids 0,1,2,3.\n",
    "    for w in [UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
    "      self._word_to_id[w] = self._count\n",
    "      self._id_to_word[self._count] = w\n",
    "      self._count += 1\n",
    "\n",
    "    # Read the vocab file and add words up to max_size\n",
    "    with open(vocab_file, 'r') as vocab_f:\n",
    "      for line in vocab_f:\n",
    "        pieces = line.split()\n",
    "        if len(pieces) != 2:\n",
    "          print('Warning: incorrectly formatted line in vocabulary file: %s\\n' % line)\n",
    "          continue\n",
    "        w = pieces[0]\n",
    "        if w in [SENTENCE_START, SENTENCE_END, UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
    "          raise Exception('<s>, </s>, [UNK], [PAD], [START] and [STOP] shouldn\\'t be in the vocab file, but %s is' % w)\n",
    "        if w in self._word_to_id:\n",
    "          raise Exception('Duplicated word in vocabulary file: %s' % w)\n",
    "        self._word_to_id[w] = self._count\n",
    "        self._id_to_word[self._count] = w\n",
    "        self._count += 1\n",
    "        if max_size != 0 and self._count >= max_size:\n",
    "          print(\"max_size of vocab was specified as %i; we now have %i words. Stopping reading.\" % (max_size, self._count))\n",
    "          break\n",
    "\n",
    "    print(\"Finished constructing vocabulary of %i total words. Last word added: %s\" % (self._count, self._id_to_word[self._count-1]))\n",
    "\n",
    "  def word2id(self, word):\n",
    "    if word not in self._word_to_id:\n",
    "      return self._word_to_id[UNKNOWN_TOKEN]\n",
    "    return self._word_to_id[word]\n",
    "\n",
    "  def id2word(self, word_id):\n",
    "    if word_id not in self._id_to_word:\n",
    "      raise ValueError('Id not found in vocab: %d' % word_id)\n",
    "    return self._id_to_word[word_id]\n",
    "\n",
    "  def size(self):\n",
    "    return self._count\n",
    "\n",
    "  def write_metadata(self, fpath):\n",
    "    print(\"Writing word embedding metadata file to %s...\" % (fpath))\n",
    "    with open(fpath, \"w\") as f:\n",
    "      fieldnames = ['word']\n",
    "      writer = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=fieldnames)\n",
    "      for i in xrange(self.size()):\n",
    "        writer.writerow({\"word\": self._id_to_word[i]})\n",
    "        \n",
    "\n",
    "\n",
    "def article2ids(article_words, vocab):\n",
    "    ids = []\n",
    "    oovs = []\n",
    "    unk_id = vocab.word2id(UNKNOWN_TOKEN)\n",
    "    for w in article_words.split():\n",
    "        w = str(w)\n",
    "        i = vocab.word2id(w)\n",
    "        if i == unk_id: # If w is OOV\n",
    "            if w not in oovs: # Add to list of OOVs\n",
    "                oovs.append(w)\n",
    "            oov_num = oovs.index(w) # This is 0 for the first article OOV, 1 for the second article OOV...\n",
    "            if oov_num < ADDITIONAL_WORDS:\n",
    "                ids.append(vocab.size() + oov_num) # This is e.g. 50000 for the first article OOV, 50001 for the second...\n",
    "        else:\n",
    "            ids.append(i)\n",
    "    return ids, oovs\n",
    "\n",
    "\n",
    "def abstract2ids(abstract_words, vocab, article_oovs):\n",
    "    ids = []\n",
    "    unk_id = vocab.word2id(UNKNOWN_TOKEN)\n",
    "    for w in abstract_words.split():\n",
    "        i = vocab.word2id(w)\n",
    "        if i == unk_id: # If w is an OOV word\n",
    "            if w in article_oovs: # If w is an in-article OOV\n",
    "                vocab_idx = vocab.size() + article_oovs.index(w) # Map to its temporary article OOV number\n",
    "                if vocab_idx < VOCAB_SIZE + ADDITIONAL_WORDS:\n",
    "                    ids.append(vocab_idx)\n",
    "        else:\n",
    "            ids.append(i)\n",
    "    return ids\n",
    "\n",
    "def outputids2words(id_list, vocab, article_oovs):\n",
    "    words = []\n",
    "    for i in id_list:\n",
    "        if i < VOCAB_SIZE:\n",
    "            w = vocab.id2word(i)\n",
    "        else:\n",
    "            article_oov_idx = i - vocab.size()\n",
    "            w = article_oovs[article_oov_idx]\n",
    "        words.append(w)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "from tensorflow.core.example import example_pb2\n",
    "\n",
    "def example_gen(filename):\n",
    "    reader = open(filename, 'rb')\n",
    "    examples = []\n",
    "    while True:\n",
    "        len_bytes = reader.read(8)\n",
    "        if not len_bytes: break # finished reading this file\n",
    "        str_len = struct.unpack('q', len_bytes)[0]\n",
    "        example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
    "        e = example_pb2.Example.FromString(example_str)\n",
    "        examples.append(e)\n",
    "        \n",
    "    for e in examples:  \n",
    "        article_text = e.features.feature['article'].bytes_list.value[0]\n",
    "        abstract_text = e.features.feature['abstract'].bytes_list.value[0]\n",
    "        yield (article_text.decode('utf-8'), abstract_text.decode('utf-8'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_size of vocab was specified as 20000; we now have 20000 words. Stopping reading.\n",
      "Finished constructing vocabulary of 20000 total words. Last word added: then-president\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(\"finished_files/vocab\", VOCAB_SIZE)\n",
    "SENTENCE_START_ID = vocab.word2id('<s>')\n",
    "SENTENCE_END_ID = vocab.word2id('</s>')\n",
    "\n",
    "PAD_TOKEN_ID = vocab.word2id('[PAD]')\n",
    "UNKNOWN_TOKEN_ID = vocab.word2id('[UNK]')\n",
    "START_DECODING_ID = vocab.word2id('[START]')\n",
    "STOP_DECODING_ID = vocab.word2id('[STOP]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoder import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17142784"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from decoder import *\n",
    "from pointerTBRU import PointerTBRU, BeamSearchProviderTBRU\n",
    "\n",
    "def build_decoder_model():\n",
    "    master = DRAGNNDecoderMaster()\n",
    "    embeddings_computer = EmbeddingComputer(VOCAB_SIZE + ADDITIONAL_WORDS, 100, PAD_TOKEN_ID)\n",
    "    master.add_component_encoder(TBRU(\"embed\", TaggerRecurrent(\"input\", \"embed\", False), embeddings_computer, (1,), True).cuda())\n",
    "    #master.add_component(TBRU(\"extractive\", TaggerRecurrent(\"embed\", \"extractive\"), TaggerComputer(1000, 1000), (1,), True).cuda())\n",
    "    master.add_component_encoder(TBRU(\"rnn\", RNNSolidRecurrent(\"embed\", \"rnn\"), RNNSolidComputer(100, 50), (1,), True).cuda())\n",
    "    \n",
    "    master.add_component_decoder(TBRU(\"decoder_embed\", TaggerRecurrent(None, \"decoder_embed\", True),embeddings_computer, (1,), True).cuda())\n",
    "    master.add_component_decoder(BeamSearchProviderTBRU(\"beamProvider\",None, \"decoder\", True, False))\n",
    "    pTBRU = PointerTBRU(\"decoder\", (1,), True, 100, 100, 100, \"rnn\", \"decoder_embed\", False)\n",
    "    master.add_component_decoder(pTBRU.cuda())\n",
    "    master.add_component_decoder(TBRU(\"output\", TaggerRecurrent(\"decoder\", \"output\", False), TaggerComputer(100, VOCAB_SIZE), (1,), True).cuda())\n",
    "    return master\n",
    "\n",
    "model = build_decoder_model()\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(articles):\n",
    "    lens = [len(article) for article in articles]\n",
    "    max_len = max(lens)\n",
    "    \n",
    "    for i in range(len(articles)):\n",
    "        articles[i].extend([vocab.word2id(PAD_TOKEN)]*(max_len - len(articles[i])))\n",
    "    return np.array(articles).T\n",
    "\n",
    "def add_padding_for_tagging(articles, targets):\n",
    "    lens = [len(article) for article in articles]\n",
    "    max_len = max(lens)\n",
    "    \n",
    "    for i in range(len(articles)):\n",
    "        targets[i].extend([0]*(max_len - len(articles[i])))\n",
    "        articles[i].extend([vocab.word2id(PAD_TOKEN)]*(max_len - len(articles[i])))\n",
    "    return np.array(articles).T, np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def calculate_mask(articles):\n",
    "    mask = (articles == vocab.word2id(PAD_TOKEN))\n",
    "    mask = np.logical_xor(mask, np.ones(articles.shape))\n",
    "    return mask\n",
    "\n",
    "def get_target(self, article, abstract):\n",
    "    return [ int(i in abstract and i != vocab.word2id(UNKNOWN_TOKEN)) for i in article]\n",
    "\n",
    "def push_abs_ptr(article, abstract, i, abs_ptr):\n",
    "    while abs_ptr < len(abstract) and (not abstract[abs_ptr] in article[i+1:] \n",
    "                                               or abstract[abs_ptr] == vocab.word2id(SENTENCE_START)\n",
    "                                               or abstract[abs_ptr] == vocab.word2id(SENTENCE_STOP)\n",
    "                                               or abstract[abs_ptr] == vocab.word2id(UNKNOWN_TOKEN)):\n",
    "        abs_ptr += 1\n",
    "    return abs_ptr\n",
    "\n",
    "class Batcher():\n",
    "    \n",
    "    def __init__(self, filename, batch_size, max_article_len, max_target_len):\n",
    "        self.batch_size = batch_size\n",
    "        generator = example_gen(filename)\n",
    "\n",
    "        self.batches = []\n",
    "        unknown_words_cnt = 0\n",
    "        self.articles = []\n",
    "        self.targets = []\n",
    "        self.unknown_words = []\n",
    "        self.decoder_inputs = []\n",
    "        while True:\n",
    "            articles = []\n",
    "            targets = []\n",
    "            unknown_words = []\n",
    "            decoder_inputs = []\n",
    "            for i in range(batch_size):\n",
    "                try:\n",
    "                    article_text, abstract_text = next(generator)\n",
    "                    article_ids, unknown_words_list = article2ids(article_text, vocab)\n",
    "                    article_ids = article_ids[:max_article_len]\n",
    "                    target = abstract2ids(abstract_text, vocab, unknown_words_list)\n",
    "                    target = target[:max_target_len - 1]\n",
    "                    target.append(STOP_DECODING_ID)\n",
    "                    decoder_input = [START_DECODING_ID]\n",
    "                    decoder_input.extend(abstract2ids(abstract_text, vocab, unknown_words_list))\n",
    "                    decoder_input = decoder_input[:max_target_len]\n",
    "                    articles.append(article_ids)\n",
    "                    targets.append(target)\n",
    "                    decoder_inputs.append(decoder_input)\n",
    "                    unknown_words.append(unknown_words_list)\n",
    "                    unknown_words_cnt = max(len(unknown_words_list), unknown_words_cnt)\n",
    "                except:\n",
    "                    break\n",
    "            if len(articles) == 0:\n",
    "                break\n",
    "            self.articles.extend(articles)\n",
    "            self.targets.extend(targets)\n",
    "            self.unknown_words.extend(unknown_words)\n",
    "            self.decoder_inputs.extend(decoder_inputs)\n",
    "            articles = add_padding(articles)\n",
    "            targets = add_padding(targets)\n",
    "            decoder_inputs = add_padding(decoder_inputs)\n",
    "            mask = calculate_mask(articles)\n",
    "            self.batches.append( (articles, targets, mask, decoder_inputs) )\n",
    "        print(len(self.batches))\n",
    "        print(unknown_words_cnt)\n",
    "    \n",
    "    def generator(self):\n",
    "        for batch in self.batches:\n",
    "            yield batch\n",
    "            \n",
    "    def get_random_sample(self):\n",
    "        i = random.randint(0, len(self.articles) - 1)\n",
    "        return np.array([self.articles[i]]).T, np.array(self.targets[i]), np.array([self.decoder_inputs[i]]).T, self.unknown_words[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "SENTENCE_START_ID = vocab.word2id('<s>')\n",
    "SENTENCE_END_ID = vocab.word2id('</s>')\n",
    "\n",
    "PAD_TOKEN_ID = vocab.word2id('[PAD]')\n",
    "UNKNOWN_TOKEN_ID = vocab.word2id('[UNK]')\n",
    "START_DECODING_ID = vocab.word2id('[START]')\n",
    "STOP_DECODING_ID = vocab.word2id('[STOP]')\n",
    "\n",
    "def calculate_bleu(result, target): #TODO\n",
    "    if not isinstance(result, list):\n",
    "        result = result.tolist()\n",
    "    while STOP_DECODING_ID in result:\n",
    "        result.remove(STOP_DECODING_ID)\n",
    "    while PAD_TOKEN_ID in result:\n",
    "        result.remove(PAD_TOKEN_ID)\n",
    "    if not isinstance(target, list):\n",
    "        target = target.tolist()\n",
    "    while STOP_DECODING_ID in target:\n",
    "        target.remove(STOP_DECODING_ID)\n",
    "    while PAD_TOKEN_ID in target:\n",
    "        target.remove(PAD_TOKEN_ID)\n",
    "    BLEUscore = sentence_bleu([target], result, weights=(1, 0, 0, 0))\n",
    "    return BLEUscore\n",
    "    \n",
    "def calculate_bleu_ngramm(result, target, n):\n",
    "    cnt = 0\n",
    "    for i in range(len(result) - n + 1):\n",
    "        if check_ngramm_in_string(result[i:i+n], target):\n",
    "            cnt += 1\n",
    "    return cnt / (len(result) - n + 1)\n",
    "    \n",
    "def check_ngramm_in_string(ngramm, target):\n",
    "    for i in range(len(target) - len(ngramm) + 1):\n",
    "        flag = True\n",
    "        for j in range(len(ngramm)):\n",
    "            flag = flag and (ngramm[j] == target[i + j])\n",
    "        if flag:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def calculate_logits_bleu_and_rouge(logits, target):\n",
    "    result = logits.argmax(-1).cpu().detach().numpy().T\n",
    "    \n",
    "    target = target.T\n",
    "    #print(result)\n",
    "    #print(target)\n",
    "    bleu = 0.\n",
    "    rouge = 0.\n",
    "    for i in range(result.shape[0]):\n",
    "        bleu += calculate_bleu(result[i], target[i])\n",
    "        rouge += calculate_bleu(target[i], result[i])\n",
    "    return bleu / result.shape[0], rouge / result.shape[0]\n",
    "\n",
    "def generate_summary(article, model, beam_width):\n",
    "    symbols = [START_DECODING_ID]\n",
    "    beam_ids = [0]\n",
    "    probs = [1.]\n",
    "    result = np.array([[]*beam_width])\n",
    "    X_batch = LongTensor(article)\n",
    "    inputs = InputLayerState(\"input\", False, X_batch)\n",
    "    model.eval_run_encoder(inputs)\n",
    "    for i in range(40):\n",
    "        hidden = model.decode((LongTensor([symbols]), LongTensor(beam_ids)))\n",
    "        new_probs = []\n",
    "        new_result = []\n",
    "        for i, s in enumerate(symbols):\n",
    "            values, indices = hidden[i].topk(beam_width)\n",
    "            new_probs.extend(((values + 1) * probs[i]).cpu().detach().tolist())\n",
    "            for j in range(beam_width):\n",
    "                tmp = result[i].tolist()\n",
    "                tmp.append(indices[j].item())\n",
    "                new_result.append(tmp)\n",
    "        top_idx = np.argsort(new_probs)[-beam_width:]\n",
    "        probs = np.array(new_probs)[top_idx]\n",
    "        result = np.array(new_result)[top_idx]\n",
    "        symbols = result[:,-1]\n",
    "        beam_ids = top_idx // beam_width\n",
    "        #symbol[0] == STOP_DECODING_ID\n",
    "    return result[0]\n",
    "\n",
    "def gen_and_print_summary(batcher, model, beam_width):\n",
    "    article_text, target, decoder_input, unk_words = batcher.get_random_sample()\n",
    "    result = generate_summary(article_text, model, beam_width)\n",
    "    result = outputids2words(result, vocab, unk_words)\n",
    "    target = outputids2words(target, vocab, unk_words)\n",
    "    print('result is \\n' + result)\n",
    "    print('target is \\n' + target)\n",
    "    print('BLEU = {:.4f}'.format(sentence_bleu([target], result, weights=(1, 0, 0, 0))))\n",
    "    \n",
    "def gen_and_print_summary_by_target(batcher, model):\n",
    "    article_text, target, decoder_inputs, unk_words = batcher.get_random_sample()\n",
    "    X_batch, y_batch, decoder_batch = LongTensor(article_text), LongTensor(target), LongTensor(decoder_inputs)\n",
    "    inputs = InputLayerState(\"input\", True, X_batch)\n",
    "    targetLayer = InputLayerState(\"target\", True, decoder_batch)\n",
    "    logits = model.train_run(inputs, targetLayer)\n",
    "    \n",
    "\n",
    "    print(calculate_logits_bleu_and_rouge(logits, np.array([target]).T))\n",
    "    result = logits.argmax(-1).squeeze(0).squeeze(-1).cpu().detach().tolist()\n",
    "    \n",
    "    result = outputids2words(result, vocab, unk_words)\n",
    "    target = outputids2words(target, vocab, unk_words)\n",
    "    print('result is \\n' + result)\n",
    "    print('target is \\n' + target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_f1(tp, fp, tn, fn):\n",
    "    precision = tp/(fp + tp)\n",
    "    recall = tp/(tp + fn)\n",
    "    f1 = 2*precision*recall/(precision+recall)\n",
    "    return f1\n",
    "\n",
    "def precalc_f1(articles_tokens, articles, target):\n",
    "    mask = calculate_mask(articles_tokens).T\n",
    "    result = (articles > 0.5)\n",
    "    #print(result[0])\n",
    "    #print(articles.shape, target.shape, mask.shape)\n",
    "    n_res = np.logical_not(result)\n",
    "    n_tar = np.logical_not(target)\n",
    "    tp = (result * target * mask).sum()\n",
    "    fp = (n_res * target * mask).sum()\n",
    "    tn = (n_res * n_tar * mask).sum()\n",
    "    fn = (result * n_tar * mask).sum()\n",
    "    return tp, fp, tn, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n",
      "146\n"
     ]
    }
   ],
   "source": [
    "train_data=Batcher(\"finished_files/chunked/train_000.bin\", 8, 1000, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 0.0)\n",
      "result is \n",
      "au captured 850 clearing bmw expected dejan recommended suffocated stimulation peggy overseen beverage presley swimwear crimestoppers haiti inspecting crowley mogadishu type issa brawl couples standoff leftist dunne traveler announcements larry marriott lead integrate additions doubted slater uniquely juan bloomberg emirates fluids frein lifeguard sheep pioneer osman represent scratching announcements bailout\n",
      "target is \n",
      "new : weapons , ammunition found with dead drug lord , colombian government says . new : defense minister warns drug dealers will `` end up in the jail or in a tomb '' victor manuel mejia munera linked to paramilitaries , also wanted in the u.s. the slain [STOP]\n"
     ]
    }
   ],
   "source": [
    "gen_and_print_summary_by_target(train_data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44725248"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, optimizer=None):  \n",
    "    epoch_loss = 0.\n",
    "    bleu = 0.\n",
    "    rouge = 0.\n",
    "    batch_cnt = 1\n",
    "    is_train = not optimizer is None\n",
    "    model.train(is_train)\n",
    "\n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        for i, (article_text, target, mask, decoder_inputs) in enumerate(data.generator()):\n",
    "            batch_cnt =  i + 1\n",
    "            X_batch, y_batch, decoder_batch = LongTensor(article_text), LongTensor(target), LongTensor(decoder_inputs)\n",
    "            inputs = InputLayerState(\"input\", True, X_batch)\n",
    "            targetLayer = InputLayerState(\"target\", True, decoder_batch)\n",
    "            logits = model.train_run(inputs, targetLayer)\n",
    "        \n",
    "            #print(logits.view(1, -1).shape)\n",
    "            loss = criterion(logits.view(-1, logits.shape[-1]), y_batch.view(-1))\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            cur_bleu, cur_rouge = calculate_logits_bleu_and_rouge(logits, target)\n",
    "            \n",
    "            bleu += cur_bleu\n",
    "            rouge += cur_rouge\n",
    "            \n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                #nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "                optimizer.step()\n",
    "            \n",
    "            print('\\r[{}]: Loss = {:.4f}, BLEU = {:.4f}, ROUGE = {:.4f}'.format(i, loss.item(), cur_bleu, cur_rouge), end='')\n",
    "    \n",
    "    print()\n",
    "    gen_and_print_summary(data, model, 10)\n",
    "    #gen_and_print_summary_by_target(data, model)\n",
    "    return epoch_loss, bleu / batch_cnt, rouge / batch_cnt\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, \n",
    "        batch_size=32, val_data=None, val_batch_size=None):\n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        start_time = time.time()\n",
    "        train_loss, bleu, rouge = do_epoch(model, criterion, train_data, batch_size, optimizer)\n",
    "        output_info = '\\rEpoch {} / {}, Epoch Time = {:.2f}s: Train Loss = {:.4f}: BLEU = {:.4f}, ROUGE = {:.4f}'\n",
    "        if not val_data is None:\n",
    "            val_loss, bleu, rouge = do_epoch(model, criterion, val_data, val_batch_size, None)\n",
    "            epoch_time = time.time() - start_time\n",
    "            output_info += ', Val Loss = {:.4f}'\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss, bleu, rouge, val_loss))\n",
    "        else:\n",
    "            epoch_time = time.time() - start_time\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss, bleu, rouge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n",
      "146\n",
      "\r",
      "[0]: Loss = 9.9063, BLEU = 0.0000, ROUGE = 0.0000\r",
      "[1]: Loss = 9.9055, BLEU = 0.0000, ROUGE = 0.0000\r",
      "[2]: Loss = 9.9065, BLEU = 0.0000, ROUGE = 0.0000\r",
      "[3]: Loss = 9.8936, BLEU = 0.0063, ROUGE = 0.0062\r",
      "[4]: Loss = 9.8963, BLEU = 0.0000, ROUGE = 0.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\nltk\\translate\\bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\python36\\lib\\site-packages\\nltk\\translate\\bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\python36\\lib\\site-packages\\nltk\\translate\\bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[124]: Loss = 6.9564, BLEU = 0.0004, ROUGE = 0.0329\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "index_select(): argument 'input' (position 1) must be Tensor, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-7e9ead809faa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m fit(model, criterion, optimizer, epochs_count=50, batch_size=32, train_data=Batcher(\"finished_files/chunked/train_000.bin\",8, 40, 20),\n\u001b[1;32m----> 5\u001b[1;33m     val_data=None, val_batch_size=32)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-91f207360bb8>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(model, criterion, optimizer, train_data, epochs_count, batch_size, val_data, val_batch_size)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbleu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrouge\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdo_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[0moutput_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\rEpoch {} / {}, Epoch Time = {:.2f}s: Train Loss = {:.4f}: BLEU = {:.4f}, ROUGE = {:.4f}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mval_data\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-91f207360bb8>\u001b[0m in \u001b[0;36mdo_epoch\u001b[1;34m(model, criterion, data, batch_size, optimizer)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mgen_and_print_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[1;31m#gen_and_print_summary_by_target(data, model)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbleu\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatch_cnt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrouge\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatch_cnt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-8354367c1715>\u001b[0m in \u001b[0;36mgen_and_print_summary\u001b[1;34m(batcher, model, beam_width)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgen_and_print_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatcher\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeam_width\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[0marticle_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munk_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_random_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeam_width\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputids2words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munk_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputids2words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munk_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-8354367c1715>\u001b[0m in \u001b[0;36mgenerate_summary\u001b[1;34m(article, model, beam_width)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_run_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msymbols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeam_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[0mnew_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0mnew_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\NIR\\drag\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, symbol)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msymbol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder_input_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msymbol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_decoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mLSTMEncoderComputer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\NIR\\drag\\decoder.py\u001b[0m in \u001b[0;36mstep_decoder\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\NIR\\drag\\pointerTBRU.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, state, net)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[0mwork_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworking_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[0mbeam_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_last_beam_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m         \u001b[0mwork_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrearrange_last\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeam_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\NIR\\drag\\pointerTBRU.py\u001b[0m in \u001b[0;36mrearrange_last\u001b[1;34m(self, ids)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrearrange_last\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrearrange_last\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mnew_hiddens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistrib\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_hiddens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: index_select(): argument 'input' (position 1) must be Tensor, not NoneType"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = vocab.word2id(PAD_TOKEN)).cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, epochs_count=50, batch_size=32, train_data=Batcher(\"finished_files/chunked/train_000.bin\",8, 40, 20),\n",
    "    val_data=None, val_batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"first_try_decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, criterion, optimizer, epochs_count=50, batch_size=32, train_data=\"finished_files/chunked/train_000.bin\",\n",
    "    val_data=\"finished_files/chunked/val_000.bin\", val_batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
