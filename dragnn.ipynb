{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.cuda import FloatTensor, LongTensor\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetState():\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.components = []\n",
    "    \n",
    "    def add_layer(self, component):\n",
    "        self.components.append(component)\n",
    "        \n",
    "    def get_value_by_name(self, name, index, module_name):\n",
    "        for c in self.components:\n",
    "            if c.name == name:\n",
    "                return c.get(index, module_name)\n",
    "            \n",
    "    def get_full(self, name, module_name):\n",
    "        for c in self.components:\n",
    "            if c.name == name:\n",
    "                return c.get_full(module_name)\n",
    "        \n",
    "    def add(self, hidden, name):\n",
    "        for c in self.components:\n",
    "            if c.name == name:\n",
    "                c.add(hidden)\n",
    "                return\n",
    "        \n",
    "class ComponentLayerState():\n",
    "    def __init__(self, name, is_solid):\n",
    "        self.name = name\n",
    "        self.is_solid = is_solid\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.pos = {}\n",
    "        self.hiddens = []\n",
    "    \n",
    "    def get(self, index, module_name):\n",
    "        if not module_name in self.pos:\n",
    "            self.pos[module_name] = -1\n",
    "        if index > 0:\n",
    "            self.pos[module_name] += 1\n",
    "            if self.pos[module_name] >= len(self.hiddens):\n",
    "                return None\n",
    "            else:\n",
    "                return self.hiddens[self.pos[module_name]]\n",
    "    \n",
    "    def get_full(self, module_name):\n",
    "        if not module_name in self.pos:\n",
    "            self.pos[module_name] = len(self.hiddens)\n",
    "            return self.hiddens\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def add(self, token):\n",
    "        if self.is_solid:\n",
    "            self.hiddens = token\n",
    "        else:\n",
    "            self.hiddens.append(token)\n",
    "            \n",
    "class InputLayerState(ComponentLayerState):\n",
    "    def __init__(self, name, is_solid, inputs):\n",
    "        super().__init__(name, is_solid)\n",
    "        self.hiddens = inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNComputer(nn.Module):\n",
    "    def __init__(self, hidden_size, input_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._hidden_size = hidden_size\n",
    "        self._hidden = nn.Linear(hidden_size + input_size, hidden_size)\n",
    "\n",
    "    def forward(self, state, input_token):\n",
    "        inputs, hidden = input_token\n",
    "        if inputs is None:\n",
    "            return state, None\n",
    "        if hidden is None:\n",
    "            hidden = inputs.new_zeros(inputs.size(0), self._hidden_size)\n",
    "        x = torch.cat((hidden, inputs), -1)\n",
    "        hidden = torch.tanh(self._hidden(x))\n",
    "\n",
    "        return state, hidden\n",
    "    \n",
    "class RNNSolidComputer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._rnn = nn.LSTM(input_size, hidden_size, bidirectional=True)\n",
    "\n",
    "    def forward(self, state, input_token):\n",
    "        if input_token is None:\n",
    "            return state, None\n",
    "        #if hidden is None:\n",
    "         #   hidden = inputs.new_zeros(inputs.size(0), self._hidden_size)\n",
    "        output, hidden = self._rnn(input_token, None)\n",
    "\n",
    "        return state, output\n",
    "    \n",
    "class TaggerComputer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._hidden_size = hidden_size\n",
    "        self._hidden = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, state, input_token):\n",
    "        if input_token is None:\n",
    "            return state, None\n",
    "        hidden = self._hidden(input_token)\n",
    "        return state, hidden\n",
    "    \n",
    "class EmbeddingComputer(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._embed = nn.Embedding(vocab_size, hidden_size)\n",
    "\n",
    "    def forward(self, state, input_token):\n",
    "        if input_token is None:\n",
    "            return state, None\n",
    "        hidden = self._embed(input_token)\n",
    "        return state, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNRecurrent():\n",
    "    def __init__(self, input_name, self_name):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._input_name = input_name\n",
    "        self._self_name = self_name\n",
    "\n",
    "    def get(self, state, net):\n",
    "        inputs = net.get_value_by_name(self._input_name, 1, self._self_name)\n",
    "        hidden = net.get_value_by_name(self._self_name, 1, self._self_name)\n",
    "        return inputs, hidden\n",
    "    \n",
    "class RNNSolidRecurrent():\n",
    "    def __init__(self, input_name, self_name):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._input_name = input_name\n",
    "        self._self_name = self_name\n",
    "\n",
    "    def get(self, state, net):\n",
    "        inputs = net.get_full(self._input_name, self._self_name)\n",
    "        return inputs\n",
    "    \n",
    "class TaggerRecurrent():\n",
    "    def __init__(self, input_name, self_name):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._input_name = input_name\n",
    "        self._self_name = self_name\n",
    "\n",
    "    def get(self, state, net):\n",
    "        inputs = net.get_full(self._input_name, self._self_name)\n",
    "        #inputs = net.get_value_by_name(self._input_name,1, self._self_name)\n",
    "        if isinstance(inputs, list):\n",
    "            inputs = torch.stack(inputs)\n",
    "            inputs.requires_grad_()\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TBRU(nn.Module):\n",
    "    def __init__(self, name, recurrent, computer, state_shape, is_solid):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.is_solid = is_solid\n",
    "        self.name = name\n",
    "        self.state_shape = state_shape\n",
    "        self._rec = recurrent\n",
    "        self._comp = computer\n",
    "\n",
    "    def forward(self, state, net):\n",
    "        state, hidden = self._comp(state, (self._rec.get(state, net)))\n",
    "        if hidden is not None:\n",
    "            net.add(hidden, self.name)\n",
    "        return state, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MasterComponent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.comp = []\n",
    "\n",
    "    def add_component(self, component):\n",
    "        self.add_module(component.name, component)\n",
    "        \n",
    "    def prepare_net(self, net):\n",
    "        for c in self._modules:\n",
    "            comp_layer = ComponentLayerState(self._modules[c].name, self._modules[c].is_solid)\n",
    "            net.add_layer(comp_layer)\n",
    "        return net\n",
    "        \n",
    "    def forward(self, net):\n",
    "        for c in self._modules:\n",
    "            module = self._modules[c]\n",
    "            state, hidden = module(np.zeros(module.state_shape), net)\n",
    "            while hidden is not None:\n",
    "                state, hidden = module(state, net)\n",
    "                \n",
    "        return net\n",
    "\n",
    "class DRAGNNMaster():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = NetState()\n",
    "        self.main = MasterComponent().cuda()\n",
    "        \n",
    "    def add_component(self, component):\n",
    "        self.main.add_component(component)\n",
    "\n",
    "    def build_net(self, input_layer):\n",
    "        if self.net is not None:\n",
    "            del self.net\n",
    "            self.net = NetState()\n",
    "        self.net.reset()\n",
    "        self.net.add_layer(input_layer)\n",
    "        self.main.prepare_net(self.net)\n",
    "        \n",
    "    def forward(self, input_layer):\n",
    "        self.build_net(input_layer)\n",
    "        self.net = self.main(self.net)\n",
    "        output = self.net.components[-1].hiddens\n",
    "        if self.net.components[-1].is_solid:\n",
    "            return output\n",
    "        else:\n",
    "            output = torch.stack(output)\n",
    "            output.requires_grad_()\n",
    "            return output\n",
    "    \n",
    "    def save_model(self, filename):\n",
    "        torch.save(self.main.state_dict(), filename)\n",
    "        \n",
    "    def load_model(self, filename):\n",
    "        self.main.load_state_dict(torch.load(filename))\n",
    "        \n",
    "    def save_checkpoint(self, epoch, optimizer, filename='checkpoint.pth.tar'):\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': self.main.state_dict(),\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "\n",
    "PAD_TOKEN = '[PAD]' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNKNOWN_TOKEN = '[UNK]' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "START_DECODING = '[START]' # This has a vocab id, which is used at the start of every decoder input sequence\n",
    "STOP_DECODING = '[STOP]' # This has a vocab id, which is used at the end of untruncated target sequences\n",
    "\n",
    "\n",
    "class Vocab(object):\n",
    "\n",
    "  def __init__(self, vocab_file, max_size):\n",
    "    self._word_to_id = {}\n",
    "    self._id_to_word = {}\n",
    "    self._count = 0 # keeps track of total number of words in the Vocab\n",
    "\n",
    "    # [UNK], [PAD], [START] and [STOP] get the ids 0,1,2,3.\n",
    "    for w in [UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
    "      self._word_to_id[w] = self._count\n",
    "      self._id_to_word[self._count] = w\n",
    "      self._count += 1\n",
    "\n",
    "    # Read the vocab file and add words up to max_size\n",
    "    with open(vocab_file, 'r') as vocab_f:\n",
    "      for line in vocab_f:\n",
    "        pieces = line.split()\n",
    "        if len(pieces) != 2:\n",
    "          print('Warning: incorrectly formatted line in vocabulary file: %s\\n' % line)\n",
    "          continue\n",
    "        w = pieces[0]\n",
    "        if w in [SENTENCE_START, SENTENCE_END, UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
    "          raise Exception('<s>, </s>, [UNK], [PAD], [START] and [STOP] shouldn\\'t be in the vocab file, but %s is' % w)\n",
    "        if w in self._word_to_id:\n",
    "          raise Exception('Duplicated word in vocabulary file: %s' % w)\n",
    "        self._word_to_id[w] = self._count\n",
    "        self._id_to_word[self._count] = w\n",
    "        self._count += 1\n",
    "        if max_size != 0 and self._count >= max_size:\n",
    "          print(\"max_size of vocab was specified as %i; we now have %i words. Stopping reading.\" % (max_size, self._count))\n",
    "          break\n",
    "\n",
    "    print(\"Finished constructing vocabulary of %i total words. Last word added: %s\" % (self._count, self._id_to_word[self._count-1]))\n",
    "\n",
    "  def word2id(self, word):\n",
    "    if word not in self._word_to_id:\n",
    "      return self._word_to_id[UNKNOWN_TOKEN]\n",
    "    return self._word_to_id[word]\n",
    "\n",
    "  def id2word(self, word_id):\n",
    "    if word_id not in self._id_to_word:\n",
    "      raise ValueError('Id not found in vocab: %d' % word_id)\n",
    "    return self._id_to_word[word_id]\n",
    "\n",
    "  def size(self):\n",
    "    return self._count\n",
    "\n",
    "  def write_metadata(self, fpath):\n",
    "    print(\"Writing word embedding metadata file to %s...\" % (fpath))\n",
    "    with open(fpath, \"w\") as f:\n",
    "      fieldnames = ['word']\n",
    "      writer = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=fieldnames)\n",
    "      for i in xrange(self.size()):\n",
    "        writer.writerow({\"word\": self._id_to_word[i]})\n",
    "        \n",
    "\n",
    "\n",
    "def article2ids(article_words, vocab):\n",
    "    ids = []\n",
    "    oovs = []\n",
    "    unk_id = vocab.word2id(UNKNOWN_TOKEN)\n",
    "    for w in article_words.split():\n",
    "        w = str(w)\n",
    "        i = vocab.word2id(w)\n",
    "        if i == unk_id: # If w is OOV\n",
    "            if w not in oovs: # Add to list of OOVs\n",
    "                oovs.append(w)\n",
    "                oov_num = oovs.index(w) # This is 0 for the first article OOV, 1 for the second article OOV...\n",
    "                #ids.append(vocab.size() + oov_num) # This is e.g. 50000 for the first article OOV, 50001 for the second...\n",
    "        else:\n",
    "            ids.append(i)\n",
    "    return ids, oovs\n",
    "\n",
    "\n",
    "def abstract2ids(abstract_words, vocab, article_oovs):\n",
    "  ids = []\n",
    "  unk_id = vocab.word2id(UNKNOWN_TOKEN)\n",
    "  for w in abstract_words:\n",
    "    i = vocab.word2id(w)\n",
    "    if i == unk_id: # If w is an OOV word\n",
    "      if w in article_oovs: # If w is an in-article OOV\n",
    "        vocab_idx = vocab.size() + article_oovs.index(w) # Map to its temporary article OOV number\n",
    "        #ids.append(vocab_idx)\n",
    "      else: # If w is an out-of-article OOV\n",
    "        ids.append(unk_id) # Map to the UNK token id\n",
    "    else:\n",
    "      ids.append(i)\n",
    "  return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "from tensorflow.core.example import example_pb2\n",
    "\n",
    "def example_gen(filename):\n",
    "    reader = open(filename, 'rb')\n",
    "    examples = []\n",
    "    while True:\n",
    "        len_bytes = reader.read(8)\n",
    "        if not len_bytes: break # finished reading this file\n",
    "        str_len = struct.unpack('q', len_bytes)[0]\n",
    "        example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
    "        e = example_pb2.Example.FromString(example_str)\n",
    "        examples.append(e)\n",
    "        \n",
    "    for e in examples:  \n",
    "        article_text = e.features.feature['article'].bytes_list.value[0]\n",
    "        abstract_text = e.features.feature['abstract'].bytes_list.value[0]\n",
    "        yield (article_text.decode('utf-8'), abstract_text.decode('utf-8'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_size of vocab was specified as 10000; we now have 10000 words. Stopping reading.\n",
      "Finished constructing vocabulary of 10000 total words. Last word added: obligations\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(\"finished_files/vocab\", 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tagger_model():\n",
    "    master = DRAGNNMaster()\n",
    "    master.add_component(TBRU(\"embed\", TaggerRecurrent(\"input\", \"embed\"), EmbeddingComputer(10000, 1000), (1,), True).cuda())\n",
    "    master.add_component(TBRU(\"extractive\", TaggerRecurrent(\"embed\", \"extractive\"), TaggerComputer(1000, 1000), (1,), True).cuda())\n",
    "    master.add_component(TBRU(\"rnn\", RNNSolidRecurrent(\"extractive\", \"rnn\"), RNNSolidComputer(1000, 500), (1,), True).cuda())\n",
    "    \n",
    "    master.add_component(TBRU(\"extractive2\", TaggerRecurrent(\"rnn\", \"extractive2\"), TaggerComputer(1000, 1), (1,), True).cuda())\n",
    "    return master\n",
    "\n",
    "model = build_tagger_model()\n",
    "#torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(articles, targets):\n",
    "    lens = [len(article) for article in articles]\n",
    "    max_len = max(lens)\n",
    "    \n",
    "    for i in range(len(articles)):\n",
    "        targets[i].extend([0]*(max_len - len(articles[i])))\n",
    "        articles[i].extend([vocab.word2id('PAD_TOKEN')]*(max_len - len(articles[i])))\n",
    "    return np.array(articles).T, np.array(targets)\n",
    "\n",
    "def iterate_batches(filename, batch_size):\n",
    "    generator = example_gen(filename)\n",
    "    while True:\n",
    "        articles = []\n",
    "        targets = []\n",
    "        for i in range(batch_size):\n",
    "            try:\n",
    "                article_text, abstract_text = next(generator)\n",
    "                article_ids, _ = article2ids(article_text, vocab)\n",
    "                abstract_ids, _ = article2ids(abstract_text, vocab)\n",
    "                target = [ int(i in abstract_ids) for i in article_ids]\n",
    "                \n",
    "                articles.append(article_ids)\n",
    "                targets.append(target)\n",
    "            except:\n",
    "                break\n",
    "        if len(articles) == 0:\n",
    "            break\n",
    "        yield add_padding(articles, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mask(articles):\n",
    "    mask = (articles == vocab.word2id('PAD_TOKEN'))\n",
    "    mask = np.logical_xor(mask, np.ones(articles.shape))\n",
    "    return mask\n",
    "\n",
    "def calc_f1(tp, fp, tn, fn):\n",
    "    precision = tp/(fp+tp)\n",
    "    recall = tp/(tp + fn)\n",
    "    f1 = 2*precision*recall/(precision+recall)\n",
    "    return f1\n",
    "\n",
    "def precalc_f1(articles_tokens, articles, target):\n",
    "    mask = calculate_mask(articles_tokens).T\n",
    "    result = (articles > 0.5)\n",
    "    #print(result[0])\n",
    "    #print(articles.shape, target.shape, mask.shape)\n",
    "    n_res = np.logical_not(result)\n",
    "    n_tar = np.logical_not(target)\n",
    "    tp = (result * target * mask).sum()\n",
    "    fp = (n_res * target * mask).sum()\n",
    "    tn = (n_res * n_tar * mask).sum()\n",
    "    fn = (result * n_tar * mask).sum()\n",
    "    return tp, fp, tn, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, optimizer=None):  \n",
    "    epoch_loss = 0.\n",
    "    tp, fp, tn, fn = 0, 0, 0, 0\n",
    "    is_train = not optimizer is None\n",
    "    model.main.train(is_train)\n",
    "\n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        for i, (article_text, target) in enumerate(iterate_batches(data, batch_size)):\n",
    "            X_batch, y_batch = LongTensor(article_text), FloatTensor(target)\n",
    "            inputs = InputLayerState(\"input\", False, X_batch)\n",
    "            logits = model.forward(inputs)\n",
    "            \n",
    "            logits = logits.squeeze(-1)\n",
    "            loss = criterion(logits.transpose(0,1), y_batch)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                #nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "                optimizer.step()\n",
    "            \n",
    "            tpb, fpb, tnb, fnb = precalc_f1(article_text, logits.cpu().detach().numpy().T, target)\n",
    "            tp += tpb\n",
    "            fp += fpb\n",
    "            tn += tnb\n",
    "            fn += fnb\n",
    "            f1 = calc_f1(tp,fp,tn,fn)\n",
    "            print('\\r[{}]: Loss = {:.4f}, F1 = {:.4f}'.format(i, loss.item(), f1), end='')\n",
    "             \n",
    "    f1 = calc_f1(tp,fp,tn,fn)            \n",
    "    return epoch_loss, f1\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, \n",
    "        batch_size=32, val_data=None, val_batch_size=None):\n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        start_time = time.time()\n",
    "        train_loss, f1 = do_epoch(model, criterion, train_data, batch_size, optimizer)\n",
    "        \n",
    "        output_info = '\\rEpoch {} / {}, Epoch Time = {:.2f}s: Train Loss = {:.4f}: F1-Score = {:.4f}'\n",
    "        if not val_data is None:\n",
    "            val_loss, f1 = do_epoch(model, criterion, val_data, val_batch_size, None)\n",
    "            \n",
    "            epoch_time = time.time() - start_time\n",
    "            output_info += ', Val Loss = {:.4f}'\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss, f1, val_loss))\n",
    "        else:\n",
    "            epoch_time = time.time() - start_time\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2653]: Loss = 0.0672, F1 = 0.6723"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss().cuda()\n",
    "optimizer = optim.Adam(model.main.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, epochs_count=50, batch_size=32, train_data=\"finished_files/train.bin\",\n",
    "    val_data=None, val_batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
