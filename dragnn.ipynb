{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.cuda import FloatTensor, LongTensor\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetState():\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.components = []\n",
    "    \n",
    "    def add_layer(self, component):\n",
    "        self.components.append(component)\n",
    "        \n",
    "    def get_value_by_name(self, name, index, module_name):\n",
    "        for c in self.components:\n",
    "            if c.name == name:\n",
    "                return c.get(index, module_name)\n",
    "            \n",
    "    def get_full(self, name, module_name):\n",
    "        for c in self.components:\n",
    "            if c.name == name:\n",
    "                return c.get_full(module_name)\n",
    "        \n",
    "    def add(self, hidden, name):\n",
    "        for c in self.components:\n",
    "            if c.name == name:\n",
    "                c.add(hidden)\n",
    "                return\n",
    "        \n",
    "class ComponentLayerState():\n",
    "    def __init__(self, name, is_solid):\n",
    "        self.name = name\n",
    "        self.is_solid = is_solid\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.pos = {}\n",
    "        self.hiddens = []\n",
    "    \n",
    "    def get(self, index, module_name):\n",
    "        if not module_name in self.pos:\n",
    "            self.pos[module_name] = -1\n",
    "        if index > 0:\n",
    "            self.pos[module_name] += 1\n",
    "            if self.pos[module_name] >= len(self.hiddens):\n",
    "                return None\n",
    "            else:\n",
    "                return self.hiddens[self.pos[module_name]]\n",
    "    \n",
    "    def get_full(self, module_name):\n",
    "        if not module_name in self.pos:\n",
    "            self.pos[module_name] = len(self.hiddens)\n",
    "            return self.hiddens\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def add(self, token):\n",
    "        if self.is_solid:\n",
    "            self.hiddens = token\n",
    "        else:\n",
    "            self.hiddens.append(token)\n",
    "            \n",
    "class InputLayerState(ComponentLayerState):\n",
    "    def __init__(self, name, is_solid, inputs):\n",
    "        super().__init__(name, is_solid)\n",
    "        self.hiddens = inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNComputer(nn.Module):\n",
    "    def __init__(self, hidden_size, input_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._hidden_size = hidden_size\n",
    "        self._hidden = nn.Linear(hidden_size + input_size, hidden_size)\n",
    "\n",
    "    def forward(self, state, input_token):\n",
    "        inputs, hidden = input_token\n",
    "        if inputs is None:\n",
    "            return state, None\n",
    "        if hidden is None:\n",
    "            hidden = inputs.new_zeros(inputs.size(0), self._hidden_size)\n",
    "        x = torch.cat((hidden, inputs), -1)\n",
    "        hidden = torch.tanh(self._hidden(x))\n",
    "\n",
    "        return state, hidden\n",
    "    \n",
    "class RNNSolidComputer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._hidden_size = hidden_size\n",
    "        self._rnn = nn.LSTM(input_size, hidden_size, bidirectional=True)\n",
    "\n",
    "    def forward(self, state, input_token):\n",
    "        if input_token is None:\n",
    "            return state, None\n",
    "        #if hidden is None:\n",
    "        hidden = (input_token.new_zeros((2, input_token.shape[1], self._hidden_size)),\n",
    "                  input_token.new_zeros((2, input_token.shape[1], self._hidden_size)))\n",
    "        #print(input_token.shape)\n",
    "        output, hidden = self._rnn(input_token, hidden)\n",
    "\n",
    "        return state, output\n",
    "    \n",
    "class TaggerComputer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._hidden_size = hidden_size\n",
    "        self._hidden = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, state, input_token):\n",
    "        if input_token is None:\n",
    "            return state, None\n",
    "        hidden = self._hidden(input_token)\n",
    "        return state, hidden\n",
    "    \n",
    "class EmbeddingComputer(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._embed = nn.Embedding(vocab_size, hidden_size)\n",
    "\n",
    "    def forward(self, state, input_token):\n",
    "        if input_token is None:\n",
    "            return state, None\n",
    "        hidden = self._embed(input_token)\n",
    "        return state, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNRecurrent():\n",
    "    def __init__(self, input_name, self_name):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._input_name = input_name\n",
    "        self._self_name = self_name\n",
    "\n",
    "    def get(self, state, net):\n",
    "        inputs = net.get_value_by_name(self._input_name, 1, self._self_name)\n",
    "        hidden = net.get_value_by_name(self._self_name, 1, self._self_name)\n",
    "        return inputs, hidden\n",
    "    \n",
    "class RNNSolidRecurrent():\n",
    "    def __init__(self, input_name, self_name):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._input_name = input_name\n",
    "        self._self_name = self_name\n",
    "\n",
    "    def get(self, state, net):\n",
    "        inputs = net.get_full(self._input_name, self._self_name)\n",
    "        return inputs\n",
    "    \n",
    "class TaggerRecurrent():\n",
    "    def __init__(self, input_name, self_name):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._input_name = input_name\n",
    "        self._self_name = self_name\n",
    "\n",
    "    def get(self, state, net):\n",
    "        inputs = net.get_full(self._input_name, self._self_name)\n",
    "        #inputs = net.get_value_by_name(self._input_name,1, self._self_name)\n",
    "        if isinstance(inputs, list):\n",
    "            inputs = torch.stack(inputs)\n",
    "            inputs.requires_grad_()\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TBRU(nn.Module):\n",
    "    def __init__(self, name, recurrent, computer, state_shape, is_solid):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.is_solid = is_solid\n",
    "        self.name = name\n",
    "        self.state_shape = state_shape\n",
    "        self._rec = recurrent\n",
    "        self._comp = computer\n",
    "\n",
    "    def forward(self, state, net):\n",
    "        state, hidden = self._comp(state, (self._rec.get(state, net)))\n",
    "        if hidden is not None:\n",
    "            net.add(hidden, self.name)\n",
    "        return state, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MasterComponent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.comp = []\n",
    "\n",
    "    def add_component(self, component):\n",
    "        self.add_module(component.name, component)\n",
    "        \n",
    "    def prepare_net(self, net):\n",
    "        for c in self._modules:\n",
    "            comp_layer = ComponentLayerState(self._modules[c].name, self._modules[c].is_solid)\n",
    "            net.add_layer(comp_layer)\n",
    "        return net\n",
    "        \n",
    "    def forward(self, net):\n",
    "        for c in self._modules:\n",
    "            module = self._modules[c]\n",
    "            state, hidden = module(np.zeros(module.state_shape), net)\n",
    "            while hidden is not None:\n",
    "                state, hidden = module(state, net)\n",
    "                \n",
    "        return net\n",
    "\n",
    "class DRAGNNMaster():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = NetState()\n",
    "        self.main = MasterComponent().cuda()\n",
    "        \n",
    "    def add_component(self, component):\n",
    "        self.main.add_component(component)\n",
    "\n",
    "    def build_net(self, input_layer):\n",
    "        if self.net is not None:\n",
    "            del self.net\n",
    "            self.net = NetState()\n",
    "        self.net.reset()\n",
    "        self.net.add_layer(input_layer)\n",
    "        self.main.prepare_net(self.net)\n",
    "        \n",
    "    def forward(self, input_layer):\n",
    "        self.build_net(input_layer)\n",
    "        self.net = self.main(self.net)\n",
    "        output = self.net.components[-1].hiddens\n",
    "        if self.net.components[-1].is_solid:\n",
    "            return output\n",
    "        else:\n",
    "            output = torch.stack(output)\n",
    "            output.requires_grad_()\n",
    "            return output\n",
    "    \n",
    "    def save_model(self, filename):\n",
    "        torch.save(self.main.state_dict(), filename)\n",
    "        \n",
    "    def load_model(self, filename):\n",
    "        self.main.load_state_dict(torch.load(filename))\n",
    "        \n",
    "    def save_checkpoint(self, epoch, optimizer, filename='checkpoint.pth.tar'):\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': self.main.state_dict(),\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "\n",
    "PAD_TOKEN = '[PAD]' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNKNOWN_TOKEN = '[UNK]' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "START_DECODING = '[START]' # This has a vocab id, which is used at the start of every decoder input sequence\n",
    "STOP_DECODING = '[STOP]' # This has a vocab id, which is used at the end of untruncated target sequences\n",
    "\n",
    "\n",
    "class Vocab(object):\n",
    "\n",
    "  def __init__(self, vocab_file, max_size):\n",
    "    self._word_to_id = {}\n",
    "    self._id_to_word = {}\n",
    "    self._count = 0 # keeps track of total number of words in the Vocab\n",
    "\n",
    "    # [UNK], [PAD], [START] and [STOP] get the ids 0,1,2,3.\n",
    "    for w in [UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
    "      self._word_to_id[w] = self._count\n",
    "      self._id_to_word[self._count] = w\n",
    "      self._count += 1\n",
    "\n",
    "    # Read the vocab file and add words up to max_size\n",
    "    with open(vocab_file, 'r') as vocab_f:\n",
    "      for line in vocab_f:\n",
    "        pieces = line.split()\n",
    "        if len(pieces) != 2:\n",
    "          print('Warning: incorrectly formatted line in vocabulary file: %s\\n' % line)\n",
    "          continue\n",
    "        w = pieces[0]\n",
    "        if w in [SENTENCE_START, SENTENCE_END, UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
    "          raise Exception('<s>, </s>, [UNK], [PAD], [START] and [STOP] shouldn\\'t be in the vocab file, but %s is' % w)\n",
    "        if w in self._word_to_id:\n",
    "          raise Exception('Duplicated word in vocabulary file: %s' % w)\n",
    "        self._word_to_id[w] = self._count\n",
    "        self._id_to_word[self._count] = w\n",
    "        self._count += 1\n",
    "        if max_size != 0 and self._count >= max_size:\n",
    "          print(\"max_size of vocab was specified as %i; we now have %i words. Stopping reading.\" % (max_size, self._count))\n",
    "          break\n",
    "\n",
    "    print(\"Finished constructing vocabulary of %i total words. Last word added: %s\" % (self._count, self._id_to_word[self._count-1]))\n",
    "\n",
    "  def word2id(self, word):\n",
    "    if word not in self._word_to_id:\n",
    "      return self._word_to_id[UNKNOWN_TOKEN]\n",
    "    return self._word_to_id[word]\n",
    "\n",
    "  def id2word(self, word_id):\n",
    "    if word_id not in self._id_to_word:\n",
    "      raise ValueError('Id not found in vocab: %d' % word_id)\n",
    "    return self._id_to_word[word_id]\n",
    "\n",
    "  def size(self):\n",
    "    return self._count\n",
    "\n",
    "  def write_metadata(self, fpath):\n",
    "    print(\"Writing word embedding metadata file to %s...\" % (fpath))\n",
    "    with open(fpath, \"w\") as f:\n",
    "      fieldnames = ['word']\n",
    "      writer = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=fieldnames)\n",
    "      for i in xrange(self.size()):\n",
    "        writer.writerow({\"word\": self._id_to_word[i]})\n",
    "        \n",
    "\n",
    "\n",
    "def article2ids(article_words, vocab):\n",
    "    ids = []\n",
    "    oovs = []\n",
    "    unk_id = vocab.word2id(UNKNOWN_TOKEN)\n",
    "    for w in article_words.split():\n",
    "        w = str(w)\n",
    "        i = vocab.word2id(w)\n",
    "        if i == unk_id: # If w is OOV\n",
    "            if w not in oovs: # Add to list of OOVs\n",
    "                oovs.append(w)\n",
    "                oov_num = oovs.index(w) # This is 0 for the first article OOV, 1 for the second article OOV...\n",
    "                #ids.append(vocab.size() + oov_num) # This is e.g. 50000 for the first article OOV, 50001 for the second...\n",
    "        else:\n",
    "            ids.append(i)\n",
    "    return ids, oovs\n",
    "\n",
    "\n",
    "def abstract2ids(abstract_words, vocab, article_oovs):\n",
    "  ids = []\n",
    "  unk_id = vocab.word2id(UNKNOWN_TOKEN)\n",
    "  for w in abstract_words:\n",
    "    i = vocab.word2id(w)\n",
    "    if i == unk_id: # If w is an OOV word\n",
    "      if w in article_oovs: # If w is an in-article OOV\n",
    "        vocab_idx = vocab.size() + article_oovs.index(w) # Map to its temporary article OOV number\n",
    "        #ids.append(vocab_idx)\n",
    "      else: # If w is an out-of-article OOV\n",
    "        ids.append(unk_id) # Map to the UNK token id\n",
    "    else:\n",
    "      ids.append(i)\n",
    "  return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-35e05b6e549f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstruct\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexample\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexample_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mexample_gen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeature_column_lib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minitializers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcifar100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfashion_mnist\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mreuters\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_remove_long_seq\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_file\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf_logging\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\preprocessing\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mkeras_preprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_keras_submodules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras_preprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m  \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\keras_preprocessing\\image\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# flake8: noqa:F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0maffine_transformations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdataframe_iterator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataFrameIterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdirectory_iterator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDirectoryIterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\keras_preprocessing\\image\\affine_transformations.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# scipy.ndimage cannot be accessed until explicitly imported\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mndimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mscipy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\scipy\\ndimage\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mfilters\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mfourier\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0minterpolation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\scipy\\ndimage\\filters.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_ni_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_nd_image\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdoccer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_version\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNumpyVersion\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\scipy\\misc\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwho\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_who\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_source\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pade\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpade\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_pade\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m from scipy.special import (comb as _comb, logsumexp as _lsm,\n\u001b[0;32m     69\u001b[0m         factorial as _fact, factorial2 as _fact2, factorialk as _factk)\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\scipy\\interpolate\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mfitpack\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\scipy\\interpolate\\interpolate.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m                    dot, ravel, poly1d, asarray, intp)\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcomb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\scipy\\linalg\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_solvers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_procrustes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_decomp_update\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_sketches\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_decomp_update.pyx\u001b[0m in \u001b[0;36minit scipy.linalg._decomp_update\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36mmodule_from_spec\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mcreate_module\u001b[1;34m(self, spec)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import struct\n",
    "from tensorflow.core.example import example_pb2\n",
    "\n",
    "def example_gen(filename):\n",
    "    reader = open(filename, 'rb')\n",
    "    examples = []\n",
    "    while True:\n",
    "        len_bytes = reader.read(8)\n",
    "        if not len_bytes: break # finished reading this file\n",
    "        str_len = struct.unpack('q', len_bytes)[0]\n",
    "        example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
    "        e = example_pb2.Example.FromString(example_str)\n",
    "        examples.append(e)\n",
    "        \n",
    "    for e in examples:  \n",
    "        article_text = e.features.feature['article'].bytes_list.value[0]\n",
    "        abstract_text = e.features.feature['abstract'].bytes_list.value[0]\n",
    "        yield (article_text.decode('utf-8'), abstract_text.decode('utf-8'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(\"finished_files/vocab\", 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tagger_model():\n",
    "    master = DRAGNNMaster()\n",
    "    master.add_component(TBRU(\"embed\", TaggerRecurrent(\"input\", \"embed\"), EmbeddingComputer(50000, 1000), (1,), True).cuda())\n",
    "    #master.add_component(TBRU(\"extractive\", TaggerRecurrent(\"embed\", \"extractive\"), TaggerComputer(1000, 1000), (1,), True).cuda())\n",
    "    master.add_component(TBRU(\"rnn\", RNNSolidRecurrent(\"embed\", \"rnn\"), RNNSolidComputer(1000, 500), (1,), True).cuda())\n",
    "    \n",
    "    master.add_component(TBRU(\"extractive2\", TaggerRecurrent(\"rnn\", \"extractive2\"), TaggerComputer(1000, 1), (1,), True).cuda())\n",
    "    return master\n",
    "\n",
    "model = build_tagger_model()\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(articles, targets):\n",
    "    lens = [len(article) for article in articles]\n",
    "    max_len = max(lens)\n",
    "    \n",
    "    for i in range(len(articles)):\n",
    "        targets[i].extend([0]*(max_len - len(articles[i])))\n",
    "        articles[i].extend([vocab.word2id(PAD_TOKEN)]*(max_len - len(articles[i])))\n",
    "    return np.array(articles).T, np.array(targets)\n",
    "\n",
    "def iterate_batches(filename, batch_size):\n",
    "    generator = example_gen(filename)\n",
    "    while True:\n",
    "        articles = []\n",
    "        targets = []\n",
    "        for i in range(batch_size):\n",
    "            try:\n",
    "                article_text, abstract_text = next(generator)\n",
    "                article_ids, _ = article2ids(article_text, vocab)\n",
    "                abstract_ids, _ = article2ids(abstract_text, vocab)\n",
    "                target = [ int(i in abstract_ids and i != vocab.word2id(UNKNOWN_TOKEN)) for i in article_ids]\n",
    "                \n",
    "                articles.append(article_ids)\n",
    "                targets.append(target)\n",
    "            except:\n",
    "                break\n",
    "        if len(articles) == 0:\n",
    "            break\n",
    "        yield add_padding(articles, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mask(articles):\n",
    "    mask = (articles == vocab.word2id(PAD_TOKEN))\n",
    "    mask = np.logical_xor(mask, np.ones(articles.shape))\n",
    "    return mask\n",
    "\n",
    "def get_target(self, article, abstract):\n",
    "    return [ int(i in abstract and i != vocab.word2id(UNKNOWN_TOKEN)) for i in article]\n",
    "\n",
    "def push_abs_ptr(article, abstract, i, abs_ptr):\n",
    "    while abs_ptr < len(abstract) and (not abstract[abs_ptr] in article[i+1:] \n",
    "                                               or abstract[abs_ptr] == vocab.word2id(SENTENCE_START)\n",
    "                                               or abstract[abs_ptr] == vocab.word2id(SENTENCE_STOP)\n",
    "                                               or abstract[abs_ptr] == vocab.word2id(UNKNOWN_TOKEN)):\n",
    "        abs_ptr += 1\n",
    "    return abs_ptr\n",
    "\n",
    "def get_target2(self, article, abstract):\n",
    "    abs_ptr = 0\n",
    "    target = []\n",
    "    abs_ptr = push_abs_ptr(article, abstract, -1, abs_ptr)\n",
    "    if abs_ptr == len(abstract):\n",
    "        target.extend([0]* len(article))\n",
    "        break\n",
    "    for i, art in enumerate(article):\n",
    "        if art == abstract[abs_ptr]:\n",
    "            target.append(1)\n",
    "            abs_ptr += 1\n",
    "            abs_ptr = push_abs_ptr(article, abstract, i, abs_ptr)\n",
    "            if abs_ptr == len(abstract):\n",
    "                target.extend([0]* (len(article) - i + 1))\n",
    "                break\n",
    "        else:\n",
    "            target.append(0)\n",
    "    return [ int(i in abstract and i != vocab.word2id(UNKNOWN_TOKEN)) for i in article]\n",
    "\n",
    "\n",
    "class Batcher():\n",
    "    \n",
    "    def __init__(self, filename, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        generator = example_gen(filename)\n",
    "\n",
    "        self.batches = []\n",
    "        while True:\n",
    "            articles = []\n",
    "            targets = []\n",
    "            for i in range(batch_size):\n",
    "                try:\n",
    "                    article_text, abstract_text = next(generator)\n",
    "                    article_ids, _ = article2ids(article_text, vocab)\n",
    "                    abstract_ids, _ = article2ids(abstract_text, vocab)\n",
    "                    target = get_target(article, abstract)\n",
    "                    \n",
    "                    articles.append(article_ids)\n",
    "                    targets.append(target)\n",
    "                except:\n",
    "                    break\n",
    "            if len(articles) == 0:\n",
    "                break\n",
    "            articles, targets = add_padding(articles, targets)\n",
    "            mask = calculate_mask(articles)\n",
    "            batch.append( (articles, targets, mask) )\n",
    "    \n",
    "    def generator(self):\n",
    "        for batch in self.batches:\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_f1(tp, fp, tn, fn):\n",
    "    precision = tp/(fp + tp)\n",
    "    recall = tp/(tp + fn)\n",
    "    f1 = 2*precision*recall/(precision+recall)\n",
    "    return f1\n",
    "\n",
    "def precalc_f1(articles_tokens, articles, target):\n",
    "    mask = calculate_mask(articles_tokens).T\n",
    "    result = (articles > 0.5)\n",
    "    #print(result[0])\n",
    "    #print(articles.shape, target.shape, mask.shape)\n",
    "    n_res = np.logical_not(result)\n",
    "    n_tar = np.logical_not(target)\n",
    "    tp = (result * target * mask).sum()\n",
    "    fp = (n_res * target * mask).sum()\n",
    "    tn = (n_res * n_tar * mask).sum()\n",
    "    fn = (result * n_tar * mask).sum()\n",
    "    return tp, fp, tn, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, optimizer=None):  \n",
    "    epoch_loss = 0.\n",
    "    tp, fp, tn, fn = 0, 0, 0, 0\n",
    "    is_train = not optimizer is None\n",
    "    model.main.train(is_train)\n",
    "\n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        for i, (article_text, target) in enumerate(iterate_batches(data, batch_size)):\n",
    "            X_batch, y_batch = LongTensor(article_text), FloatTensor(target)\n",
    "            inputs = InputLayerState(\"input\", False, X_batch)\n",
    "            logits = model.forward(inputs)\n",
    "            \n",
    "            logits = logits.squeeze(-1)\n",
    "            loss = criterion(logits.transpose(0,1), y_batch)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                #nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "                optimizer.step()\n",
    "            \n",
    "            tpb, fpb, tnb, fnb = precalc_f1(article_text, logits.cpu().detach().numpy().T, target)\n",
    "            tp += tpb\n",
    "            fp += fpb\n",
    "            tn += tnb\n",
    "            fn += fnb\n",
    "            f1 = calc_f1(tp,fp,tn,fn)\n",
    "            print('\\r[{}]: Loss = {:.4f}, F1 = {:.4f}'.format(i, loss.item(), f1), end='')\n",
    "             \n",
    "    f1 = calc_f1(tp,fp,tn,fn)            \n",
    "    return epoch_loss, f1\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, \n",
    "        batch_size=32, val_data=None, val_batch_size=None):\n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        start_time = time.time()\n",
    "        train_loss, f1 = do_epoch(model, criterion, train_data, batch_size, optimizer)\n",
    "        \n",
    "        output_info = '\\rEpoch {} / {}, Epoch Time = {:.2f}s: Train Loss = {:.4f}: F1-Score = {:.4f}'\n",
    "        if not val_data is None:\n",
    "            val_loss, f1 = do_epoch(model, criterion, val_data, val_batch_size, None)\n",
    "            \n",
    "            epoch_time = time.time() - start_time\n",
    "            output_info += ', Val Loss = {:.4f}'\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss, f1, val_loss))\n",
    "        else:\n",
    "            epoch_time = time.time() - start_time\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss().cuda()\n",
    "optimizer = optim.Adam(model.main.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, epochs_count=50, batch_size=32, train_data=\"finished_files/train.bin\",\n",
    "    val_data=None, val_batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"first_try\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, criterion, optimizer, epochs_count=50, batch_size=32, train_data=\"finished_files/chunked/train_000.bin\",\n",
    "    val_data=\"finished_files/chunked/val_000.bin\", val_batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
