{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.cuda import FloatTensor, LongTensor\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "\n",
    "PAD_TOKEN = '[PAD]' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNKNOWN_TOKEN = '[UNK]' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "START_DECODING = '[START]' # This has a vocab id, which is used at the start of every decoder input sequence\n",
    "STOP_DECODING = '[STOP]' # This has a vocab id, which is used at the end of untruncated target sequences\n",
    "\n",
    "VOCAB_SIZE = 20000\n",
    "ADDITIONAL_WORDS = 100\n",
    "\n",
    "class Vocab(object):\n",
    "\n",
    "  def __init__(self, vocab_file, max_size):\n",
    "    self._word_to_id = {}\n",
    "    self._id_to_word = {}\n",
    "    self._count = 0 # keeps track of total number of words in the Vocab\n",
    "\n",
    "    # [UNK], [PAD], [START] and [STOP] get the ids 0,1,2,3.\n",
    "    for w in [UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
    "      self._word_to_id[w] = self._count\n",
    "      self._id_to_word[self._count] = w\n",
    "      self._count += 1\n",
    "\n",
    "    # Read the vocab file and add words up to max_size\n",
    "    with open(vocab_file, 'r') as vocab_f:\n",
    "      for line in vocab_f:\n",
    "        pieces = line.split()\n",
    "        if len(pieces) != 2:\n",
    "          print('Warning: incorrectly formatted line in vocabulary file: %s\\n' % line)\n",
    "          continue\n",
    "        w = pieces[0]\n",
    "        if w in [SENTENCE_START, SENTENCE_END, UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
    "          raise Exception('<s>, </s>, [UNK], [PAD], [START] and [STOP] shouldn\\'t be in the vocab file, but %s is' % w)\n",
    "        if w in self._word_to_id:\n",
    "          raise Exception('Duplicated word in vocabulary file: %s' % w)\n",
    "        self._word_to_id[w] = self._count\n",
    "        self._id_to_word[self._count] = w\n",
    "        self._count += 1\n",
    "        if max_size != 0 and self._count >= max_size:\n",
    "          print(\"max_size of vocab was specified as %i; we now have %i words. Stopping reading.\" % (max_size, self._count))\n",
    "          break\n",
    "\n",
    "    print(\"Finished constructing vocabulary of %i total words. Last word added: %s\" % (self._count, self._id_to_word[self._count-1]))\n",
    "\n",
    "  def word2id(self, word):\n",
    "    if word not in self._word_to_id:\n",
    "      return self._word_to_id[UNKNOWN_TOKEN]\n",
    "    return self._word_to_id[word]\n",
    "\n",
    "  def id2word(self, word_id):\n",
    "    if word_id not in self._id_to_word:\n",
    "      raise ValueError('Id not found in vocab: %d' % word_id)\n",
    "    return self._id_to_word[word_id]\n",
    "\n",
    "  def size(self):\n",
    "    return self._count\n",
    "\n",
    "  def write_metadata(self, fpath):\n",
    "    print(\"Writing word embedding metadata file to %s...\" % (fpath))\n",
    "    with open(fpath, \"w\") as f:\n",
    "      fieldnames = ['word']\n",
    "      writer = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=fieldnames)\n",
    "      for i in xrange(self.size()):\n",
    "        writer.writerow({\"word\": self._id_to_word[i]})\n",
    "        \n",
    "\n",
    "\n",
    "def article2ids(article_words, vocab):\n",
    "    ids = []\n",
    "    oovs = []\n",
    "    unk_id = vocab.word2id(UNKNOWN_TOKEN)\n",
    "    for w in article_words.split():\n",
    "        w = str(w)\n",
    "        i = vocab.word2id(w)\n",
    "        if i == unk_id: # If w is OOV\n",
    "            if w not in oovs: # Add to list of OOVs\n",
    "                oovs.append(w)\n",
    "            oov_num = oovs.index(w) # This is 0 for the first article OOV, 1 for the second article OOV...\n",
    "            if oov_num < ADDITIONAL_WORDS:\n",
    "                ids.append(vocab.size() + oov_num) # This is e.g. 50000 for the first article OOV, 50001 for the second...\n",
    "        else:\n",
    "            ids.append(i)\n",
    "    return ids, oovs\n",
    "\n",
    "\n",
    "def abstract2ids(abstract_words, vocab, article_oovs):\n",
    "    ids = []\n",
    "    unk_id = vocab.word2id(UNKNOWN_TOKEN)\n",
    "    for w in abstract_words.split():\n",
    "        i = vocab.word2id(w)\n",
    "        if i == unk_id: # If w is an OOV word\n",
    "            if w in article_oovs: # If w is an in-article OOV\n",
    "                vocab_idx = vocab.size() + article_oovs.index(w) # Map to its temporary article OOV number\n",
    "                if vocab_idx < VOCAB_SIZE + ADDITIONAL_WORDS:\n",
    "                    ids.append(vocab_idx)\n",
    "        else:\n",
    "            ids.append(i)\n",
    "    return ids\n",
    "\n",
    "def outputids2words(id_list, vocab, article_oovs):\n",
    "    words = []\n",
    "    for i in id_list:\n",
    "        if i < VOCAB_SIZE:\n",
    "            w = vocab.id2word(i)\n",
    "        else:\n",
    "            article_oov_idx = i - vocab.size()\n",
    "            print('unknown generated')\n",
    "            w = article_oovs[article_oov_idx]\n",
    "        words.append(w)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "from tensorflow.core.example import example_pb2\n",
    "\n",
    "def example_gen(filename):\n",
    "    reader = open(filename, 'rb')\n",
    "    examples = []\n",
    "    while True:\n",
    "        len_bytes = reader.read(8)\n",
    "        if not len_bytes: break # finished reading this file\n",
    "        str_len = struct.unpack('q', len_bytes)[0]\n",
    "        example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
    "        e = example_pb2.Example.FromString(example_str)\n",
    "        examples.append(e)\n",
    "        \n",
    "    for e in examples:  \n",
    "        article_text = e.features.feature['article'].bytes_list.value[0]\n",
    "        abstract_text = e.features.feature['abstract'].bytes_list.value[0]\n",
    "        yield (article_text.decode('utf-8'), abstract_text.decode('utf-8'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_size of vocab was specified as 20000; we now have 20000 words. Stopping reading.\n",
      "Finished constructing vocabulary of 20000 total words. Last word added: then-president\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(\"finished_files/vocab\", VOCAB_SIZE)\n",
    "SENTENCE_START_ID = vocab.word2id('<s>')\n",
    "SENTENCE_END_ID = vocab.word2id('</s>')\n",
    "\n",
    "PAD_TOKEN_ID = vocab.word2id('[PAD]')\n",
    "UNKNOWN_TOKEN_ID = vocab.word2id('[UNK]')\n",
    "START_DECODING_ID = vocab.word2id('[START]')\n",
    "STOP_DECODING_ID = vocab.word2id('[STOP]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoder import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24981504"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from decoder import *\n",
    "from pointerTBRU import BeamSearchProviderTBRU, LSTMTBRU, AttentionTBRU, ConcatTBRU, PointerTBRU, ContextTBRU, CoverageAttentionTBRU\n",
    "\n",
    "def build_decoder_model2():\n",
    "    master = DRAGNNDecoderMaster()\n",
    "    embeddings_computer = EmbeddingComputer(VOCAB_SIZE + ADDITIONAL_WORDS, 100, PAD_TOKEN_ID)\n",
    "    master.add_component_encoder(TBRU(\"embed\", TaggerRecurrent(\"input\", \"embed\", False), embeddings_computer, (1,), True).cuda())\n",
    "    #master.add_component(TBRU(\"extractive\", TaggerRecurrent(\"embed\", \"extractive\"), TaggerComputer(1000, 1000), (1,), True).cuda())\n",
    "    master.add_component_encoder(TBRU(\"rnn\", RNNSolidRecurrent(\"embed\", \"rnn\"), RNNSolidComputer(100, 50), (1,), True).cuda())\n",
    "    \n",
    "    master.add_component_decoder(TBRU(\"decoder_embed\", TaggerRecurrent(None, \"decoder_embed\", True),embeddings_computer, (1,), True).cuda())\n",
    "    master.add_component_decoder(BeamSearchProviderTBRU(\"beamProvider\", (1,), None, \"decoder\", True, False))\n",
    "    pTBRU = AttentiveLSTMTBRU(\"decoder\", (1,), True, 100, 100, 100, \"rnn\", \"decoder_embed\", False)\n",
    "    master.add_component_decoder(pTBRU.cuda())\n",
    "    master.add_component_decoder(TBRU(\"output\", TaggerRecurrent(\"decoder\", \"output\", False), TaggerComputer(100, VOCAB_SIZE), (1,), True).cuda())\n",
    "    return master\n",
    "\n",
    "def build_decoder_model():\n",
    "    master = DRAGNNDecoderMaster()\n",
    "    embeddings_computer = EmbeddingComputer(VOCAB_SIZE + ADDITIONAL_WORDS, 100, PAD_TOKEN_ID)\n",
    "    master.add_component_encoder(TBRU(\"embed\", TaggerRecurrent(\"input\", \"embed\", False), embeddings_computer, (1,), True).cuda())\n",
    "    #master.add_component(TBRU(\"extractive\", TaggerRecurrent(\"embed\", \"extractive\"), TaggerComputer(1000, 1000), (1,), True).cuda())\n",
    "    master.add_component_encoder(TBRU(\"rnn\", RNNSolidRecurrent(\"embed\", \"rnn\"), RNNSolidComputer(100, 50), (1,), True).cuda())\n",
    "    \n",
    "    \n",
    "    master.add_component_decoder(TBRU(\"decoder_embed\", TaggerRecurrent(None, \"decoder_embed\", True),embeddings_computer, (1,), is_solid=True).cuda())\n",
    "    \n",
    "    master.add_component_decoder(BeamSearchProviderTBRU(\"beamProvider\", None, 0, \"decoder\", True, is_solid=False))\n",
    "    master.add_component_decoder(BeamSearchProviderTBRU(\"beamProvider1\", None, 0, \"lstm_state_layer\", True, is_solid=False))\n",
    "    master.add_component_decoder(BeamSearchProviderTBRU(\"beamProvider2\", None, 1, \"coverage_layer\", True, is_solid=False))\n",
    "    \n",
    "    \n",
    "    pTBRU = LSTMTBRU(\"decoder\", \"lstm_state_layer\", False, 100, 100, \"rnn\", \"decoder_embed\", False)\n",
    "    master.add_component_decoder(pTBRU.cuda())\n",
    "    master.add_component_decoder(CoverageAttentionTBRU(\"attention_layer\", \"coverage_layer\", False, 100, 100, 100, 'decoder','rnn', is_first=False, solid_modifiable=False).cuda())\n",
    "    master.add_component_decoder(ContextTBRU('context', False, 'attention_layer', 'rnn', is_first=False, solid_modifiable=False))\n",
    "    master.add_component_decoder(ConcatTBRU('context_concat', False, 'decoder', 'context', is_first=False, solid_modifiable=False))\n",
    "    master.add_component_decoder(TBRU(\"output\", TaggerRecurrent(\"context_concat\", \"output\", False), TaggerComputer(200, VOCAB_SIZE), (1,), is_solid=False, solid_modifiable=False).cuda())\n",
    "    master.add_component_decoder(PointerTBRU(\"pointer_final\", False, 300, VOCAB_SIZE + ADDITIONAL_WORDS, \"attention_layer\", \"context\", \"output\", \"decoder_embed\", \"input\", \"decoder\", solid_modifiable=False).cuda())\n",
    "    return master\n",
    "\n",
    "model = build_decoder_model()\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(articles):\n",
    "    lens = [len(article) for article in articles]\n",
    "    max_len = max(lens)\n",
    "    \n",
    "    for i in range(len(articles)):\n",
    "        articles[i].extend([vocab.word2id(PAD_TOKEN)]*(max_len - len(articles[i])))\n",
    "    return np.array(articles).T\n",
    "\n",
    "def add_padding_for_tagging(articles, targets):\n",
    "    lens = [len(article) for article in articles]\n",
    "    max_len = max(lens)\n",
    "    \n",
    "    for i in range(len(articles)):\n",
    "        targets[i].extend([0]*(max_len - len(articles[i])))\n",
    "        articles[i].extend([vocab.word2id(PAD_TOKEN)]*(max_len - len(articles[i])))\n",
    "    return np.array(articles).T, np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def calculate_mask(articles):\n",
    "    mask = (articles == vocab.word2id(PAD_TOKEN))\n",
    "    mask = np.logical_xor(mask, np.ones(articles.shape))\n",
    "    return mask\n",
    "\n",
    "def get_target(self, article, abstract):\n",
    "    return [ int(i in abstract and i != vocab.word2id(UNKNOWN_TOKEN)) for i in article]\n",
    "\n",
    "def push_abs_ptr(article, abstract, i, abs_ptr):\n",
    "    while abs_ptr < len(abstract) and (not abstract[abs_ptr] in article[i+1:] \n",
    "                                               or abstract[abs_ptr] == vocab.word2id(SENTENCE_START)\n",
    "                                               or abstract[abs_ptr] == vocab.word2id(SENTENCE_STOP)\n",
    "                                               or abstract[abs_ptr] == vocab.word2id(UNKNOWN_TOKEN)):\n",
    "        abs_ptr += 1\n",
    "    return abs_ptr\n",
    "\n",
    "class Batcher():\n",
    "    \n",
    "    def __init__(self, filename, batch_size, max_article_len, max_target_len):\n",
    "        self.batch_size = batch_size\n",
    "        generator = example_gen(filename)\n",
    "\n",
    "        self.batches = []\n",
    "        unknown_words_cnt = 0\n",
    "        self.articles = []\n",
    "        self.targets = []\n",
    "        self.unknown_words = []\n",
    "        self.decoder_inputs = []\n",
    "        while True:\n",
    "            articles = []\n",
    "            targets = []\n",
    "            unknown_words = []\n",
    "            decoder_inputs = []\n",
    "            for i in range(batch_size):\n",
    "                try:\n",
    "                    \n",
    "                    article_text, abstract_text = next(generator)\n",
    "                    article_ids, unknown_words_list = article2ids(article_text, vocab)\n",
    "                    art_len = min(max_article_len, len(article_ids))\n",
    "                    article_ids = article_ids[:art_len]\n",
    "                    target = abstract2ids(abstract_text, vocab, unknown_words_list)\n",
    "                    tar_len = min(max_target_len, len(target))\n",
    "                    target = target[:tar_len - 1]\n",
    "                    target.append(STOP_DECODING_ID)\n",
    "                    decoder_input = [START_DECODING_ID]\n",
    "                    decoder_input.extend(abstract2ids(abstract_text, vocab, unknown_words_list))\n",
    "                    decoder_input = decoder_input[:tar_len]\n",
    "                    articles.append(article_ids)\n",
    "                    targets.append(target)\n",
    "                    decoder_inputs.append(decoder_input)\n",
    "                    unknown_words.append(unknown_words_list)\n",
    "                    unknown_words_cnt = max(len(unknown_words_list), unknown_words_cnt)\n",
    "                except:\n",
    "                    break\n",
    "            if len(articles) == 0:\n",
    "                break\n",
    "            self.articles.extend(articles)\n",
    "            self.targets.extend(targets)\n",
    "            self.unknown_words.extend(unknown_words)\n",
    "            self.decoder_inputs.extend(decoder_inputs)\n",
    "            articles = add_padding(articles)\n",
    "            targets = add_padding(targets)\n",
    "            decoder_inputs = add_padding(decoder_inputs)\n",
    "            mask = calculate_mask(articles)\n",
    "            self.batches.append( (articles, targets, mask, decoder_inputs) )\n",
    "        print(len(self.batches))\n",
    "        print(unknown_words_cnt)\n",
    "    \n",
    "    def generator(self):\n",
    "        for batch in self.batches:\n",
    "            yield batch\n",
    "            \n",
    "    def get_random_sample(self):\n",
    "        i = random.randint(0, len(self.articles) - 1)\n",
    "        return np.array([self.articles[i]]).T, np.array(self.targets[i]), np.array([self.decoder_inputs[i]]).T, self.unknown_words[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "SENTENCE_START_ID = vocab.word2id('<s>')\n",
    "SENTENCE_END_ID = vocab.word2id('</s>')\n",
    "\n",
    "PAD_TOKEN_ID = vocab.word2id('[PAD]')\n",
    "UNKNOWN_TOKEN_ID = vocab.word2id('[UNK]')\n",
    "START_DECODING_ID = vocab.word2id('[START]')\n",
    "STOP_DECODING_ID = vocab.word2id('[STOP]')\n",
    "\n",
    "def calculate_bleu(result, target, weights): #TODO\n",
    "    if not isinstance(result, list):\n",
    "        result = result.tolist()\n",
    "    if STOP_DECODING_ID in result:\n",
    "        result = result[:result.index(STOP_DECODING_ID)]\n",
    "    if not isinstance(target, list):\n",
    "        target = target.tolist()\n",
    "    if STOP_DECODING_ID in target:\n",
    "        target = target[:target.index(STOP_DECODING_ID)]\n",
    "    while PAD_TOKEN_ID in target:\n",
    "        target.remove(PAD_TOKEN_ID)\n",
    "    BLEUscore = sentence_bleu([target], result, weights=weights)\n",
    "    return BLEUscore\n",
    "    \n",
    "def calculate_bleu_ngramm(result, target, n):\n",
    "    cnt = 0\n",
    "    for i in range(len(result) - n + 1):\n",
    "        if check_ngramm_in_string(result[i:i+n], target):\n",
    "            cnt += 1\n",
    "    return cnt / (len(result) - n + 1)\n",
    "    \n",
    "def check_ngramm_in_string(ngramm, target):\n",
    "    for i in range(len(target) - len(ngramm) + 1):\n",
    "        flag = True\n",
    "        for j in range(len(ngramm)):\n",
    "            flag = flag and (ngramm[j] == target[i + j])\n",
    "        if flag:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def calculate_logits_bleu_and_rouge(logits, target, weights):\n",
    "    result = logits.argmax(-1).cpu().detach().numpy().T\n",
    "    \n",
    "    target = target.T\n",
    "    #print(result)\n",
    "    #print(target)\n",
    "    bleu = 0.\n",
    "    rouge = 0.\n",
    "    for i in range(result.shape[0]):\n",
    "        bleu += calculate_bleu(result[i], target[i], weights)\n",
    "        rouge += calculate_bleu(target[i], result[i], weights)\n",
    "    return bleu / result.shape[0], rouge / result.shape[0]\n",
    "\n",
    "def generate_summary(article, model, beam_width):\n",
    "    symbols = [START_DECODING_ID]\n",
    "    beam_ids = [0]\n",
    "    probs = [1.]\n",
    "    result = np.array([[]*beam_width])\n",
    "    X_batch = LongTensor(article)\n",
    "    inputs = InputLayerState(\"input\", False, X_batch)\n",
    "    model.eval_run_encoder(inputs)\n",
    "    for i in range(40):\n",
    "        hidden = model.decode((LongTensor([symbols]), LongTensor(beam_ids)))\n",
    "        new_probs = []\n",
    "        new_result = []\n",
    "        for i, s in enumerate(symbols):\n",
    "            values, indices = hidden[i].topk(beam_width)\n",
    "            new_probs.extend(((values + 1) * probs[i]).cpu().detach().tolist())\n",
    "            for j in range(beam_width):\n",
    "                tmp = result[i].tolist()\n",
    "                tmp.append(indices[j].item())\n",
    "                new_result.append(tmp)\n",
    "        top_idx = np.argsort(new_probs)[-beam_width:]\n",
    "        probs = np.array(new_probs)[top_idx]\n",
    "        result = np.array(new_result)[top_idx]\n",
    "        symbols = result[:,-1]\n",
    "        beam_ids = top_idx // beam_width\n",
    "        #symbol[0] == STOP_DECODING_ID\n",
    "    return result[0]\n",
    "\n",
    "def gen_and_print_summary(batcher, model, beam_width):\n",
    "    article_text, target, decoder_input, unk_words = batcher.get_random_sample()\n",
    "    result = generate_summary(article_text, model, beam_width)\n",
    "    result = outputids2words(result, vocab, unk_words)\n",
    "    target = outputids2words(target, vocab, unk_words)\n",
    "    print('result is \\n' + result)\n",
    "    print('target is \\n' + target)\n",
    "    print('BLEU = {:.4f}'.format(sentence_bleu([target], result, weights=(0.33, 0.33, 0.33, 0))))\n",
    "    \n",
    "def gen_and_print_summary_by_target(batcher, model):\n",
    "    article_text, target, decoder_inputs, unk_words = batcher.get_random_sample()\n",
    "    X_batch, y_batch, decoder_batch = LongTensor(article_text), LongTensor(target), LongTensor(decoder_inputs)\n",
    "    inputs = InputLayerState(\"input\", True, X_batch)\n",
    "    targetLayer = InputLayerState(\"target\", True, decoder_batch)\n",
    "    logits = model.train_run(inputs, targetLayer)\n",
    "    \n",
    "\n",
    "    print(calculate_logits_bleu_and_rouge(logits, np.array([target]).T, (0.33,0.33,0.33,0)))\n",
    "    result = logits.argmax(-1).squeeze(0).squeeze(-1).cpu().detach().tolist()\n",
    "    \n",
    "    result = outputids2words(result, vocab, unk_words)\n",
    "    target = outputids2words(target, vocab, unk_words)\n",
    "    print('result is \\n' + result)\n",
    "    print('target is \\n' + target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_f1(tp, fp, tn, fn):\n",
    "    precision = tp/(fp + tp)\n",
    "    recall = tp/(tp + fn)\n",
    "    f1 = 2*precision*recall/(precision+recall)\n",
    "    return f1\n",
    "\n",
    "def precalc_f1(articles_tokens, articles, target):\n",
    "    mask = calculate_mask(articles_tokens).T\n",
    "    result = (articles > 0.5)\n",
    "    #print(result[0])\n",
    "    #print(articles.shape, target.shape, mask.shape)\n",
    "    n_res = np.logical_not(result)\n",
    "    n_tar = np.logical_not(target)\n",
    "    tp = (result * target * mask).sum()\n",
    "    fp = (n_res * target * mask).sum()\n",
    "    tn = (n_res * n_tar * mask).sum()\n",
    "    fn = (result * n_tar * mask).sum()\n",
    "    return tp, fp, tn, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n",
      "146\n"
     ]
    }
   ],
   "source": [
    "train_data=Batcher(\"finished_files/chunked/train_000.bin\", 8, 1000, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100])\n",
      "torch.Size([1, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10, 100])\n",
      "unknown generated\n",
      "result is \n",
      "[PAD] arbitrary chorley cole tend [PAD] scary cross-country held held dismissed dismissed wounding sofa stocks dress representatives sphere [PAD] plummeted premiered [PAD] victim arbitrary lifelong understand 65-year-old censorship cellar cellar jumpers cellar jumpers cellar jumpers cellar jumpers cellar jumpers responsibility\n",
      "target is \n",
      "harry potter star daniel radcliffe gets # 20m fortune as he turns 18 monday . young actor says he has no plans to fritter his cash away . radcliffe 's earnings from first five potter films have been held in trust fund [STOP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "BLEU = 0.3087\n"
     ]
    }
   ],
   "source": [
    "gen_and_print_summary(train_data, model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, bleu_weights, optimizer=None):  \n",
    "    epoch_loss = 0.\n",
    "    bleu = 0.\n",
    "    rouge = 0.\n",
    "    batch_cnt = 1\n",
    "    is_train = not optimizer is None\n",
    "    model.train(is_train)\n",
    "\n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        for i, (article_text, target, mask, decoder_inputs) in enumerate(data.generator()):\n",
    "            batch_cnt =  i + 1\n",
    "            X_batch, y_batch, decoder_batch = LongTensor(article_text), LongTensor(target), LongTensor(decoder_inputs)\n",
    "            inputs = InputLayerState(\"input\", True, X_batch)\n",
    "            targetLayer = InputLayerState(\"target\", True, decoder_batch)\n",
    "            logits = model.train_run(inputs, targetLayer)\n",
    "        \n",
    "            #print(logits.view(1, -1).shape)\n",
    "            loss = criterion(logits.view(-1, logits.shape[-1]), y_batch.view(-1))\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            cur_bleu, cur_rouge = calculate_logits_bleu_and_rouge(logits, target, bleu_weights)\n",
    "            \n",
    "            bleu += cur_bleu\n",
    "            rouge += cur_rouge\n",
    "            \n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                #nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "                optimizer.step()\n",
    "            \n",
    "            print('\\r[{}]: Loss = {:.4f}, BLEU = {:.4f}, ROUGE = {:.4f}'.format(i, loss.item(), cur_bleu, cur_rouge), end='')\n",
    "    \n",
    "    print()\n",
    "    gen_and_print_summary(data, model, 10)\n",
    "    #gen_and_print_summary_by_target(data, model)\n",
    "    return epoch_loss, bleu / batch_cnt, rouge / batch_cnt\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, \n",
    "        batch_size=32, val_data=None, val_batch_size=None):\n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    bleu_weights = (0.5, 0.5, 0, 0)\n",
    "    for epoch in range(epochs_count):\n",
    "        start_time = time.time()\n",
    "        train_loss, bleu, rouge = do_epoch(model, criterion, train_data, batch_size, bleu_weights, optimizer)\n",
    "        output_info = '\\rEpoch {} / {}, Epoch Time = {:.2f}s: Train Loss = {:.4f}: BLEU = {:.4f}, ROUGE = {:.4f}'\n",
    "        if not val_data is None:\n",
    "            val_loss, bleu, rouge = do_epoch(model, criterion, val_data, val_batch_size, bleu_weights, None)\n",
    "            epoch_time = time.time() - start_time\n",
    "            output_info += ', Val Loss = {:.4f}'\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss, bleu, rouge, val_loss))\n",
    "        else:\n",
    "            epoch_time = time.time() - start_time\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss, bleu, rouge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n",
      "146\n",
      "torch.Size([8, 100])\n",
      "torch.Size([8, 100])\n",
      "torch.Size([8, 100])\n",
      "torch.Size([8, 100])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (8) to match target batch_size (160).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-7e9ead809faa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m fit(model, criterion, optimizer, epochs_count=50, batch_size=32, train_data=Batcher(\"finished_files/chunked/train_000.bin\",8, 40, 20),\n\u001b[1;32m----> 5\u001b[1;33m     val_data=None, val_batch_size=32)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-cb1a9beb23c9>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(model, criterion, optimizer, train_data, epochs_count, batch_size, val_data, val_batch_size)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbleu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrouge\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdo_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbleu_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[0moutput_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\rEpoch {} / {}, Epoch Time = {:.2f}s: Train Loss = {:.4f}: BLEU = {:.4f}, ROUGE = {:.4f}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mval_data\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-cb1a9beb23c9>\u001b[0m in \u001b[0;36mdo_epoch\u001b[1;34m(model, criterion, data, batch_size, bleu_weights, optimizer)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;31m#print(logits.view(1, -1).shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    902\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    903\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m--> 904\u001b[1;33m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[0;32m    905\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   1968\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1969\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1970\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   1786\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1787\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[1;32m-> 1788\u001b[1;33m                          .format(input.size(0), target.size(0)))\n\u001b[0m\u001b[0;32m   1789\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1790\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (8) to match target batch_size (160)."
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = vocab.word2id(PAD_TOKEN)).cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, epochs_count=50, batch_size=32, train_data=Batcher(\"finished_files/chunked/train_000.bin\",8, 40, 20),\n",
    "    val_data=None, val_batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"first_try_decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, criterion, optimizer, epochs_count=50, batch_size=32, train_data=\"finished_files/chunked/train_000.bin\",\n",
    "    val_data=\"finished_files/chunked/val_000.bin\", val_batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
