{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.cuda import FloatTensor, LongTensor\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "\n",
    "PAD_TOKEN = '[PAD]' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNKNOWN_TOKEN = '[UNK]' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "START_DECODING = '[START]' # This has a vocab id, which is used at the start of every decoder input sequence\n",
    "STOP_DECODING = '[STOP]' # This has a vocab id, which is used at the end of untruncated target sequences\n",
    "\n",
    "VOCAB_SIZE = 20000\n",
    "ADDITIONAL_WORDS = 100\n",
    "\n",
    "class Vocab(object):\n",
    "\n",
    "  def __init__(self, vocab_file, max_size):\n",
    "    self._word_to_id = {}\n",
    "    self._id_to_word = {}\n",
    "    self._count = 0 # keeps track of total number of words in the Vocab\n",
    "\n",
    "    # [UNK], [PAD], [START] and [STOP] get the ids 0,1,2,3.\n",
    "    for w in [UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
    "      self._word_to_id[w] = self._count\n",
    "      self._id_to_word[self._count] = w\n",
    "      self._count += 1\n",
    "\n",
    "    # Read the vocab file and add words up to max_size\n",
    "    with open(vocab_file, 'r') as vocab_f:\n",
    "      for line in vocab_f:\n",
    "        pieces = line.split()\n",
    "        if len(pieces) != 2:\n",
    "          print('Warning: incorrectly formatted line in vocabulary file: %s\\n' % line)\n",
    "          continue\n",
    "        w = pieces[0]\n",
    "        if w in [SENTENCE_START, SENTENCE_END, UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
    "          raise Exception('<s>, </s>, [UNK], [PAD], [START] and [STOP] shouldn\\'t be in the vocab file, but %s is' % w)\n",
    "        if w in self._word_to_id:\n",
    "          raise Exception('Duplicated word in vocabulary file: %s' % w)\n",
    "        self._word_to_id[w] = self._count\n",
    "        self._id_to_word[self._count] = w\n",
    "        self._count += 1\n",
    "        if max_size != 0 and self._count >= max_size:\n",
    "          print(\"max_size of vocab was specified as %i; we now have %i words. Stopping reading.\" % (max_size, self._count))\n",
    "          break\n",
    "\n",
    "    print(\"Finished constructing vocabulary of %i total words. Last word added: %s\" % (self._count, self._id_to_word[self._count-1]))\n",
    "\n",
    "  def word2id(self, word):\n",
    "    if word not in self._word_to_id:\n",
    "      return self._word_to_id[UNKNOWN_TOKEN]\n",
    "    return self._word_to_id[word]\n",
    "\n",
    "  def id2word(self, word_id):\n",
    "    if word_id not in self._id_to_word:\n",
    "      raise ValueError('Id not found in vocab: %d' % word_id)\n",
    "    return self._id_to_word[word_id]\n",
    "\n",
    "  def size(self):\n",
    "    return self._count\n",
    "\n",
    "  def write_metadata(self, fpath):\n",
    "    print(\"Writing word embedding metadata file to %s...\" % (fpath))\n",
    "    with open(fpath, \"w\") as f:\n",
    "      fieldnames = ['word']\n",
    "      writer = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=fieldnames)\n",
    "      for i in xrange(self.size()):\n",
    "        writer.writerow({\"word\": self._id_to_word[i]})\n",
    "        \n",
    "\n",
    "\n",
    "def article2ids(article_words, vocab):\n",
    "    ids = []\n",
    "    oovs = []\n",
    "    unk_id = vocab.word2id(UNKNOWN_TOKEN)\n",
    "    for w in article_words.split():\n",
    "        w = str(w)\n",
    "        i = vocab.word2id(w)\n",
    "        if i == unk_id: # If w is OOV\n",
    "            if w not in oovs: # Add to list of OOVs\n",
    "                oovs.append(w)\n",
    "            oov_num = oovs.index(w) # This is 0 for the first article OOV, 1 for the second article OOV...\n",
    "            if oov_num < ADDITIONAL_WORDS:\n",
    "                ids.append(vocab.size() + oov_num) # This is e.g. 50000 for the first article OOV, 50001 for the second...\n",
    "        else:\n",
    "            ids.append(i)\n",
    "    return ids, oovs\n",
    "\n",
    "\n",
    "def abstract2ids(abstract_words, vocab, article_oovs):\n",
    "    ids = []\n",
    "    unk_id = vocab.word2id(UNKNOWN_TOKEN)\n",
    "    for w in abstract_words.split():\n",
    "        i = vocab.word2id(w)\n",
    "        if i == unk_id: # If w is an OOV word\n",
    "            if w in article_oovs: # If w is an in-article OOV\n",
    "                vocab_idx = vocab.size() + article_oovs.index(w) # Map to its temporary article OOV number\n",
    "                if vocab_idx < VOCAB_SIZE + ADDITIONAL_WORDS:\n",
    "                    ids.append(vocab_idx)\n",
    "        else:\n",
    "            ids.append(i)\n",
    "    return ids\n",
    "\n",
    "def outputids2words(id_list, vocab, article_oovs):\n",
    "    words = []\n",
    "    for i in id_list:\n",
    "        if i < VOCAB_SIZE:\n",
    "            w = vocab.id2word(i)\n",
    "        else:\n",
    "            article_oov_idx = i - vocab.size()\n",
    "            w = article_oovs[article_oov_idx]\n",
    "        words.append(w)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "from tensorflow.core.example import example_pb2\n",
    "\n",
    "def example_gen(filename):\n",
    "    reader = open(filename, 'rb')\n",
    "    examples = []\n",
    "    while True:\n",
    "        len_bytes = reader.read(8)\n",
    "        if not len_bytes: break # finished reading this file\n",
    "        str_len = struct.unpack('q', len_bytes)[0]\n",
    "        example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
    "        e = example_pb2.Example.FromString(example_str)\n",
    "        examples.append(e)\n",
    "        \n",
    "    for e in examples:  \n",
    "        article_text = e.features.feature['article'].bytes_list.value[0]\n",
    "        abstract_text = e.features.feature['abstract'].bytes_list.value[0]\n",
    "        yield (article_text.decode('utf-8'), abstract_text.decode('utf-8'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_size of vocab was specified as 20000; we now have 20000 words. Stopping reading.\n",
      "Finished constructing vocabulary of 20000 total words. Last word added: then-president\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(\"finished_files/vocab\", VOCAB_SIZE)\n",
    "SENTENCE_START_ID = vocab.word2id('<s>')\n",
    "SENTENCE_END_ID = vocab.word2id('</s>')\n",
    "\n",
    "PAD_TOKEN_ID = vocab.word2id('[PAD]')\n",
    "UNKNOWN_TOKEN_ID = vocab.word2id('[UNK]')\n",
    "START_DECODING_ID = vocab.word2id('[START]')\n",
    "STOP_DECODING_ID = vocab.word2id('[STOP]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoder import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17142784"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from decoder import *\n",
    "from pointerTBRU import AttentiveLSTMTBRU, BeamSearchProviderTBRU\n",
    "\n",
    "def build_decoder_model():\n",
    "    master = DRAGNNDecoderMaster()\n",
    "    embeddings_computer = EmbeddingComputer(VOCAB_SIZE + ADDITIONAL_WORDS, 100, PAD_TOKEN_ID)\n",
    "    master.add_component_encoder(TBRU(\"embed\", TaggerRecurrent(\"input\", \"embed\", False), embeddings_computer, (1,), True).cuda())\n",
    "    #master.add_component(TBRU(\"extractive\", TaggerRecurrent(\"embed\", \"extractive\"), TaggerComputer(1000, 1000), (1,), True).cuda())\n",
    "    master.add_component_encoder(TBRU(\"rnn\", RNNSolidRecurrent(\"embed\", \"rnn\"), RNNSolidComputer(100, 50), (1,), True).cuda())\n",
    "    \n",
    "    master.add_component_decoder(TBRU(\"decoder_embed\", TaggerRecurrent(None, \"decoder_embed\", True),embeddings_computer, (1,), True).cuda())\n",
    "    master.add_component_decoder(BeamSearchProviderTBRU(\"beamProvider\",None, \"decoder\", True, False))\n",
    "    pTBRU = AttentiveLSTMTBRU(\"decoder\", (1,), True, 100, 100, 100, \"rnn\", \"decoder_embed\", False)\n",
    "    master.add_component_decoder(pTBRU.cuda())\n",
    "    master.add_component_decoder(TBRU(\"output\", TaggerRecurrent(\"decoder\", \"output\", False), TaggerComputer(100, VOCAB_SIZE), (1,), True).cuda())\n",
    "    return master\n",
    "\n",
    "model = build_decoder_model()\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(articles):\n",
    "    lens = [len(article) for article in articles]\n",
    "    max_len = max(lens)\n",
    "    \n",
    "    for i in range(len(articles)):\n",
    "        articles[i].extend([vocab.word2id(PAD_TOKEN)]*(max_len - len(articles[i])))\n",
    "    return np.array(articles).T\n",
    "\n",
    "def add_padding_for_tagging(articles, targets):\n",
    "    lens = [len(article) for article in articles]\n",
    "    max_len = max(lens)\n",
    "    \n",
    "    for i in range(len(articles)):\n",
    "        targets[i].extend([0]*(max_len - len(articles[i])))\n",
    "        articles[i].extend([vocab.word2id(PAD_TOKEN)]*(max_len - len(articles[i])))\n",
    "    return np.array(articles).T, np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def calculate_mask(articles):\n",
    "    mask = (articles == vocab.word2id(PAD_TOKEN))\n",
    "    mask = np.logical_xor(mask, np.ones(articles.shape))\n",
    "    return mask\n",
    "\n",
    "def get_target(self, article, abstract):\n",
    "    return [ int(i in abstract and i != vocab.word2id(UNKNOWN_TOKEN)) for i in article]\n",
    "\n",
    "def push_abs_ptr(article, abstract, i, abs_ptr):\n",
    "    while abs_ptr < len(abstract) and (not abstract[abs_ptr] in article[i+1:] \n",
    "                                               or abstract[abs_ptr] == vocab.word2id(SENTENCE_START)\n",
    "                                               or abstract[abs_ptr] == vocab.word2id(SENTENCE_STOP)\n",
    "                                               or abstract[abs_ptr] == vocab.word2id(UNKNOWN_TOKEN)):\n",
    "        abs_ptr += 1\n",
    "    return abs_ptr\n",
    "\n",
    "class Batcher():\n",
    "    \n",
    "    def __init__(self, filename, batch_size, max_article_len, max_target_len):\n",
    "        self.batch_size = batch_size\n",
    "        generator = example_gen(filename)\n",
    "\n",
    "        self.batches = []\n",
    "        unknown_words_cnt = 0\n",
    "        self.articles = []\n",
    "        self.targets = []\n",
    "        self.unknown_words = []\n",
    "        self.decoder_inputs = []\n",
    "        while True:\n",
    "            articles = []\n",
    "            targets = []\n",
    "            unknown_words = []\n",
    "            decoder_inputs = []\n",
    "            for i in range(batch_size):\n",
    "                try:\n",
    "                    article_text, abstract_text = next(generator)\n",
    "                    article_ids, unknown_words_list = article2ids(article_text, vocab)\n",
    "                    article_ids = article_ids[:max_article_len]\n",
    "                    target = abstract2ids(abstract_text, vocab, unknown_words_list)\n",
    "                    target = target[:max_target_len - 1]\n",
    "                    target.append(STOP_DECODING_ID)\n",
    "                    decoder_input = [START_DECODING_ID]\n",
    "                    decoder_input.extend(abstract2ids(abstract_text, vocab, unknown_words_list))\n",
    "                    decoder_input = decoder_input[:max_target_len]\n",
    "                    articles.append(article_ids)\n",
    "                    targets.append(target)\n",
    "                    decoder_inputs.append(decoder_input)\n",
    "                    unknown_words.append(unknown_words_list)\n",
    "                    unknown_words_cnt = max(len(unknown_words_list), unknown_words_cnt)\n",
    "                except:\n",
    "                    break\n",
    "            if len(articles) == 0:\n",
    "                break\n",
    "            self.articles.extend(articles)\n",
    "            self.targets.extend(targets)\n",
    "            self.unknown_words.extend(unknown_words)\n",
    "            self.decoder_inputs.extend(decoder_inputs)\n",
    "            articles = add_padding(articles)\n",
    "            targets = add_padding(targets)\n",
    "            decoder_inputs = add_padding(decoder_inputs)\n",
    "            mask = calculate_mask(articles)\n",
    "            self.batches.append( (articles, targets, mask, decoder_inputs) )\n",
    "        print(len(self.batches))\n",
    "        print(unknown_words_cnt)\n",
    "    \n",
    "    def generator(self):\n",
    "        for batch in self.batches:\n",
    "            yield batch\n",
    "            \n",
    "    def get_random_sample(self):\n",
    "        i = random.randint(0, len(self.articles) - 1)\n",
    "        return np.array([self.articles[i]]).T, np.array(self.targets[i]), np.array([self.decoder_inputs[i]]).T, self.unknown_words[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "SENTENCE_START_ID = vocab.word2id('<s>')\n",
    "SENTENCE_END_ID = vocab.word2id('</s>')\n",
    "\n",
    "PAD_TOKEN_ID = vocab.word2id('[PAD]')\n",
    "UNKNOWN_TOKEN_ID = vocab.word2id('[UNK]')\n",
    "START_DECODING_ID = vocab.word2id('[START]')\n",
    "STOP_DECODING_ID = vocab.word2id('[STOP]')\n",
    "\n",
    "def calculate_bleu(result, target):\n",
    "    if not isinstance(result, list):\n",
    "        result = result.tolist()\n",
    "    while STOP_DECODING_ID in result:\n",
    "        result.remove(STOP_DECODING_ID)\n",
    "    while PAD_TOKEN_ID in result:\n",
    "        result.remove(PAD_TOKEN_ID)\n",
    "    if not isinstance(target, list):\n",
    "        target = target.tolist()\n",
    "    while STOP_DECODING_ID in target:\n",
    "        target.remove(STOP_DECODING_ID)\n",
    "    while PAD_TOKEN_ID in target:\n",
    "        target.remove(PAD_TOKEN_ID)\n",
    "    BLEUscore = sentence_bleu([target], result, weights=(1, 0, 0, 0))\n",
    "    return BLEUscore\n",
    "    \n",
    "def calculate_bleu_ngramm(result, target, n):\n",
    "    cnt = 0\n",
    "    for i in range(len(result) - n + 1):\n",
    "        if check_ngramm_in_string(result[i:i+n], target):\n",
    "            cnt += 1\n",
    "    return cnt / (len(result) - n + 1)\n",
    "    \n",
    "def check_ngramm_in_string(ngramm, target):\n",
    "    for i in range(len(target) - len(ngramm) + 1):\n",
    "        flag = True\n",
    "        for j in range(len(ngramm)):\n",
    "            flag = flag and (ngramm[j] == target[i + j])\n",
    "        if flag:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def calculate_logits_bleu_and_rouge(logits, target):\n",
    "    result = logits.argmax(-1).cpu().detach().numpy().T\n",
    "    \n",
    "    target = target.T\n",
    "    #print(result)\n",
    "    #print(target)\n",
    "    bleu = 0.\n",
    "    rouge = 0.\n",
    "    for i in range(result.shape[0]):\n",
    "        bleu += calculate_bleu(result[i], target[i])\n",
    "        rouge += calculate_bleu(target[i], result[i])\n",
    "    return bleu / result.shape[0], rouge / result.shape[0]\n",
    "\n",
    "def generate_summary(article, model, beam_width):\n",
    "    symbols = [START_DECODING_ID]\n",
    "    beam_ids = [0]\n",
    "    probs = [1.]\n",
    "    result = np.array([[]*beam_width])\n",
    "    X_batch = LongTensor(article)\n",
    "    inputs = InputLayerState(\"input\", False, X_batch)\n",
    "    model.eval_run_encoder(inputs)\n",
    "    for i in range(40):\n",
    "        hidden = model.decode((LongTensor([symbols]), LongTensor(beam_ids)))\n",
    "        new_probs = []\n",
    "        new_result = []\n",
    "        for i, s in enumerate(symbols):\n",
    "            values, indices = hidden[i].topk(beam_width)\n",
    "            new_probs.extend(((values + 1) * probs[i]).cpu().detach().tolist())\n",
    "            for j in range(beam_width):\n",
    "                tmp = result[i].tolist()\n",
    "                tmp.append(indices[j].item())\n",
    "                new_result.append(tmp)\n",
    "        top_idx = np.argsort(new_probs)[-beam_width:]\n",
    "        probs = np.array(new_probs)[top_idx]\n",
    "        result = np.array(new_result)[top_idx]\n",
    "        symbols = result[:,-1]\n",
    "        beam_ids = top_idx // beam_width\n",
    "        #symbol[0] == STOP_DECODING_ID\n",
    "    return result[0]\n",
    "\n",
    "def gen_and_print_summary(batcher, model, beam_width):\n",
    "    article_text, target, decoder_input, unk_words = batcher.get_random_sample()\n",
    "    result = generate_summary(article_text, model, beam_width)\n",
    "    result = outputids2words(result, vocab, unk_words)\n",
    "    target = outputids2words(target, vocab, unk_words)\n",
    "    print('result is \\n' + result)\n",
    "    print('target is \\n' + target)\n",
    "    print('BLEU = {:.4f}'.format(sentence_bleu([target], result, weights=(1, 0, 0, 0))))\n",
    "    \n",
    "def gen_and_print_summary_by_target(batcher, model):\n",
    "    article_text, target, decoder_inputs, unk_words = batcher.get_random_sample()\n",
    "    X_batch, y_batch, decoder_batch = LongTensor(article_text), LongTensor(target), LongTensor(decoder_inputs)\n",
    "    inputs = InputLayerState(\"input\", True, X_batch)\n",
    "    targetLayer = InputLayerState(\"target\", True, decoder_batch)\n",
    "    logits = model.train_run(inputs, targetLayer)\n",
    "    \n",
    "\n",
    "    print(calculate_logits_bleu_and_rouge(logits, np.array([target]).T))\n",
    "    result = logits.argmax(-1).squeeze(0).squeeze(-1).cpu().detach().tolist()\n",
    "    \n",
    "    result = outputids2words(result, vocab, unk_words)\n",
    "    target = outputids2words(target, vocab, unk_words)\n",
    "    print('result is \\n' + result)\n",
    "    print('target is \\n' + target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_f1(tp, fp, tn, fn):\n",
    "    precision = tp/(fp + tp)\n",
    "    recall = tp/(tp + fn)\n",
    "    f1 = 2*precision*recall/(precision+recall)\n",
    "    return f1\n",
    "\n",
    "def precalc_f1(articles_tokens, articles, target):\n",
    "    mask = calculate_mask(articles_tokens).T\n",
    "    result = (articles > 0.5)\n",
    "    #print(result[0])\n",
    "    #print(articles.shape, target.shape, mask.shape)\n",
    "    n_res = np.logical_not(result)\n",
    "    n_tar = np.logical_not(target)\n",
    "    tp = (result * target * mask).sum()\n",
    "    fp = (n_res * target * mask).sum()\n",
    "    tn = (n_res * n_tar * mask).sum()\n",
    "    fn = (result * n_tar * mask).sum()\n",
    "    return tp, fp, tn, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n",
      "146\n"
     ]
    }
   ],
   "source": [
    "train_data=Batcher(\"finished_files/chunked/train_000.bin\", 8, 1000, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 0.0)\n",
      "result is \n",
      "5s denied mushroom ahmed strap ludicrous interactive troubled kinds spans vettel blueprint compares blitzer 2011 jordan pearson posters skate eruption threatens analysts graph newest common southwestern helium helium helium bribery unleashed gently discouraged regardless stand-up slaves counseling confuse counseling tea static taking postings discrimination zac eruption eruption discouraged eldest exposing\n",
      "target is \n",
      "woman gets paid $ 500 extra a month , plus free gas , for turning car into billboard . `` people used to tease me , '' says misha di bono . survey finds companies are looking at ways to help ease commuting costs . ceo : `` we [STOP]\n"
     ]
    }
   ],
   "source": [
    "gen_and_print_summary_by_target(train_data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44725248"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, optimizer=None):  \n",
    "    epoch_loss = 0.\n",
    "    bleu = 0.\n",
    "    rouge = 0.\n",
    "    batch_cnt = 1\n",
    "    is_train = not optimizer is None\n",
    "    model.train(is_train)\n",
    "\n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        for i, (article_text, target, mask, decoder_inputs) in enumerate(data.generator()):\n",
    "            batch_cnt =  i + 1\n",
    "            X_batch, y_batch, decoder_batch = LongTensor(article_text), LongTensor(target), LongTensor(decoder_inputs)\n",
    "            inputs = InputLayerState(\"input\", True, X_batch)\n",
    "            targetLayer = InputLayerState(\"target\", True, decoder_batch)\n",
    "            logits = model.train_run(inputs, targetLayer)\n",
    "        \n",
    "            #print(logits.view(1, -1).shape)\n",
    "            loss = criterion(logits.view(-1, logits.shape[-1]), y_batch.view(-1))\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            cur_bleu, cur_rouge = calculate_logits_bleu_and_rouge(logits, target)\n",
    "            \n",
    "            bleu += cur_bleu\n",
    "            rouge += cur_rouge\n",
    "            \n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                #nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "                optimizer.step()\n",
    "            \n",
    "            print('\\r[{}]: Loss = {:.4f}, BLEU = {:.4f}, ROUGE = {:.4f}'.format(i, loss.item(), cur_bleu, cur_rouge), end='')\n",
    "    \n",
    "    gen_and_print_summary(data, model, 10)\n",
    "    #gen_and_print_summary_by_target(data, model)\n",
    "    return epoch_loss, bleu / batch_cnt, rouge / batch_cnt\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, \n",
    "        batch_size=32, val_data=None, val_batch_size=None):\n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        start_time = time.time()\n",
    "        train_loss, bleu, rouge = do_epoch(model, criterion, train_data, batch_size, optimizer)\n",
    "        output_info = '\\rEpoch {} / {}, Epoch Time = {:.2f}s: Train Loss = {:.4f}: BLEU = {:.4f}, ROUGE = {:.4f}'\n",
    "        if not val_data is None:\n",
    "            val_loss, bleu, rouge = do_epoch(model, criterion, val_data, val_batch_size, None)\n",
    "            epoch_time = time.time() - start_time\n",
    "            output_info += ', Val Loss = {:.4f}'\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss, bleu, rouge, val_loss))\n",
    "        else:\n",
    "            epoch_time = time.time() - start_time\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss, bleu, rouge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n",
      "146\n",
      "[9]: Loss = 9.8645, BLEU = 0.0125, ROUGE = 0.0125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\nltk\\translate\\bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\python36\\lib\\site-packages\\nltk\\translate\\bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\python36\\lib\\site-packages\\nltk\\translate\\bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[124]: Loss = 6.9716, BLEU = 0.1000, ROUGE = 0.1000result is \n",
      "[STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] . [STOP] [STOP] [STOP] [STOP] [STOP]\n",
      "target is \n",
      "rebel group claims responsibility another strike on a gas pipeline . mend group say it killed 11 government soldiers [STOP]\n",
      "BLEU = 0.0949\n",
      "Epoch 1 / 50, Epoch Time = 5.34s: Train Loss = 994.2039: BLEU = 0.0773, ROUGE = 0.0773\n",
      "[124]: Loss = 6.5881, BLEU = 0.1375, ROUGE = 0.1375result is \n",
      "new : [STOP] in [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] . [STOP]\n",
      "target is \n",
      "three people dead , at least 84 injured after earthquake hits japan . u.s. geological survey reports 7.0 magnitude [STOP]\n",
      "BLEU = 0.1145\n",
      "Epoch 2 / 50, Epoch Time = 5.27s: Train Loss = 841.7066: BLEU = 0.1147, ROUGE = 0.1147\n",
      "[124]: Loss = 6.3502, BLEU = 0.1875, ROUGE = 0.1875result is \n",
      "new : new : : [UNK] [STOP] . new : [STOP] in [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] to\n",
      "target is \n",
      "hezbollah leader hassan nasrallah says lebanon needs collaboration of everyone . nasrallah praises election of michel sleiman as president [STOP]\n",
      "BLEU = 0.1525\n",
      "Epoch 3 / 50, Epoch Time = 5.28s: Train Loss = 818.9075: BLEU = 0.1471, ROUGE = 0.1471\n",
      "[124]: Loss = 6.1574, BLEU = 0.1812, ROUGE = 0.1812result is \n",
      "new : new : `` `` `` `` [UNK] to [STOP] [STOP] . new : [UNK] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] in\n",
      "target is \n",
      "the rev. rick warren will play host to candidates at his church . warren says he 's friends with [STOP]\n",
      "BLEU = 0.1741\n",
      "Epoch 4 / 50, Epoch Time = 5.10s: Train Loss = 798.5378: BLEU = 0.1767, ROUGE = 0.1767\n",
      "[124]: Loss = 5.9846, BLEU = 0.1875, ROUGE = 0.1875result is \n",
      "new : new : `` `` `` `` `` `` `` `` `` `` `` `` `` [UNK] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] to\n",
      "target is \n",
      "super cup winners ac milan are held to a 1-1 home draw by fiorentina . brazilian kaka gave milan [STOP]\n",
      "BLEU = 0.1602\n",
      "Epoch 5 / 50, Epoch Time = 5.49s: Train Loss = 778.1411: BLEU = 0.1883, ROUGE = 0.1883\n",
      "[124]: Loss = 5.8245, BLEU = 0.2000, ROUGE = 0.2000result is \n",
      "new : new : `` `` `` `` `` [UNK] to [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] to\n",
      "target is \n",
      "u.n. to send special envoy to myanmar amid reports of crackdown . world leaders condemn events in myanmar , [STOP]\n",
      "BLEU = 0.1496\n",
      "Epoch 6 / 50, Epoch Time = 5.38s: Train Loss = 760.2760: BLEU = 0.1948, ROUGE = 0.1948\n",
      "[124]: Loss = 5.6706, BLEU = 0.1875, ROUGE = 0.1875result is \n",
      "new : new : `` `` `` `` `` `` `` i [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] to\n",
      "target is \n",
      "new : hospitalized incest daughter 's condition is grave but stable , police say . fritzl imprisoned and raped [STOP]\n",
      "BLEU = 0.1504\n",
      "Epoch 7 / 50, Epoch Time = 5.25s: Train Loss = 742.7403: BLEU = 0.2004, ROUGE = 0.2004\n",
      "[124]: Loss = 5.5385, BLEU = 0.2062, ROUGE = 0.2062result is \n",
      "new : new : `` `` `` `` `` `` `` `` `` i [STOP] to [STOP] [STOP] to [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP]\n",
      "target is \n",
      "singapore airlines a380 superjumbo completes historic maiden flight . luxury first class cabins have separate leather seats and double [STOP]\n",
      "BLEU = 0.1589\n",
      "Epoch 8 / 50, Epoch Time = 5.48s: Train Loss = 724.2293: BLEU = 0.2070, ROUGE = 0.2070\n",
      "[124]: Loss = 5.4034, BLEU = 0.2187, ROUGE = 0.2187result is \n",
      "new : clinton : `` `` `` `` `` i [STOP] to [STOP] [STOP] [STOP] [STOP] in [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] .\n",
      "target is \n",
      "report reveals medical evidence of torture , including beatings and electric shock . study calls on u.s. government to [STOP]\n",
      "BLEU = 0.1747\n",
      "Epoch 9 / 50, Epoch Time = 5.35s: Train Loss = 706.1388: BLEU = 0.2125, ROUGE = 0.2125\n",
      "[124]: Loss = 5.2789, BLEU = 0.2312, ROUGE = 0.2312result is \n",
      "new : clinton : `` `` `` `` i [STOP] to [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] in\n",
      "target is \n",
      "sailor 's wife could be deported while he 's overseas . sailor : `` defending the country that 's [STOP]\n",
      "BLEU = 0.1765\n",
      "Epoch 10 / 50, Epoch Time = 5.33s: Train Loss = 688.8785: BLEU = 0.2220, ROUGE = 0.2220\n",
      "[124]: Loss = 5.1325, BLEU = 0.2375, ROUGE = 0.2375result is \n",
      "new : president bush : `` `` `` `` `` i [STOP] '' `` `` `` `` i '' '' `` `` i '' '' `` `` i '' '' `` `` i '' '' `` `` i a the\n",
      "target is \n",
      "yuvraj singh becomes first player to hit six sixes in an over in twenty20 game . singh hits 58 [STOP]\n",
      "BLEU = 0.4000\n",
      "Epoch 11 / 50, Epoch Time = 5.18s: Train Loss = 670.2929: BLEU = 0.2281, ROUGE = 0.2281\n",
      "[124]: Loss = 4.9684, BLEU = 0.2250, ROUGE = 0.2250result is \n",
      "new : president bush : `` `` `` `` `` i '' '' `` `` i '' '' `` `` i '' '' `` i [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] in [STOP]\n",
      "target is \n",
      "`` every three minutes '' statistic made witherspoon feel `` vulnerable '' and `` scared '' oscar-winning actress : [STOP]\n",
      "BLEU = 0.3352\n",
      "Epoch 12 / 50, Epoch Time = 5.25s: Train Loss = 652.4959: BLEU = 0.2341, ROUGE = 0.2341\n",
      "[124]: Loss = 4.8254, BLEU = 0.2375, ROUGE = 0.2375result is \n",
      "new : president bush : `` `` `` `` i [STOP] '' and [STOP] to [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] . [STOP]\n",
      "target is \n",
      "russia 's participation in wto , g-8 at risk because of georgia , mccain says . mccain says crisis [STOP]\n",
      "BLEU = 0.2000\n",
      "Epoch 13 / 50, Epoch Time = 5.24s: Train Loss = 636.4554: BLEU = 0.2372, ROUGE = 0.2372\n",
      "[124]: Loss = 4.7332, BLEU = 0.2125, ROUGE = 0.2125result is \n",
      "new : sen. hillary clinton : `` `` `` i a [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] .\n",
      "target is \n",
      "on this week 's mme we look at gulf interests in english football . we talk to sheikha hanadi [STOP]\n",
      "BLEU = 0.1925\n",
      "Epoch 14 / 50, Epoch Time = 5.17s: Train Loss = 619.6540: BLEU = 0.2409, ROUGE = 0.2409\n",
      "[124]: Loss = 4.5970, BLEU = 0.2312, ROUGE = 0.2312result is \n",
      "new : sen. hillary clinton : `` i am '' and [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] in [STOP]\n",
      "target is \n",
      "report : chinese authorities confirm that captive giant pandas are safe . concerns grow over road to the reserves [STOP]\n",
      "BLEU = 0.2107\n",
      "Epoch 15 / 50, Epoch Time = 5.25s: Train Loss = 601.7455: BLEU = 0.2471, ROUGE = 0.2471\n",
      "[124]: Loss = 4.4585, BLEU = 0.2562, ROUGE = 0.2562result is \n",
      "new : sen. hillary clinton : `` i am '' and [STOP] [STOP] [STOP] [STOP] . [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] .\n",
      "target is \n",
      "brazilian kaka is named european player of the year . the ac milan player is chosen ahead of cristiano [STOP]\n",
      "BLEU = 0.2161\n",
      "Epoch 16 / 50, Epoch Time = 5.26s: Train Loss = 584.5342: BLEU = 0.2528, ROUGE = 0.2528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[124]: Loss = 4.3811, BLEU = 0.2812, ROUGE = 0.2812result is \n",
      "new : `` i am '' and `` i am '' and [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] . [STOP]\n",
      "target is \n",
      "the greatest japanese dancer is known as `` teddy '' to friends and fans . experienced a meteoric rise [STOP]\n",
      "BLEU = 0.1947\n",
      "Epoch 17 / 50, Epoch Time = 5.22s: Train Loss = 567.1878: BLEU = 0.2612, ROUGE = 0.2612\n",
      "[124]: Loss = 4.2223, BLEU = 0.2812, ROUGE = 0.2812result is \n",
      "new : [START] new : sen. hillary clinton : `` i have been to have been to [STOP] , '' says . new : `` i [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] .\n",
      "target is \n",
      "sailor 's wife could be deported while he 's overseas . sailor : `` defending the country that 's [STOP]\n",
      "BLEU = 0.3776\n",
      "Epoch 18 / 50, Epoch Time = 5.14s: Train Loss = 549.5421: BLEU = 0.2716, ROUGE = 0.2716\n",
      "[124]: Loss = 4.0512, BLEU = 0.3000, ROUGE = 0.3000result is \n",
      "new : sen. hillary clinton : `` i am '' and [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] in [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP]\n",
      "target is \n",
      "new : mayor blames islamist fighters for blast ; islamist fighters blame government . new : women part of [STOP]\n",
      "BLEU = 0.2190\n",
      "Epoch 19 / 50, Epoch Time = 5.31s: Train Loss = 531.7654: BLEU = 0.2795, ROUGE = 0.2795\n",
      "[124]: Loss = 3.9097, BLEU = 0.3125, ROUGE = 0.3125result is \n",
      "new : sen. hillary clinton : `` i am '' and [STOP] [STOP] to [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] ,\n",
      "target is \n",
      "senator 's weight , blood pressure and cholesterol are all healthy , doctor says . obama is an `` [STOP]\n",
      "BLEU = 0.2321\n",
      "Epoch 20 / 50, Epoch Time = 5.07s: Train Loss = 513.8862: BLEU = 0.2907, ROUGE = 0.2907\n",
      "[124]: Loss = 3.8111, BLEU = 0.3312, ROUGE = 0.3312result is \n",
      "new : `` i am '' and `` i am '' and `` i am '' and [STOP] [STOP] in [STOP] 's [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP]\n",
      "target is \n",
      "news reporter john mcwethy , 61 , dies in colorado ski accident . mcwethy was wearing helmet at time [STOP]\n",
      "BLEU = 0.2365\n",
      "Epoch 21 / 50, Epoch Time = 5.39s: Train Loss = 496.4294: BLEU = 0.3018, ROUGE = 0.3018\n",
      "[124]: Loss = 3.6537, BLEU = 0.3250, ROUGE = 0.3250result is \n",
      "new : sen. hillary clinton : `` i am '' and `` i am '' and [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] ,\n",
      "target is \n",
      "new : democratic leadership aides say bill must help low - , earners . president calls for broad-based tax [STOP]\n",
      "BLEU = 0.2579\n",
      "Epoch 22 / 50, Epoch Time = 5.33s: Train Loss = 479.9261: BLEU = 0.3179, ROUGE = 0.3179\n",
      "[124]: Loss = 3.5673, BLEU = 0.3312, ROUGE = 0.3312result is \n",
      "new : sen. hillary clinton : `` i am '' `` i am '' `` i am '' and [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] . the\n",
      "target is \n",
      "ireporters share their thoughts on how the iraq war has affected their lives . `` having -lsb- my husband [STOP]\n",
      "BLEU = 0.2843\n",
      "Epoch 23 / 50, Epoch Time = 5.31s: Train Loss = 463.9187: BLEU = 0.3348, ROUGE = 0.3348\n",
      "[124]: Loss = 3.4231, BLEU = 0.3500, ROUGE = 0.3500result is \n",
      "new : president bush to be used in the uniforms . the united states has [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] . [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] .\n",
      "target is \n",
      "new : adam gilchrist to retire from cricket at end of this australian summer . the wicketkeeper set new [STOP]\n",
      "BLEU = 0.3093\n",
      "Epoch 24 / 50, Epoch Time = 5.13s: Train Loss = 448.4138: BLEU = 0.3525, ROUGE = 0.3525\n",
      "[124]: Loss = 3.2984, BLEU = 0.3875, ROUGE = 0.3875result is \n",
      "new : sen. hillary clinton : `` i am '' and `` i love have been a dream '' and [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] . [STOP]\n",
      "target is \n",
      "cnn 's : will lebanon 's brief renaissance be snuffed out ? says lebanon 's key power players have [STOP]\n",
      "BLEU = 0.3286\n",
      "Epoch 25 / 50, Epoch Time = 5.33s: Train Loss = 433.0713: BLEU = 0.3694, ROUGE = 0.3694\n",
      "[124]: Loss = 3.2277, BLEU = 0.3750, ROUGE = 0.3750result is \n",
      "new : sen. hillary clinton : `` i am '' and `` i love have been a very obvious [STOP] [STOP] '' [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP]\n",
      "target is \n",
      "new : tsa backs officers who made passenger remove nipple rings . new : agency acknowledges that procedures need [STOP]\n",
      "BLEU = 0.3167\n",
      "Epoch 26 / 50, Epoch Time = 5.30s: Train Loss = 417.8347: BLEU = 0.3878, ROUGE = 0.3878\n",
      "[124]: Loss = 3.0705, BLEU = 0.4062, ROUGE = 0.4062result is \n",
      "new : sen. hillary clinton : `` i am '' and `` i am '' and [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] on\n",
      "target is \n",
      "six young italians found shot in the head in western german city of duisburg . five were dead when [STOP]\n",
      "BLEU = 0.2568\n",
      "Epoch 27 / 50, Epoch Time = 5.32s: Train Loss = 401.8821: BLEU = 0.4068, ROUGE = 0.4068\n",
      "[124]: Loss = 2.9550, BLEU = 0.4500, ROUGE = 0.4500result is \n",
      "new : sen. hillary clinton : `` i am '' and `` i am '' and `` i am '' and `` i am '' and `` i a very obvious [STOP] '' u.s. warns serbia . new : ``\n",
      "target is \n",
      "prachanda , communist party of nepal chairman , won 464 out of 577 votes . a simple majority was [STOP]\n",
      "BLEU = 0.5170\n",
      "Epoch 28 / 50, Epoch Time = 5.25s: Train Loss = 386.1553: BLEU = 0.4280, ROUGE = 0.4280\n",
      "[124]: Loss = 2.9007, BLEU = 0.4562, ROUGE = 0.4562result is \n",
      "new : sen. hillary clinton delivered with laughter . new : `` i am '' and [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP]\n",
      "target is \n",
      "mary winkler convicted earlier this year of shooting her husband to death . winkler served time and was released [STOP]\n",
      "BLEU = 0.3112\n",
      "Epoch 29 / 50, Epoch Time = 5.18s: Train Loss = 370.5012: BLEU = 0.4480, ROUGE = 0.4480\n",
      "[124]: Loss = 2.7593, BLEU = 0.4875, ROUGE = 0.4875result is \n",
      "new : `` i thought i love have been crushed '' [STOP] 's [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] .\n",
      "target is \n",
      "video shows , rusty building with paint chipping ; broken drain pipe . picture : soldier in a sink [STOP]\n",
      "BLEU = 0.2333\n",
      "Epoch 30 / 50, Epoch Time = 5.23s: Train Loss = 356.5204: BLEU = 0.4659, ROUGE = 0.4659\n",
      "[124]: Loss = 2.6792, BLEU = 0.5375, ROUGE = 0.5375result is \n",
      "president cristina fernandez de kirchner in the year . the united states is believed for the [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] . new\n",
      "target is \n",
      "22-year-old arms dealer faces congressional inquiry . company supplied ammunition made in china decades ago . company 's contract [STOP]\n",
      "BLEU = 0.3333\n",
      "Epoch 31 / 50, Epoch Time = 5.22s: Train Loss = 344.0217: BLEU = 0.4828, ROUGE = 0.4828\n",
      "[124]: Loss = 2.5635, BLEU = 0.5125, ROUGE = 0.5125result is \n",
      "new : `` i thought i am '' and `` i am '' and `` i a very rare own positions to [STOP] [STOP] . new : `` i think it 's a miracle [STOP] [STOP] [STOP] [STOP] [STOP] [STOP]\n",
      "target is \n",
      "new : british counter-terrorism experts bhutto 's vehicle . new : bhutto 's assassination was her own fault , [STOP]\n",
      "BLEU = 0.4824\n",
      "Epoch 32 / 50, Epoch Time = 5.33s: Train Loss = 331.9476: BLEU = 0.4976, ROUGE = 0.4976\n",
      "[124]: Loss = 2.5618, BLEU = 0.4688, ROUGE = 0.4688result is \n",
      "new : `` i am '' and `` it 's the country '' and [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] . [STOP]\n",
      "target is \n",
      "on tape pilot says he has limited ability to move plane 's nose up and down . abc obtained [STOP]\n",
      "BLEU = 0.2267\n",
      "Epoch 33 / 50, Epoch Time = 5.34s: Train Loss = 319.5985: BLEU = 0.5169, ROUGE = 0.5169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[124]: Loss = 2.4116, BLEU = 0.5375, ROUGE = 0.5375result is \n",
      "president bush : `` i love have been crushed to be used , [STOP] says he 's [STOP] [STOP] to [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] .\n",
      "target is \n",
      "secret court ruling prompted push for updating the program . president bush had urged the house to pass the [STOP]\n",
      "BLEU = 0.3128\n",
      "Epoch 34 / 50, Epoch Time = 5.32s: Train Loss = 307.1277: BLEU = 0.5340, ROUGE = 0.5340\n",
      "[124]: Loss = 2.3169, BLEU = 0.5563, ROUGE = 0.5563result is \n",
      "new : [START] new : `` i thought i am '' and `` i was going to be used in the actions as [STOP] . new : `` i kind [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP]\n",
      "target is \n",
      "new : donors pledge $ 7.4 billion to help build a palestinian state . new : imf offers to [STOP]\n",
      "BLEU = 0.3989\n",
      "Epoch 35 / 50, Epoch Time = 5.45s: Train Loss = 295.4441: BLEU = 0.5506, ROUGE = 0.5506\n",
      "[124]: Loss = 2.2271, BLEU = 0.5750, ROUGE = 0.5750result is \n",
      "new : sen. barack obama made passenger in rugby union this week in iraq . new : [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] . [STOP]\n",
      "target is \n",
      "new : bertha 's winds increase from 115 mph to 120 mph . bermuda could be affected by the [STOP]\n",
      "BLEU = 0.2894\n",
      "Epoch 36 / 50, Epoch Time = 5.34s: Train Loss = 284.1606: BLEU = 0.5693, ROUGE = 0.5693\n",
      "[124]: Loss = 2.1784, BLEU = 0.5812, ROUGE = 0.5812result is \n",
      "jordan opens school doors to be based in the world 's open republican primary . the brazilian beats cristiano [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] . [STOP] [STOP]\n",
      "target is \n",
      "dubai court sentences two men to 15 years for rape , kidnapping of 15-year-old boy . one of the [STOP]\n",
      "BLEU = 0.3426\n",
      "Epoch 37 / 50, Epoch Time = 5.44s: Train Loss = 273.0162: BLEU = 0.5845, ROUGE = 0.5845\n",
      "[124]: Loss = 2.0883, BLEU = 0.5938, ROUGE = 0.5938result is \n",
      "the condemned include people convicted of drug and alcohol offenses . the united states has reduced [STOP] , [STOP] says [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] , [STOP]\n",
      "target is \n",
      "daughter of austrian incest victim reunited with family , expected to make full recovery . kerstin fritzl , 19 [STOP]\n",
      "BLEU = 0.3529\n",
      "Epoch 38 / 50, Epoch Time = 5.37s: Train Loss = 263.6153: BLEU = 0.6006, ROUGE = 0.6006\n",
      "[124]: Loss = 2.0604, BLEU = 0.5938, ROUGE = 0.5938result is \n",
      "the condemned include people convicted of drug and alcohol offenses . the group linked to help secure release of bread [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [UNK]\n",
      "target is \n",
      "the national transportation safety board is investigating us airways flight . plane was en route from orlando , florida [STOP]\n",
      "BLEU = 0.3541\n",
      "Epoch 39 / 50, Epoch Time = 5.50s: Train Loss = 253.5957: BLEU = 0.6140, ROUGE = 0.6140\n",
      "[124]: Loss = 1.9819, BLEU = 0.6000, ROUGE = 0.6000result is \n",
      "new : `` i thought i was dating '' in the country [STOP] . new : `` i 'm [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] . [STOP]\n",
      "target is \n",
      "new : attorney says inmate was shocked , relieved , somber . new : victim 's family too devastated [STOP]\n",
      "BLEU = 0.2804\n",
      "Epoch 40 / 50, Epoch Time = 5.38s: Train Loss = 243.7744: BLEU = 0.6328, ROUGE = 0.6328\n",
      "[124]: Loss = 1.8866, BLEU = 0.6250, ROUGE = 0.6250result is \n",
      "iranian teenager loses appeal to remain in the netherlands . `` daughters of prejudice '' in [STOP] of the [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] . [STOP] [STOP]\n",
      "target is \n",
      "iranian teenager loses appeal to remain in the netherlands . 19-year-old had sought asylum in uk but is to [STOP]\n",
      "BLEU = 0.3790\n",
      "Epoch 41 / 50, Epoch Time = 5.28s: Train Loss = 234.8984: BLEU = 0.6481, ROUGE = 0.6481\n",
      "[124]: Loss = 1.8340, BLEU = 0.6625, ROUGE = 0.6625result is \n",
      "ugandan officer reports tensions with libyan leader 's kandahar commitment . opposition unable to extend commitment as long as [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] . [STOP] [STOP]\n",
      "target is \n",
      "bill would allow regulators to crack down on speculation in oil markets . gop wants to add 28 amendments [STOP]\n",
      "BLEU = 0.3694\n",
      "Epoch 42 / 50, Epoch Time = 5.28s: Train Loss = 226.9935: BLEU = 0.6579, ROUGE = 0.6579\n",
      "[124]: Loss = 1.7433, BLEU = 0.6625, ROUGE = 0.6625result is \n",
      "new : obama vows to have yet , charged with three counts of murder . new : `` i kind [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] says\n",
      "target is \n",
      "new : all vehicles banned in city of amara through thursday , provincial governor says . death toll continues [STOP]\n",
      "BLEU = 0.3829\n",
      "Epoch 43 / 50, Epoch Time = 5.18s: Train Loss = 218.6317: BLEU = 0.6716, ROUGE = 0.6716\n",
      "[124]: Loss = 1.6599, BLEU = 0.6937, ROUGE = 0.6937result is \n",
      "southern california governments band together to $ 100,000 ancient persian . she has opened in bombing , [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [UNK] [STOP]\n",
      "target is \n",
      "kenya 's 36 million population made up of around 40 tribal groups . president mwai kibaki belongs to the [STOP]\n",
      "BLEU = 0.3258\n",
      "Epoch 44 / 50, Epoch Time = 5.25s: Train Loss = 210.2041: BLEU = 0.6832, ROUGE = 0.6832\n",
      "[124]: Loss = 1.6235, BLEU = 0.6750, ROUGE = 0.6750result is \n",
      "udinese stay fifth in spanish enclave of the united arab emirates agrees 's open republican primary . the brazilian kaka gave to [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP]\n",
      "target is \n",
      "udinese stay fifth in italy 's serie a after suffering a 3-1 defeat away to napoli . ezequiel lavezzi [STOP]\n",
      "BLEU = 0.3780\n",
      "Epoch 45 / 50, Epoch Time = 5.35s: Train Loss = 201.6274: BLEU = 0.6990, ROUGE = 0.6990\n",
      "[124]: Loss = 1.5727, BLEU = 0.6687, ROUGE = 0.6687result is \n",
      "new : `` i thought i love have been crushed '' [STOP] says [STOP] [STOP] says [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] says\n",
      "target is \n",
      "new : tocopilla mayor : more than 1,200 homes flattened , shelters damaged . dozens of workers freed from [STOP]\n",
      "BLEU = 0.2387\n",
      "Epoch 46 / 50, Epoch Time = 5.43s: Train Loss = 193.4565: BLEU = 0.7123, ROUGE = 0.7123\n",
      "[124]: Loss = 1.5457, BLEU = 0.6625, ROUGE = 0.6625result is \n",
      "two bombs explode in algerian capital near government and [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] . the [STOP]\n",
      "target is \n",
      "f-15s grounded after a november 1 crash in missouri . f-15 is used for ground support in the wars [STOP]\n",
      "BLEU = 0.2556\n",
      "Epoch 47 / 50, Epoch Time = 5.28s: Train Loss = 185.1254: BLEU = 0.7247, ROUGE = 0.7247\n",
      "[124]: Loss = 1.4293, BLEU = 0.7000, ROUGE = 0.7000result is \n",
      "lionel messi is named to earn 0-0 draw away to [STOP] . the group to be [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] . [STOP] 's\n",
      "target is \n",
      "sailors say roger stone stayed behind to get others to safety . the five survivors were found and airlifted [STOP]\n",
      "BLEU = 0.3000\n",
      "Epoch 48 / 50, Epoch Time = 5.21s: Train Loss = 176.8324: BLEU = 0.7387, ROUGE = 0.7387\n",
      "[124]: Loss = 1.3468, BLEU = 0.7312, ROUGE = 0.7312result is \n",
      "actress selma blair wore huge prosthetic boobs for years ago . the brazilian kaka named in [STOP] to [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] . [STOP] [STOP]\n",
      "target is \n",
      "report : chinese authorities confirm that captive giant pandas are safe . concerns grow over road to the reserves [STOP]\n",
      "BLEU = 0.3574\n",
      "Epoch 49 / 50, Epoch Time = 5.41s: Train Loss = 168.9265: BLEU = 0.7509, ROUGE = 0.7509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[124]: Loss = 1.2844, BLEU = 0.7312, ROUGE = 0.7312result is \n",
      "actress selma blair wore huge prosthetic boobs for a john waters movie . she 's gained at least 15 [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] . [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] .\n",
      "target is \n",
      "report : chinese authorities confirm that captive giant pandas are safe . concerns grow over road to the reserves [STOP]\n",
      "BLEU = 0.3872\n",
      "Epoch 50 / 50, Epoch Time = 5.60s: Train Loss = 161.7022: BLEU = 0.7617, ROUGE = 0.7617\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = vocab.word2id(PAD_TOKEN)).cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, epochs_count=50, batch_size=32, train_data=Batcher(\"finished_files/chunked/train_000.bin\",8, 40, 20),\n",
    "    val_data=None, val_batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"first_try_decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-d9726117edcb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m fit(model, criterion, optimizer, epochs_count=50, batch_size=32, train_data=\"finished_files/chunked/train_000.bin\",\n\u001b[1;32m----> 2\u001b[1;33m     val_data=\"finished_files/chunked/val_000.bin\", val_batch_size=32)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-16320cb9dcb6>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(model, criterion, optimizer, train_data, epochs_count, batch_size, val_data, val_batch_size)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbleu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrouge\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdo_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[0moutput_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\rEpoch {} / {}, Epoch Time = {:.2f}s: Train Loss = {:.4f}: BLEU = {:.4f}, ROUGE = {:.4f}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mval_data\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-16320cb9dcb6>\u001b[0m in \u001b[0;36mdo_epoch\u001b[1;34m(model, criterion, data, batch_size, optimizer)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0marticle_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_inputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0mbatch_cnt\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'generator'"
     ]
    }
   ],
   "source": [
    "fit(model, criterion, optimizer, epochs_count=50, batch_size=32, train_data=\"finished_files/chunked/train_000.bin\",\n",
    "    val_data=\"finished_files/chunked/val_000.bin\", val_batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
