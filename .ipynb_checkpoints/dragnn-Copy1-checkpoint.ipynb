{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.cuda import FloatTensor, LongTensor\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "\n",
    "PAD_TOKEN = '[PAD]' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNKNOWN_TOKEN = '[UNK]' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "START_DECODING = '[START]' # This has a vocab id, which is used at the start of every decoder input sequence\n",
    "STOP_DECODING = '[STOP]' # This has a vocab id, which is used at the end of untruncated target sequences\n",
    "\n",
    "VOCAB_SIZE = 20000\n",
    "ADDITIONAL_WORDS = 100\n",
    "\n",
    "class Vocab(object):\n",
    "\n",
    "  def __init__(self, vocab_file, max_size):\n",
    "    self._word_to_id = {}\n",
    "    self._id_to_word = {}\n",
    "    self._count = 0 # keeps track of total number of words in the Vocab\n",
    "\n",
    "    # [UNK], [PAD], [START] and [STOP] get the ids 0,1,2,3.\n",
    "    for w in [UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
    "      self._word_to_id[w] = self._count\n",
    "      self._id_to_word[self._count] = w\n",
    "      self._count += 1\n",
    "\n",
    "    # Read the vocab file and add words up to max_size\n",
    "    with open(vocab_file, 'r') as vocab_f:\n",
    "      for line in vocab_f:\n",
    "        pieces = line.split()\n",
    "        if len(pieces) != 2:\n",
    "          print('Warning: incorrectly formatted line in vocabulary file: %s\\n' % line)\n",
    "          continue\n",
    "        w = pieces[0]\n",
    "        if w in [SENTENCE_START, SENTENCE_END, UNKNOWN_TOKEN, PAD_TOKEN, START_DECODING, STOP_DECODING]:\n",
    "          raise Exception('<s>, </s>, [UNK], [PAD], [START] and [STOP] shouldn\\'t be in the vocab file, but %s is' % w)\n",
    "        if w in self._word_to_id:\n",
    "          raise Exception('Duplicated word in vocabulary file: %s' % w)\n",
    "        self._word_to_id[w] = self._count\n",
    "        self._id_to_word[self._count] = w\n",
    "        self._count += 1\n",
    "        if max_size != 0 and self._count >= max_size:\n",
    "          print(\"max_size of vocab was specified as %i; we now have %i words. Stopping reading.\" % (max_size, self._count))\n",
    "          break\n",
    "\n",
    "    print(\"Finished constructing vocabulary of %i total words. Last word added: %s\" % (self._count, self._id_to_word[self._count-1]))\n",
    "\n",
    "  def word2id(self, word):\n",
    "    if word not in self._word_to_id:\n",
    "      return self._word_to_id[UNKNOWN_TOKEN]\n",
    "    return self._word_to_id[word]\n",
    "\n",
    "  def id2word(self, word_id):\n",
    "    if word_id not in self._id_to_word:\n",
    "      raise ValueError('Id not found in vocab: %d' % word_id)\n",
    "    return self._id_to_word[word_id]\n",
    "\n",
    "  def size(self):\n",
    "    return self._count\n",
    "\n",
    "  def write_metadata(self, fpath):\n",
    "    print(\"Writing word embedding metadata file to %s...\" % (fpath))\n",
    "    with open(fpath, \"w\") as f:\n",
    "      fieldnames = ['word']\n",
    "      writer = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=fieldnames)\n",
    "      for i in xrange(self.size()):\n",
    "        writer.writerow({\"word\": self._id_to_word[i]})\n",
    "        \n",
    "\n",
    "\n",
    "def article2ids(article_words, vocab):\n",
    "    ids = []\n",
    "    oovs = []\n",
    "    unk_id = vocab.word2id(UNKNOWN_TOKEN)\n",
    "    for w in article_words.split():\n",
    "        w = str(w)\n",
    "        i = vocab.word2id(w)\n",
    "        if i == unk_id: # If w is OOV\n",
    "            if w not in oovs: # Add to list of OOVs\n",
    "                oovs.append(w)\n",
    "            oov_num = oovs.index(w) # This is 0 for the first article OOV, 1 for the second article OOV...\n",
    "            if oov_num < ADDITIONAL_WORDS:\n",
    "                ids.append(vocab.size() + oov_num) # This is e.g. 50000 for the first article OOV, 50001 for the second...\n",
    "        else:\n",
    "            ids.append(i)\n",
    "    return ids, oovs\n",
    "\n",
    "\n",
    "def abstract2ids(abstract_words, vocab, article_oovs):\n",
    "    ids = []\n",
    "    unk_id = vocab.word2id(UNKNOWN_TOKEN)\n",
    "    for w in abstract_words.split():\n",
    "        i = vocab.word2id(w)\n",
    "        if i == unk_id: # If w is an OOV word\n",
    "            if w in article_oovs: # If w is an in-article OOV\n",
    "                vocab_idx = vocab.size() + article_oovs.index(w) # Map to its temporary article OOV number\n",
    "                if vocab_idx < VOCAB_SIZE + ADDITIONAL_WORDS:\n",
    "                    ids.append(vocab_idx)\n",
    "        else:\n",
    "            ids.append(i)\n",
    "    return ids\n",
    "\n",
    "def outputids2words(id_list, vocab, article_oovs):\n",
    "    words = []\n",
    "    for i in id_list:\n",
    "        if i < VOCAB_SIZE:\n",
    "            w = vocab.id2word(i)\n",
    "        else:\n",
    "            article_oov_idx = i - vocab.size()\n",
    "            print('unknown generated')\n",
    "            w = article_oovs[article_oov_idx]\n",
    "        words.append(w)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "from tensorflow.core.example import example_pb2\n",
    "\n",
    "def example_gen(filename):\n",
    "    reader = open(filename, 'rb')\n",
    "    examples = []\n",
    "    while True:\n",
    "        len_bytes = reader.read(8)\n",
    "        if not len_bytes: break # finished reading this file\n",
    "        str_len = struct.unpack('q', len_bytes)[0]\n",
    "        example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
    "        e = example_pb2.Example.FromString(example_str)\n",
    "        examples.append(e)\n",
    "        \n",
    "    for e in examples:  \n",
    "        article_text = e.features.feature['article'].bytes_list.value[0]\n",
    "        abstract_text = e.features.feature['abstract'].bytes_list.value[0]\n",
    "        yield (article_text.decode('utf-8'), abstract_text.decode('utf-8'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_size of vocab was specified as 20000; we now have 20000 words. Stopping reading.\n",
      "Finished constructing vocabulary of 20000 total words. Last word added: then-president\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(\"finished_files/vocab\", VOCAB_SIZE)\n",
    "SENTENCE_START_ID = vocab.word2id('<s>')\n",
    "SENTENCE_END_ID = vocab.word2id('</s>')\n",
    "\n",
    "PAD_TOKEN_ID = vocab.word2id('[PAD]')\n",
    "UNKNOWN_TOKEN_ID = vocab.word2id('[UNK]')\n",
    "START_DECODING_ID = vocab.word2id('[START]')\n",
    "STOP_DECODING_ID = vocab.word2id('[STOP]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoder import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24980480"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from decoder import *\n",
    "from pointerTBRU import BeamSearchProviderTBRU, LSTMTBRU, AttentionTBRU, ConcatTBRU, PointerTBRU\n",
    "\n",
    "def build_decoder_model2():\n",
    "    master = DRAGNNDecoderMaster()\n",
    "    embeddings_computer = EmbeddingComputer(VOCAB_SIZE + ADDITIONAL_WORDS, 100, PAD_TOKEN_ID)\n",
    "    master.add_component_encoder(TBRU(\"embed\", TaggerRecurrent(\"input\", \"embed\", False), embeddings_computer, (1,), True).cuda())\n",
    "    #master.add_component(TBRU(\"extractive\", TaggerRecurrent(\"embed\", \"extractive\"), TaggerComputer(1000, 1000), (1,), True).cuda())\n",
    "    master.add_component_encoder(TBRU(\"rnn\", RNNSolidRecurrent(\"embed\", \"rnn\"), RNNSolidComputer(100, 50), (1,), True).cuda())\n",
    "    \n",
    "    master.add_component_decoder(TBRU(\"decoder_embed\", TaggerRecurrent(None, \"decoder_embed\", True),embeddings_computer, (1,), True).cuda())\n",
    "    master.add_component_decoder(BeamSearchProviderTBRU(\"beamProvider\", (1,), None, \"decoder\", True, False))\n",
    "    pTBRU = AttentiveLSTMTBRU(\"decoder\", (1,), True, 100, 100, 100, \"rnn\", \"decoder_embed\", False)\n",
    "    master.add_component_decoder(pTBRU.cuda())\n",
    "    master.add_component_decoder(TBRU(\"output\", TaggerRecurrent(\"decoder\", \"output\", False), TaggerComputer(100, VOCAB_SIZE), (1,), True).cuda())\n",
    "    return master\n",
    "\n",
    "def build_decoder_model():\n",
    "    master = DRAGNNDecoderMaster()\n",
    "    embeddings_computer = EmbeddingComputer(VOCAB_SIZE + ADDITIONAL_WORDS, 100, PAD_TOKEN_ID)\n",
    "    master.add_component_encoder(TBRU(\"embed\", TaggerRecurrent(\"input\", \"embed\", False), embeddings_computer, (1,), True).cuda())\n",
    "    #master.add_component(TBRU(\"extractive\", TaggerRecurrent(\"embed\", \"extractive\"), TaggerComputer(1000, 1000), (1,), True).cuda())\n",
    "    master.add_component_encoder(TBRU(\"rnn\", RNNSolidRecurrent(\"embed\", \"rnn\"), RNNSolidComputer(100, 50), (1,), True).cuda())\n",
    "    \n",
    "    master.add_component_decoder(TBRU(\"decoder_embed\", TaggerRecurrent(None, \"decoder_embed\", True),embeddings_computer, (1,), True).cuda())\n",
    "    \n",
    "    master.add_component_decoder(BeamSearchProviderTBRU(\"beamProvider\", None, \"decoder\", True, False))\n",
    "    \n",
    "    pTBRU = LSTMTBRU(\"decoder\", True, 100, 100, \"rnn\", \"decoder_embed\", False)\n",
    "    master.add_component_decoder(pTBRU.cuda())\n",
    "    master.add_component_decoder(AttentionTBRU(\"attention_layer\", True, \"att_layer\", 100, 100, 100, 'decoder','rnn', False).cuda())\n",
    "    master.add_component_decoder(ConcatTBRU('context_concat', True, 'decoder', 'attention_layer', False))\n",
    "    master.add_component_decoder(TBRU(\"output\", TaggerRecurrent(\"context_concat\", \"output\", False), TaggerComputer(200, VOCAB_SIZE), (1,), True).cuda())\n",
    "    master.add_component_decoder(PointerTBRU(\"pointer_final\", True, 300, VOCAB_SIZE + ADDITIONAL_WORDS, \"att_layer\", \"attention_layer\", \"output\", \"decoder_embed\", \"input\", \"decoder\").cuda())\n",
    "    return master\n",
    "\n",
    "model = build_decoder_model()\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(articles):\n",
    "    lens = [len(article) for article in articles]\n",
    "    max_len = max(lens)\n",
    "    \n",
    "    for i in range(len(articles)):\n",
    "        articles[i].extend([vocab.word2id(PAD_TOKEN)]*(max_len - len(articles[i])))\n",
    "    return np.array(articles).T\n",
    "\n",
    "def add_padding_for_tagging(articles, targets):\n",
    "    lens = [len(article) for article in articles]\n",
    "    max_len = max(lens)\n",
    "    \n",
    "    for i in range(len(articles)):\n",
    "        targets[i].extend([0]*(max_len - len(articles[i])))\n",
    "        articles[i].extend([vocab.word2id(PAD_TOKEN)]*(max_len - len(articles[i])))\n",
    "    return np.array(articles).T, np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def calculate_mask(articles):\n",
    "    mask = (articles == vocab.word2id(PAD_TOKEN))\n",
    "    mask = np.logical_xor(mask, np.ones(articles.shape))\n",
    "    return mask\n",
    "\n",
    "def get_target(self, article, abstract):\n",
    "    return [ int(i in abstract and i != vocab.word2id(UNKNOWN_TOKEN)) for i in article]\n",
    "\n",
    "def push_abs_ptr(article, abstract, i, abs_ptr):\n",
    "    while abs_ptr < len(abstract) and (not abstract[abs_ptr] in article[i+1:] \n",
    "                                               or abstract[abs_ptr] == vocab.word2id(SENTENCE_START)\n",
    "                                               or abstract[abs_ptr] == vocab.word2id(SENTENCE_STOP)\n",
    "                                               or abstract[abs_ptr] == vocab.word2id(UNKNOWN_TOKEN)):\n",
    "        abs_ptr += 1\n",
    "    return abs_ptr\n",
    "\n",
    "class Batcher():\n",
    "    \n",
    "    def __init__(self, filename, batch_size, max_article_len, max_target_len):\n",
    "        self.batch_size = batch_size\n",
    "        generator = example_gen(filename)\n",
    "\n",
    "        self.batches = []\n",
    "        unknown_words_cnt = 0\n",
    "        self.articles = []\n",
    "        self.targets = []\n",
    "        self.unknown_words = []\n",
    "        self.decoder_inputs = []\n",
    "        while True:\n",
    "            articles = []\n",
    "            targets = []\n",
    "            unknown_words = []\n",
    "            decoder_inputs = []\n",
    "            for i in range(batch_size):\n",
    "                try:\n",
    "                    \n",
    "                    article_text, abstract_text = next(generator)\n",
    "                    article_ids, unknown_words_list = article2ids(article_text, vocab)\n",
    "                    art_len = min(max_article_len, len(article_ids))\n",
    "                    article_ids = article_ids[:art_len]\n",
    "                    target = abstract2ids(abstract_text, vocab, unknown_words_list)\n",
    "                    tar_len = min(max_target_len, len(target))\n",
    "                    target = target[:tar_len - 1]\n",
    "                    target.append(STOP_DECODING_ID)\n",
    "                    decoder_input = [START_DECODING_ID]\n",
    "                    decoder_input.extend(abstract2ids(abstract_text, vocab, unknown_words_list))\n",
    "                    decoder_input = decoder_input[:tar_len]\n",
    "                    articles.append(article_ids)\n",
    "                    targets.append(target)\n",
    "                    decoder_inputs.append(decoder_input)\n",
    "                    unknown_words.append(unknown_words_list)\n",
    "                    unknown_words_cnt = max(len(unknown_words_list), unknown_words_cnt)\n",
    "                except:\n",
    "                    break\n",
    "            if len(articles) == 0:\n",
    "                break\n",
    "            self.articles.extend(articles)\n",
    "            self.targets.extend(targets)\n",
    "            self.unknown_words.extend(unknown_words)\n",
    "            self.decoder_inputs.extend(decoder_inputs)\n",
    "            articles = add_padding(articles)\n",
    "            targets = add_padding(targets)\n",
    "            decoder_inputs = add_padding(decoder_inputs)\n",
    "            mask = calculate_mask(articles)\n",
    "            self.batches.append( (articles, targets, mask, decoder_inputs) )\n",
    "        print(len(self.batches))\n",
    "        print(unknown_words_cnt)\n",
    "    \n",
    "    def generator(self):\n",
    "        for batch in self.batches:\n",
    "            yield batch\n",
    "            \n",
    "    def get_random_sample(self):\n",
    "        i = random.randint(0, len(self.articles) - 1)\n",
    "        return np.array([self.articles[i]]).T, np.array(self.targets[i]), np.array([self.decoder_inputs[i]]).T, self.unknown_words[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "SENTENCE_START_ID = vocab.word2id('<s>')\n",
    "SENTENCE_END_ID = vocab.word2id('</s>')\n",
    "\n",
    "PAD_TOKEN_ID = vocab.word2id('[PAD]')\n",
    "UNKNOWN_TOKEN_ID = vocab.word2id('[UNK]')\n",
    "START_DECODING_ID = vocab.word2id('[START]')\n",
    "STOP_DECODING_ID = vocab.word2id('[STOP]')\n",
    "\n",
    "def calculate_bleu(result, target, weights): #TODO\n",
    "    if not isinstance(result, list):\n",
    "        result = result.tolist()\n",
    "    if STOP_DECODING_ID in result:\n",
    "        result = result[:result.index(STOP_DECODING_ID)]\n",
    "    if not isinstance(target, list):\n",
    "        target = target.tolist()\n",
    "    if STOP_DECODING_ID in target:\n",
    "        target = target[:target.index(STOP_DECODING_ID)]\n",
    "    while PAD_TOKEN_ID in target:\n",
    "        target.remove(PAD_TOKEN_ID)\n",
    "    BLEUscore = sentence_bleu([target], result, weights=weights)\n",
    "    return BLEUscore\n",
    "    \n",
    "def calculate_bleu_ngramm(result, target, n):\n",
    "    cnt = 0\n",
    "    for i in range(len(result) - n + 1):\n",
    "        if check_ngramm_in_string(result[i:i+n], target):\n",
    "            cnt += 1\n",
    "    return cnt / (len(result) - n + 1)\n",
    "    \n",
    "def check_ngramm_in_string(ngramm, target):\n",
    "    for i in range(len(target) - len(ngramm) + 1):\n",
    "        flag = True\n",
    "        for j in range(len(ngramm)):\n",
    "            flag = flag and (ngramm[j] == target[i + j])\n",
    "        if flag:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def calculate_logits_bleu_and_rouge(logits, target, weights):\n",
    "    result = logits.argmax(-1).cpu().detach().numpy().T\n",
    "    \n",
    "    target = target.T\n",
    "    #print(result)\n",
    "    #print(target)\n",
    "    bleu = 0.\n",
    "    rouge = 0.\n",
    "    for i in range(result.shape[0]):\n",
    "        bleu += calculate_bleu(result[i], target[i], weights)\n",
    "        rouge += calculate_bleu(target[i], result[i], weights)\n",
    "    return bleu / result.shape[0], rouge / result.shape[0]\n",
    "\n",
    "def generate_summary(article, model, beam_width):\n",
    "    symbols = [START_DECODING_ID]\n",
    "    beam_ids = [0]\n",
    "    probs = [1.]\n",
    "    result = np.array([[]*beam_width])\n",
    "    X_batch = LongTensor(article)\n",
    "    inputs = InputLayerState(\"input\", False, X_batch)\n",
    "    model.eval_run_encoder(inputs)\n",
    "    for i in range(40):\n",
    "        hidden = model.decode((LongTensor([symbols]), LongTensor(beam_ids)))\n",
    "        new_probs = []\n",
    "        new_result = []\n",
    "        for i, s in enumerate(symbols):\n",
    "            values, indices = hidden[i].topk(beam_width)\n",
    "            new_probs.extend(((values + 1) * probs[i]).cpu().detach().tolist())\n",
    "            for j in range(beam_width):\n",
    "                tmp = result[i].tolist()\n",
    "                tmp.append(indices[j].item())\n",
    "                new_result.append(tmp)\n",
    "        top_idx = np.argsort(new_probs)[-beam_width:]\n",
    "        probs = np.array(new_probs)[top_idx]\n",
    "        result = np.array(new_result)[top_idx]\n",
    "        symbols = result[:,-1]\n",
    "        beam_ids = top_idx // beam_width\n",
    "        #symbol[0] == STOP_DECODING_ID\n",
    "    return result[0]\n",
    "\n",
    "def gen_and_print_summary(batcher, model, beam_width):\n",
    "    article_text, target, decoder_input, unk_words = batcher.get_random_sample()\n",
    "    result = generate_summary(article_text, model, beam_width)\n",
    "    result = outputids2words(result, vocab, unk_words)\n",
    "    target = outputids2words(target, vocab, unk_words)\n",
    "    print('result is \\n' + result)\n",
    "    print('target is \\n' + target)\n",
    "    print('BLEU = {:.4f}'.format(sentence_bleu([target], result, weights=(0.33, 0.33, 0.33, 0))))\n",
    "    \n",
    "def gen_and_print_summary_by_target(batcher, model):\n",
    "    article_text, target, decoder_inputs, unk_words = batcher.get_random_sample()\n",
    "    X_batch, y_batch, decoder_batch = LongTensor(article_text), LongTensor(target), LongTensor(decoder_inputs)\n",
    "    inputs = InputLayerState(\"input\", True, X_batch)\n",
    "    targetLayer = InputLayerState(\"target\", True, decoder_batch)\n",
    "    logits = model.train_run(inputs, targetLayer)\n",
    "    \n",
    "\n",
    "    print(calculate_logits_bleu_and_rouge(logits, np.array([target]).T, (0.33,0.33,0.33,0)))\n",
    "    result = logits.argmax(-1).squeeze(0).squeeze(-1).cpu().detach().tolist()\n",
    "    \n",
    "    result = outputids2words(result, vocab, unk_words)\n",
    "    target = outputids2words(target, vocab, unk_words)\n",
    "    print('result is \\n' + result)\n",
    "    print('target is \\n' + target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_f1(tp, fp, tn, fn):\n",
    "    precision = tp/(fp + tp)\n",
    "    recall = tp/(tp + fn)\n",
    "    f1 = 2*precision*recall/(precision+recall)\n",
    "    return f1\n",
    "\n",
    "def precalc_f1(articles_tokens, articles, target):\n",
    "    mask = calculate_mask(articles_tokens).T\n",
    "    result = (articles > 0.5)\n",
    "    #print(result[0])\n",
    "    #print(articles.shape, target.shape, mask.shape)\n",
    "    n_res = np.logical_not(result)\n",
    "    n_tar = np.logical_not(target)\n",
    "    tp = (result * target * mask).sum()\n",
    "    fp = (n_res * target * mask).sum()\n",
    "    tn = (n_res * n_tar * mask).sum()\n",
    "    fn = (result * n_tar * mask).sum()\n",
    "    return tp, fp, tn, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n",
      "146\n"
     ]
    }
   ],
   "source": [
    "train_data=Batcher(\"finished_files/chunked/train_000.bin\", 8, 1000, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result is \n",
      "seaside eastwood refrigerator bids bids bids bids bids bids bids bids bids bids bids bids bids bids bids bids bids bids bids bids bids bids bids bids bids bids bids bids bids bids bids bids bids bids bids estimated holidaying\n",
      "target is \n",
      "british teacher in sudan found guilty of insulting religion . gillian gibbons , 54 , arrested after her class named teddy bear `` mohammed '' gibbons was not convicted of two other charges brought against her . uk consular staff , gibbons ' defense team initially refused access to [STOP]\n",
      "BLEU = 0.1182\n"
     ]
    }
   ],
   "source": [
    "gen_and_print_summary(train_data, model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, bleu_weights, optimizer=None):  \n",
    "    epoch_loss = 0.\n",
    "    bleu = 0.\n",
    "    rouge = 0.\n",
    "    batch_cnt = 1\n",
    "    is_train = not optimizer is None\n",
    "    model.train(is_train)\n",
    "\n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        for i, (article_text, target, mask, decoder_inputs) in enumerate(data.generator()):\n",
    "            batch_cnt =  i + 1\n",
    "            X_batch, y_batch, decoder_batch = LongTensor(article_text), LongTensor(target), LongTensor(decoder_inputs)\n",
    "            inputs = InputLayerState(\"input\", True, X_batch)\n",
    "            targetLayer = InputLayerState(\"target\", True, decoder_batch)\n",
    "            logits = model.train_run(inputs, targetLayer)\n",
    "        \n",
    "            #print(logits.view(1, -1).shape)\n",
    "            loss = criterion(logits.view(-1, logits.shape[-1]), y_batch.view(-1))\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            cur_bleu, cur_rouge = calculate_logits_bleu_and_rouge(logits, target, bleu_weights)\n",
    "            \n",
    "            bleu += cur_bleu\n",
    "            rouge += cur_rouge\n",
    "            \n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                #nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "                optimizer.step()\n",
    "            \n",
    "            print('\\r[{}]: Loss = {:.4f}, BLEU = {:.4f}, ROUGE = {:.4f}'.format(i, loss.item(), cur_bleu, cur_rouge), end='')\n",
    "    \n",
    "    print()\n",
    "    gen_and_print_summary(data, model, 10)\n",
    "    #gen_and_print_summary_by_target(data, model)\n",
    "    return epoch_loss, bleu / batch_cnt, rouge / batch_cnt\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, \n",
    "        batch_size=32, val_data=None, val_batch_size=None):\n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    bleu_weights = (0.5, 0.5, 0, 0)\n",
    "    for epoch in range(epochs_count):\n",
    "        start_time = time.time()\n",
    "        train_loss, bleu, rouge = do_epoch(model, criterion, train_data, batch_size, bleu_weights, optimizer)\n",
    "        output_info = '\\rEpoch {} / {}, Epoch Time = {:.2f}s: Train Loss = {:.4f}: BLEU = {:.4f}, ROUGE = {:.4f}'\n",
    "        if not val_data is None:\n",
    "            val_loss, bleu, rouge = do_epoch(model, criterion, val_data, val_batch_size, bleu_weights, None)\n",
    "            epoch_time = time.time() - start_time\n",
    "            output_info += ', Val Loss = {:.4f}'\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss, bleu, rouge, val_loss))\n",
    "        else:\n",
    "            epoch_time = time.time() - start_time\n",
    "            print(output_info.format(epoch+1, epochs_count, epoch_time, train_loss, bleu, rouge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n",
      "146\n",
      "\r",
      "[0]: Loss = 9.9011, BLEU = 0.0000, ROUGE = 0.0000\r",
      "[1]: Loss = 9.8971, BLEU = 0.0000, ROUGE = 0.0000\r",
      "[2]: Loss = 9.8988, BLEU = 0.0000, ROUGE = 0.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\nltk\\translate\\bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\python36\\lib\\site-packages\\nltk\\translate\\bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\python36\\lib\\site-packages\\nltk\\translate\\bleu_score.py:503: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[124]: Loss = 7.0702, BLEU = 0.0089, ROUGE = 0.0231\n",
      "result is \n",
      "new new new : new : : [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] to\n",
      "target is \n",
      "new : faa says situation `` pretty much resolved '' faa facility south of atlanta , georgia , having [STOP]\n",
      "BLEU = 0.0642\n",
      "Epoch 1 / 50, Epoch Time = 7.51s: Train Loss = 1003.6838: BLEU = 0.0037, ROUGE = 0.0114\n",
      "[124]: Loss = 6.6193, BLEU = 0.0245, ROUGE = 0.0247\n",
      "result is \n",
      "new : new : : of of of the of the the [STOP] [STOP] [STOP] to [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] to [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] in .\n",
      "target is \n",
      "man rescues co-worker from jaws of crocodile in northern australia . he shot crocodile , causing it to let [STOP]\n",
      "BLEU = 0.1194\n",
      "Epoch 2 / 50, Epoch Time = 6.98s: Train Loss = 841.5608: BLEU = 0.0154, ROUGE = 0.0187\n",
      "[124]: Loss = 6.2362, BLEU = 0.0243, ROUGE = 0.0247\n",
      "result is \n",
      "new : new : : `` , , , , , , , , to [STOP] [STOP] [STOP] [STOP] in [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] .\n",
      "target is \n",
      "police found three men dead in a car parked outside a spa monday morning . more than 60 people [STOP]\n",
      "BLEU = 0.0708\n",
      "Epoch 3 / 50, Epoch Time = 7.11s: Train Loss = 803.3584: BLEU = 0.0251, ROUGE = 0.0255\n",
      "[124]: Loss = 5.9252, BLEU = 0.0296, ROUGE = 0.0301\n",
      "result is \n",
      "new : new : `` i , , , , , , , '' , [STOP] [STOP] [STOP] in [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] in\n",
      "target is \n",
      "new : accused chester arthur stiles gets additional charges . new : `` i think he 's a little [STOP]\n",
      "BLEU = 0.1347\n",
      "Epoch 4 / 50, Epoch Time = 7.11s: Train Loss = 767.4155: BLEU = 0.0291, ROUGE = 0.0293\n",
      "[124]: Loss = 5.5864, BLEU = 0.0285, ROUGE = 0.0286\n",
      "result is \n",
      "new : bush : `` i '' `` i '' '' `` i '' [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP]\n",
      "target is \n",
      "`` not even a stone was thrown at them , '' iraqi official says of blackwater guards . iraq [STOP]\n",
      "BLEU = 0.0797\n",
      "Epoch 5 / 50, Epoch Time = 7.26s: Train Loss = 730.3075: BLEU = 0.0314, ROUGE = 0.0316\n",
      "[124]: Loss = 5.1942, BLEU = 0.0456, ROUGE = 0.0461\n",
      "result is \n",
      "new : president bush : `` i i , '' says . new : `` i i [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] to [STOP]\n",
      "target is \n",
      "new : as sustained winds fall off , system to tropical storm . new : hanna could return to [STOP]\n",
      "BLEU = 0.1539\n",
      "Epoch 6 / 50, Epoch Time = 7.10s: Train Loss = 687.2460: BLEU = 0.0355, ROUGE = 0.0357\n",
      "[124]: Loss = 4.7729, BLEU = 0.0654, ROUGE = 0.0656\n",
      "result is \n",
      "new : `` i i am , '' , charged , florida says . new : `` i i [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] in\n",
      "target is \n",
      "chester stiles ' ex-girlfriend says she 's `` disgusted '' she helped him meet child . stiles sought by [STOP]\n",
      "BLEU = 0.1415\n",
      "Epoch 7 / 50, Epoch Time = 7.05s: Train Loss = 639.9427: BLEU = 0.0403, ROUGE = 0.0405\n",
      "[124]: Loss = 4.4535, BLEU = 0.0734, ROUGE = 0.0739\n",
      "result is \n",
      "new : `` i am '' in the country of the year . new : `` i [STOP] [STOP] not [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] in the\n",
      "target is \n",
      "there are more than 44 million people of hispanic origin in the u.s. the community includes diverse national origins [STOP]\n",
      "BLEU = 0.1775\n",
      "Epoch 8 / 50, Epoch Time = 7.02s: Train Loss = 590.9090: BLEU = 0.0502, ROUGE = 0.0505\n",
      "[124]: Loss = 4.0151, BLEU = 0.1191, ROUGE = 0.1202\n",
      "result is \n",
      "new : president bush says he was for the year of the year . new : `` i [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP]\n",
      "target is \n",
      "mme speaks with mohammed alshaya , ceo of m.h. alshaya 's retail division . retail division seeing annual growth [STOP]\n",
      "BLEU = 0.1080\n",
      "Epoch 9 / 50, Epoch Time = 7.11s: Train Loss = 538.7817: BLEU = 0.0763, ROUGE = 0.0767\n",
      "[124]: Loss = 3.6100, BLEU = 0.1883, ROUGE = 0.1888\n",
      "result is \n",
      "new : president bush says he was `` firm '' he was `` firm '' new : `` i [STOP] [STOP] [STOP] '' [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] to [STOP]\n",
      "target is \n",
      "new : bush says he was `` firm '' with putin and that the `` violence is unacceptable '' [STOP]\n",
      "BLEU = 0.2638\n",
      "Epoch 10 / 50, Epoch Time = 7.03s: Train Loss = 486.2935: BLEU = 0.1269, ROUGE = 0.1274\n",
      "[124]: Loss = 3.2976, BLEU = 0.2992, ROUGE = 0.2992\n",
      "result is \n",
      "new : sen. barack barack obama , obama says . new : sen. hillary clinton clinton wins in `` i [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] `` i [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] `` [STOP] ''\n",
      "target is \n",
      "new : clinton says she 's not making any decisions tonight . new : cnn projects clinton wins south [STOP]\n",
      "BLEU = 0.2561\n",
      "Epoch 11 / 50, Epoch Time = 7.04s: Train Loss = 438.5882: BLEU = 0.1904, ROUGE = 0.1911\n",
      "[124]: Loss = 2.9323, BLEU = 0.3246, ROUGE = 0.3248\n",
      "result is \n",
      "new : police say police say they men in the city of south carolina . new : police say [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] did [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP]\n",
      "target is \n",
      "coca-cola was invented in atlanta , georgia , in 1886 , by a u.s. pharmacist . today it is [STOP]\n",
      "BLEU = 0.1160\n",
      "Epoch 12 / 50, Epoch Time = 7.33s: Train Loss = 396.2250: BLEU = 0.2512, ROUGE = 0.2521\n",
      "[124]: Loss = 2.6527, BLEU = 0.3981, ROUGE = 0.3984\n",
      "result is \n",
      "new : bush says he will be tried to be tried to be used on june [STOP] [STOP] [STOP] . [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] ricin\n",
      "target is \n",
      "roger bergendorff is charged with possession of toxin , firearms . he was hospitalized for ricin exposure after it [STOP]\n",
      "BLEU = 0.1632\n",
      "Epoch 13 / 50, Epoch Time = 7.12s: Train Loss = 355.8548: BLEU = 0.3166, ROUGE = 0.3177\n",
      "[124]: Loss = 2.4295, BLEU = 0.4186, ROUGE = 0.4195\n",
      "result is \n",
      "lionel messi scores scores to pay to stay in a row in a row in a row . the united states [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP]\n",
      "target is \n",
      "lionel messi scores for the sixth game in a row as barca defeat atletico 3-0 . real madrid stay [STOP]\n",
      "BLEU = 0.2602\n",
      "Epoch 14 / 50, Epoch Time = 6.95s: Train Loss = 319.6123: BLEU = 0.3763, ROUGE = 0.3774\n",
      "[124]: Loss = 2.2353, BLEU = 0.4269, ROUGE = 0.4308\n",
      "result is \n",
      "new : red cross , jury , 19 , 19 , 19 says . new : soldier soldier [STOP] [STOP] [STOP] [STOP] soldier [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] soldier\n",
      "target is \n",
      "ethiopian soldier dragged after battle with islamic insurgents killed 19 people . the body was bound hand and foot [STOP]\n",
      "BLEU = 0.1467\n",
      "Epoch 15 / 50, Epoch Time = 7.03s: Train Loss = 290.0110: BLEU = 0.4314, ROUGE = 0.4323\n",
      "[124]: Loss = 2.0619, BLEU = 0.5151, ROUGE = 0.5153\n",
      "result is \n",
      "new : rains persist at least able to tropical storm . new : crew was able to able [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP]\n",
      "target is \n",
      "new : bomber burned after landing at al-udeid air base in qatar . new : crew was able to [STOP]\n",
      "BLEU = 0.2196\n",
      "Epoch 16 / 50, Epoch Time = 7.15s: Train Loss = 261.4855: BLEU = 0.4843, ROUGE = 0.4852\n",
      "[124]: Loss = 1.9381, BLEU = 0.5568, ROUGE = 0.5568\n",
      "result is \n",
      "new : protest moves in south carolina carolina of pregnant carolina . new : lynn lynn dead of pregnant [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] u.s. [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] driver [STOP] [STOP] driver\n",
      "target is \n",
      "authorities in north carolina are investigating death of pregnant . spc. megan lynn touma was found dead saturday in [STOP]\n",
      "BLEU = 0.2728\n",
      "Epoch 17 / 50, Epoch Time = 7.15s: Train Loss = 235.7375: BLEU = 0.5311, ROUGE = 0.5318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[124]: Loss = 1.8403, BLEU = 0.5563, ROUGE = 0.5563\n",
      "result is \n",
      "`` i 'm hiding in a clothes rack , '' sheriff says . new : `` i 'm [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] gunshots [STOP] [STOP] [STOP] [STOP] a\n",
      "target is \n",
      "`` i 'm hiding in a clothes rack , '' a caller says . a rapid burst of gunshots [STOP]\n",
      "BLEU = 0.2953\n",
      "Epoch 18 / 50, Epoch Time = 7.39s: Train Loss = 214.8638: BLEU = 0.5822, ROUGE = 0.5828\n",
      "[124]: Loss = 1.6863, BLEU = 0.6006, ROUGE = 0.6007\n",
      "result is \n",
      "new : remains is $ of conspiracy to murder , conspiracy says . new : experts say they [STOP] [STOP] [STOP] to [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] u.s. [STOP] bomb\n",
      "target is \n",
      "3 michigan residents and an ohio resident named in conspiracy , arson counts . they are said to have [STOP]\n",
      "BLEU = 0.2103\n",
      "Epoch 19 / 50, Epoch Time = 8.54s: Train Loss = 196.4498: BLEU = 0.6268, ROUGE = 0.6271\n",
      "[124]: Loss = 1.5646, BLEU = 0.6137, ROUGE = 0.6148\n",
      "result is \n",
      "troops to stay until 2011 , with the troops of troops in canada . new : nato nato [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] to [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] stay [STOP] [STOP] to\n",
      "target is \n",
      "troops to stay until 2011 , with the stipulation that nato contribute more forces . most of canada 's [STOP]\n",
      "BLEU = 0.3108\n",
      "Epoch 20 / 50, Epoch Time = 8.32s: Train Loss = 180.6058: BLEU = 0.6675, ROUGE = 0.6679\n",
      "[124]: Loss = 1.4432, BLEU = 0.6662, ROUGE = 0.6662\n",
      "result is \n",
      "new : 2 says 2 pilots in a crash , 2 says . emergency responders on scene of [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] plane [STOP] [STOP] [STOP] [STOP] .\n",
      "target is \n",
      "air force says 2 pilots in good condition after from plane . emergency responders on scene of crash at [STOP]\n",
      "BLEU = 0.3284\n",
      "Epoch 21 / 50, Epoch Time = 7.81s: Train Loss = 166.3559: BLEU = 0.7035, ROUGE = 0.7038\n",
      "[124]: Loss = 1.3606, BLEU = 0.7188, ROUGE = 0.7188\n",
      "result is \n",
      "new : obama vows to never question the patriotism of others in the campaign . new : obama camp [STOP] her [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] obama [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] his\n",
      "target is \n",
      "new : obama vows to never question the patriotism of others in the campaign . new : obama camp [STOP]\n",
      "BLEU = 0.4328\n",
      "Epoch 22 / 50, Epoch Time = 8.19s: Train Loss = 154.0586: BLEU = 0.7337, ROUGE = 0.7340\n",
      "[124]: Loss = 1.2582, BLEU = 0.7844, ROUGE = 0.7843\n",
      "result is \n",
      "atlanta la , philadelphia to $ million million million million million in the year . most of rising [STOP] [STOP] [STOP] [STOP] fbi [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] rising [STOP] [STOP] [STOP] city\n",
      "target is \n",
      "atlanta la , philadelphia as city with most bank heists . fbi says it 's the result of rising [STOP]\n",
      "BLEU = 0.2670\n",
      "Epoch 23 / 50, Epoch Time = 8.06s: Train Loss = 141.8624: BLEU = 0.7649, ROUGE = 0.7653\n",
      "[124]: Loss = 1.2960, BLEU = 0.7473, ROUGE = 0.7473\n",
      "result is \n",
      "sen. hillary clinton booed sen. hillary sen. hillary clinton booed sen. hillary sen. hillary sen. hillary sen. barack obama sen. hillary sen. hillary sen. hillary clinton booed sen. hillary sen. barack sen. hillary sen. barack clinton obama sen. hillary hillary\n",
      "target is \n",
      "sen. hillary clinton booed when she sen. barack obama at debate . clinton , sen. chris dodd slam obama [STOP]\n",
      "BLEU = 0.2908\n",
      "Epoch 24 / 50, Epoch Time = 7.87s: Train Loss = 131.9977: BLEU = 0.7928, ROUGE = 0.7931\n",
      "[124]: Loss = 1.1431, BLEU = 0.8224, ROUGE = 0.8224\n",
      "result is \n",
      "new : police say 20 is part of involvement in the recruiting suicide bombers held . arrests were killed in chain 20 as [STOP] [STOP] [STOP] [STOP] 20 [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] in [STOP] [STOP] [STOP] bomb\n",
      "target is \n",
      "police say 20 suspected of involvement in the recruiting suicide bombers held . arrests were in italy and across [STOP]\n",
      "BLEU = 0.4397\n",
      "Epoch 25 / 50, Epoch Time = 8.29s: Train Loss = 123.7321: BLEU = 0.8145, ROUGE = 0.8149\n",
      "[124]: Loss = 1.1241, BLEU = 0.7812, ROUGE = 0.7817\n",
      "result is \n",
      "magazines publish images of angelina pitt twins for the first time . ap : rights to images secured of [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] images [STOP] [STOP] [STOP] [STOP] secured\n",
      "target is \n",
      "magazines publish images of angelina pitt twins for the first time . ap : rights to images secured after [STOP]\n",
      "BLEU = 0.4282\n",
      "Epoch 26 / 50, Epoch Time = 8.25s: Train Loss = 115.6669: BLEU = 0.8345, ROUGE = 0.8348\n",
      "[124]: Loss = 0.9960, BLEU = 0.8607, ROUGE = 0.8607\n",
      "result is \n",
      "new : woman suffered brain injury , multiple skull fractures , medical examiner rules . spotted eagle ray leaps leaps [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] rules [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] spotted\n",
      "target is \n",
      "new : woman suffered brain injury , multiple skull fractures , medical examiner rules . spotted eagle ray leaps [STOP]\n",
      "BLEU = 0.4606\n",
      "Epoch 27 / 50, Epoch Time = 8.24s: Train Loss = 106.3673: BLEU = 0.8570, ROUGE = 0.8573\n",
      "[124]: Loss = 0.9521, BLEU = 0.8784, ROUGE = 0.8784\n",
      "result is \n",
      "new : president raul vote on new constitution on may 4 . proposed constitution would strengthen bolivia [STOP] [STOP] to [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] transferred\n",
      "target is \n",
      "bolivian congress decides to let people vote on new constitution on may 4 . proposed constitution would strengthen bolivia [STOP]\n",
      "BLEU = 0.3873\n",
      "Epoch 28 / 50, Epoch Time = 8.31s: Train Loss = 98.3748: BLEU = 0.8751, ROUGE = 0.8753\n",
      "[124]: Loss = 0.8971, BLEU = 0.8779, ROUGE = 0.8779\n",
      "result is \n",
      "coalition troops hand over security control of iraq 's anbar province to iraqis . anbar was once the hub [STOP] hub [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] coalition [STOP] [STOP] [STOP] [STOP] coalition [STOP] [STOP] hand\n",
      "target is \n",
      "coalition troops hand over security control of iraq 's anbar province to iraqis . anbar was once the hub [STOP]\n",
      "BLEU = 0.4419\n",
      "Epoch 29 / 50, Epoch Time = 8.38s: Train Loss = 91.5969: BLEU = 0.8924, ROUGE = 0.8925\n",
      "[124]: Loss = 0.8614, BLEU = 0.8894, ROUGE = 0.8894\n",
      "result is \n",
      "fbi , department of homeland security issue bulletin thursday thursday . bulletin says al qaeda associates made claim to fbi [STOP] [STOP] al qaeda [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] fbi [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] fbi\n",
      "target is \n",
      "fbi , department of homeland security issue bulletin thursday . bulletin says two al qaeda associates made claim to [STOP]\n",
      "BLEU = 0.4691\n",
      "Epoch 30 / 50, Epoch Time = 8.34s: Train Loss = 85.9068: BLEU = 0.9037, ROUGE = 0.9037\n",
      "[124]: Loss = 0.8232, BLEU = 0.8962, ROUGE = 0.8961\n",
      "result is \n",
      "rangers remain four points points clear in scotland after a superb superb 4-0 win at hearts in hearts . jean-claude says she [STOP] [STOP] his [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] his [STOP] [STOP] [STOP] striker\n",
      "target is \n",
      "rangers remain four points clear in scotland after a superb 4-0 win at hearts . jean-claude darcheville and nacho [STOP]\n",
      "BLEU = 0.4221\n",
      "Epoch 31 / 50, Epoch Time = 8.51s: Train Loss = 80.9622: BLEU = 0.9118, ROUGE = 0.9119\n",
      "[124]: Loss = 0.7859, BLEU = 0.8962, ROUGE = 0.8961\n",
      "result is \n",
      "president bush speaks at u.s. air force academy graduation wednesday . bush : `` only way america america could lose [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] report [STOP] [STOP] [STOP] a u.s. [STOP] a president\n",
      "target is \n",
      "president bush speaks at u.s. air force academy graduation wednesday . bush : `` only way america could lose [STOP]\n",
      "BLEU = 0.4670\n",
      "Epoch 32 / 50, Epoch Time = 8.75s: Train Loss = 76.4805: BLEU = 0.9176, ROUGE = 0.9177\n",
      "[124]: Loss = 0.7517, BLEU = 0.8962, ROUGE = 0.8961\n",
      "result is \n",
      "richard branson , founder of virgin , talks to cnn 's todd benjamin . branson left school at 16 [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] richard [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] u.s.\n",
      "target is \n",
      "richard branson , founder of virgin , talks to cnn 's todd benjamin . branson left school at 16 [STOP]\n",
      "BLEU = 0.4245\n",
      "Epoch 33 / 50, Epoch Time = 8.17s: Train Loss = 72.6285: BLEU = 0.9222, ROUGE = 0.9223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[124]: Loss = 0.7136, BLEU = 0.8962, ROUGE = 0.8961\n",
      "result is \n",
      "new : `` i am '' `` vanity fair '' `` vanity fair , '' official says film [STOP] [STOP] film [STOP] [STOP] [STOP] [STOP] [STOP] in [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] `` [STOP] [STOP] '' ``\n",
      "target is \n",
      "nair directed `` monsoon wedding , '' `` vanity fair , '' `` salaam bombay ! '' her film [STOP]\n",
      "BLEU = 0.2737\n",
      "Epoch 34 / 50, Epoch Time = 8.42s: Train Loss = 69.6508: BLEU = 0.9245, ROUGE = 0.9246\n",
      "[124]: Loss = 0.6812, BLEU = 0.8962, ROUGE = 0.8961\n",
      "result is \n",
      "saint petersburg clinch the russian premier league title on sunday . they secure the title ahead of cristiano [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] are [STOP] her [STOP] [STOP] [STOP] .\n",
      "target is \n",
      "zenit saint petersburg clinch the russian premier league title on sunday . they secure the title ahead of spartak [STOP]\n",
      "BLEU = 0.4347\n",
      "Epoch 35 / 50, Epoch Time = 8.35s: Train Loss = 67.2052: BLEU = 0.9248, ROUGE = 0.9249\n",
      "[124]: Loss = 0.6585, BLEU = 0.8962, ROUGE = 0.8961\n",
      "result is \n",
      "new : local officials say one of jet 's two engines caught fire . new : two infants among [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] two to [STOP] [STOP] [STOP] bomb\n",
      "target is \n",
      "new : local officials say one of jet 's two engines caught fire . new : two infants among [STOP]\n",
      "BLEU = 0.4240\n",
      "Epoch 36 / 50, Epoch Time = 8.36s: Train Loss = 64.8191: BLEU = 0.9259, ROUGE = 0.9260\n",
      "[124]: Loss = 0.6465, BLEU = 0.8962, ROUGE = 0.8961\n",
      "result is \n",
      "most coral reefs are to $ 400 million in iraq this week in 2005 . most has be [STOP] [STOP] [STOP] [STOP] in [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] report are\n",
      "target is \n",
      "shintaro tsuji , ceo of sanrio speaks to andrew stevens in the boardroom . sanrio 's most famous character [STOP]\n",
      "BLEU = 0.1638\n",
      "Epoch 37 / 50, Epoch Time = 8.63s: Train Loss = 62.4564: BLEU = 0.9269, ROUGE = 0.9271\n",
      "[124]: Loss = 0.6575, BLEU = 0.8960, ROUGE = 0.8960\n",
      "result is \n",
      "new : obama vows to share their thoughts on how the iraq war has affected their lives . `` having -lsb- my husband having [STOP] [STOP] [STOP] [STOP] [STOP] having [STOP] [STOP] [STOP] [STOP] to [STOP] [STOP] [STOP] [STOP] u.s.\n",
      "target is \n",
      "ireporters share their thoughts on how the iraq war has affected their lives . `` having -lsb- my husband [STOP]\n",
      "BLEU = 0.4576\n",
      "Epoch 38 / 50, Epoch Time = 8.53s: Train Loss = 61.3286: BLEU = 0.9259, ROUGE = 0.9260\n",
      "[124]: Loss = 0.6054, BLEU = 0.9029, ROUGE = 0.9029\n",
      "result is \n",
      "iraq iraq 's prime minister in iran trying trying to calm fears over security pact . nuri al-maliki visiting to [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] security [STOP] [STOP] [STOP] nuri\n",
      "target is \n",
      "iraq 's prime minister in iran trying to calm fears over any u.s.-iraq security pact . nuri al-maliki visiting [STOP]\n",
      "BLEU = 0.4326\n",
      "Epoch 39 / 50, Epoch Time = 8.62s: Train Loss = 57.8834: BLEU = 0.9282, ROUGE = 0.9283\n",
      "[124]: Loss = 0.5656, BLEU = 0.9029, ROUGE = 0.9029\n",
      "result is \n",
      "football pays tribute on the 50th anniversary of the munich air disaster . eight manchester united players died when [STOP] [STOP] on [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] out [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] his\n",
      "target is \n",
      "football pays tribute on the 50th anniversary of the munich air disaster . eight manchester united players died when [STOP]\n",
      "BLEU = 0.4876\n",
      "Epoch 40 / 50, Epoch Time = 8.85s: Train Loss = 54.4967: BLEU = 0.9320, ROUGE = 0.9320\n",
      "[124]: Loss = 0.5405, BLEU = 0.9029, ROUGE = 0.9029\n",
      "result is \n",
      "new : teen gunman is dead , finnish police say . eight people , shot at least [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] teen [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] teen\n",
      "target is \n",
      "new : teen gunman is dead , finnish police say . eight people , including headmistress , shot at [STOP]\n",
      "BLEU = 0.3520\n",
      "Epoch 41 / 50, Epoch Time = 8.82s: Train Loss = 50.3931: BLEU = 0.9345, ROUGE = 0.9346\n",
      "[124]: Loss = 0.5016, BLEU = 0.9029, ROUGE = 0.9029\n",
      "result is \n",
      "scotland yard releases report into assassination of the scotland . only apparent injury was a major trauma to the scotland [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] only [STOP] report [STOP] [STOP] a\n",
      "target is \n",
      "scotland yard releases report into assassination of benazir bhutto . only apparent injury was a major trauma to the [STOP]\n",
      "BLEU = 0.4308\n",
      "Epoch 42 / 50, Epoch Time = 8.85s: Train Loss = 47.3651: BLEU = 0.9354, ROUGE = 0.9355\n",
      "[124]: Loss = 0.4795, BLEU = 0.9029, ROUGE = 0.9029\n",
      "result is \n",
      "`` mad men '' and `` damages '' earn best drama nominations . hbo john adams , and [STOP] [STOP] [STOP] john [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] police\n",
      "target is \n",
      "`` mad men '' and `` damages '' earn best drama nominations . hbo miniseries `` john adams '' [STOP]\n",
      "BLEU = 0.3609\n",
      "Epoch 43 / 50, Epoch Time = 8.78s: Train Loss = 44.9232: BLEU = 0.9357, ROUGE = 0.9358\n",
      "[124]: Loss = 0.4616, BLEU = 0.9029, ROUGE = 0.9029\n",
      "result is \n",
      "new : hanna to pound bahamas , could hours says . new : hanna could be tried as [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] in [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] .\n",
      "target is \n",
      "new : ike goes from winds to 135-mph winds in six hours . hanna to pound bahamas , could [STOP]\n",
      "BLEU = 0.2719\n",
      "Epoch 44 / 50, Epoch Time = 8.97s: Train Loss = 42.8318: BLEU = 0.9364, ROUGE = 0.9364\n",
      "[124]: Loss = 0.4443, BLEU = 0.9029, ROUGE = 0.9029\n",
      "result is \n",
      "more than 400 children likely to return to families , cnn 's jeffrey says . she says she [STOP] [STOP] his [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] more [STOP] [STOP] [STOP] a\n",
      "target is \n",
      "more than 400 children likely to return to families , cnn 's jeffrey toobin says . mother says she [STOP]\n",
      "BLEU = 0.4073\n",
      "Epoch 45 / 50, Epoch Time = 8.72s: Train Loss = 41.3847: BLEU = 0.9364, ROUGE = 0.9365\n",
      "[124]: Loss = 0.4286, BLEU = 0.9029, ROUGE = 0.9029\n",
      "result is \n",
      "crowley : sen. hillary clinton delivered with her speech at the democratic convention . sen. hillary clinton : this was [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] her [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] over [STOP] [STOP] .\n",
      "target is \n",
      "crowley : sen. hillary clinton delivered with her speech at the democratic convention . : this was `` perhaps [STOP]\n",
      "BLEU = 0.4321\n",
      "Epoch 46 / 50, Epoch Time = 8.27s: Train Loss = 39.7815: BLEU = 0.9368, ROUGE = 0.9368\n",
      "[124]: Loss = 0.4193, BLEU = 0.9029, ROUGE = 0.9029\n",
      "result is \n",
      "`` not even a stone was thrown at them , '' iraqi official says of blackwater guards guards . iraq not [STOP] [STOP] [STOP] iraq [STOP] iraq [STOP] [STOP] [STOP] [STOP] in iraq [STOP] [STOP] . [STOP] not [STOP] iraq\n",
      "target is \n",
      "`` not even a stone was thrown at them , '' iraqi official says of blackwater guards . iraq [STOP]\n",
      "BLEU = 0.4569\n",
      "Epoch 47 / 50, Epoch Time = 8.53s: Train Loss = 38.6200: BLEU = 0.9362, ROUGE = 0.9364\n",
      "[124]: Loss = 0.4073, BLEU = 0.9029, ROUGE = 0.9029\n",
      "result is \n",
      "harry potter star daniel radcliffe gets # 20m fortune as he turns 18 monday . young actor says he has [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] as [STOP] [STOP] [STOP] [STOP] in\n",
      "target is \n",
      "harry potter star daniel radcliffe gets # 20m fortune as he turns 18 monday . young actor says he [STOP]\n",
      "BLEU = 0.4461\n",
      "Epoch 48 / 50, Epoch Time = 8.78s: Train Loss = 37.5635: BLEU = 0.9367, ROUGE = 0.9367\n",
      "[124]: Loss = 0.3964, BLEU = 0.9029, ROUGE = 0.9029\n",
      "result is \n",
      "president bush : measure is `` essentially identical '' to the proposal he vetoed before . bill would have been [STOP] [STOP] [STOP] to [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] a [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] president\n",
      "target is \n",
      "president bush : measure is `` essentially identical '' to the proposal he vetoed before . bill would have [STOP]\n",
      "BLEU = 0.4612\n",
      "Epoch 49 / 50, Epoch Time = 8.60s: Train Loss = 36.4789: BLEU = 0.9364, ROUGE = 0.9365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[124]: Loss = 0.3873, BLEU = 0.9029, ROUGE = 0.9029\n",
      "result is \n",
      "suspected islamic rebels fire at a plane carrying somalia 's transitional president . plane was carrying at [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] plane [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] [STOP] his\n",
      "target is \n",
      "suspected islamic rebels fire at a plane carrying somalia 's transitional president . plane attacked as about to take [STOP]\n",
      "BLEU = 0.3986\n",
      "Epoch 50 / 50, Epoch Time = 8.74s: Train Loss = 35.5783: BLEU = 0.9365, ROUGE = 0.9366\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = vocab.word2id(PAD_TOKEN)).cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, epochs_count=50, batch_size=32, train_data=Batcher(\"finished_files/chunked/train_000.bin\",8, 40, 20),\n",
    "    val_data=None, val_batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"first_try_decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-d9726117edcb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m fit(model, criterion, optimizer, epochs_count=50, batch_size=32, train_data=\"finished_files/chunked/train_000.bin\",\n\u001b[1;32m----> 2\u001b[1;33m     val_data=\"finished_files/chunked/val_000.bin\", val_batch_size=32)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-cb1a9beb23c9>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(model, criterion, optimizer, train_data, epochs_count, batch_size, val_data, val_batch_size)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbleu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrouge\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdo_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbleu_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[0moutput_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\rEpoch {} / {}, Epoch Time = {:.2f}s: Train Loss = {:.4f}: BLEU = {:.4f}, ROUGE = {:.4f}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mval_data\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-cb1a9beb23c9>\u001b[0m in \u001b[0;36mdo_epoch\u001b[1;34m(model, criterion, data, batch_size, bleu_weights, optimizer)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0marticle_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_inputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0mbatch_cnt\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'generator'"
     ]
    }
   ],
   "source": [
    "fit(model, criterion, optimizer, epochs_count=50, batch_size=32, train_data=\"finished_files/chunked/train_000.bin\",\n",
    "    val_data=\"finished_files/chunked/val_000.bin\", val_batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
